{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import scipy\n",
    "from scipy.optimize import minimize, fmin\n",
    "from scipy.stats import multivariate_normal\n",
    "import xlrd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "import pingouin as pg\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['tahoma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Obtaining data from a given long context expt\n",
    "\"\"\"\n",
    "Test = pd.read_csv('../auditory_categorization_noContext/important_things_not_included_in_assets/allTrials.csv')\n",
    "Data = pd.read_csv('subjectDataForPlots/noContextData/609553ac4e48356e24a27aed_categorization_task_2022-01-16_22h08.24.972.csv');\n",
    "\n",
    "TestLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "DataLc = pd.read_csv('subjectDataForPlots/biasedLowContextData/609553ac4e48356e24a27aed_categorization_task_longLow_2021-05-15_20h58.53.694.csv');\n",
    "\n",
    "TestHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "DataHc = pd.read_csv('subjectDataForPlots/biasedHighContextData/609553ac4e48356e24a27aed_categorization_task_longHigh_2022-01-21_22h20.32.122.csv');\n",
    "\n",
    "xls = pd.ExcelFile('subjectDataForPlots/ResultsForPlots.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(csv_test, csv_data, exptTotalLength, exptLengthWithBreaks):  \n",
    "    n_tones=3\n",
    "    n_trials = csv_data.shape[0]-47\n",
    "\n",
    "    test_columns = list(csv_test.columns)\n",
    "    test_tones_name = test_columns.index('Name')\n",
    "    test_tones_col_idx = test_columns.index('Tones')\n",
    "    test_toneKind_col_idx = test_columns.index('Tonekind')\n",
    "    df_names = (csv_test.iloc[0:exptTotalLength,test_tones_name]).values\n",
    "    df_tones = (csv_test.iloc[0:exptTotalLength,test_tones_col_idx]).values\n",
    "    df_toneKind = (csv_test.iloc[0:exptTotalLength,test_toneKind_col_idx]).values\n",
    "\n",
    "    tones_array_orig = np.zeros((n_trials,n_tones))\n",
    "    toneKind_array_orig = np.zeros((n_trials,n_tones))\n",
    "    tones_array_idxs_keep = []\n",
    "\n",
    "    for i_wav in range(exptLengthWithBreaks):\n",
    "        if isinstance(csv_data['Name'][i_wav+46],str):\n",
    "            tones_array_orig[i_wav,:] = np.array(df_tones[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                              ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "            toneKind_array_orig[i_wav,:] = np.array(df_toneKind[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                              ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "            tones_array_idxs_keep += [i_wav]\n",
    "\n",
    "\n",
    "    exptTones = np.copy(tones_array_orig[tones_array_idxs_keep,:])\n",
    "    exptToneKind = np.copy(toneKind_array_orig[tones_array_idxs_keep,:])\n",
    "    exptCorrans = np.copy(csv_data['corrAns'][46:csv_data.shape[0]])[tones_array_idxs_keep]\n",
    "    exptKeys = np.copy(csv_data['test_resp.keys'][46:csv_data.shape[0]])[tones_array_idxs_keep]\n",
    "    \n",
    "    return exptTones, exptToneKind, exptCorrans, exptKeys\n",
    "\n",
    "def identifyResponseTrials(keysPressed, correctAns, tonesPlayed, exptTotalLength):\n",
    "    no_response = np.intersect1d(np.where(keysPressed!='h')[0],\n",
    "                                 np.where(keysPressed!='l')[0])\n",
    "    #print(\"Did not respond to: \",no_response)\n",
    "\n",
    "    \"\"\"\n",
    "    Convert keys ['l','h'] to [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    corrans_num_orig = np.zeros_like(correctAns)\n",
    "    corrans_num_orig[correctAns == 'h'] = 1\n",
    "\n",
    "    keys_num_orig = np.zeros_like(keysPressed)\n",
    "    keys_num_orig[keysPressed == 'h'] = 1\n",
    "\n",
    "    corrans_num = corrans_num_orig[:exptTotalLength]\n",
    "    keys_num = keys_num_orig[:exptTotalLength]\n",
    "    tones_array = tonesPlayed[:exptTotalLength]\n",
    "\n",
    "    trial_tones = np.repeat(tones_array,1,axis = 0)\n",
    "    trial_behaviour = np.reshape(keys_num,np.prod(keys_num.shape)) \n",
    "    idxs_with_response = np.delete(np.arange(len(trial_tones)),no_response)\n",
    "    trialTonesResponded = trial_tones[idxs_with_response,:]\n",
    "    trialBehaviourResponded = trial_behaviour[idxs_with_response]\n",
    "    corransResponded = corrans_num[idxs_with_response]\n",
    "    #print(f\"Total trials played are {len(trial_tones)}, and total trials responded to are {len(trialTonesResponded)}\")\n",
    "    #print(\"Got correct: \", np.sum(trialBehaviourResponded==corransResponded)/len(trialTonesResponded))\n",
    "    #print(\"No. of minority category correct: \", np.sum(keys_num*corrans_num)/np.sum(corrans_num))\n",
    "    \n",
    "    return trialTonesResponded, trialBehaviourResponded, corransResponded\n",
    "\n",
    "\n",
    "def plottingInfluenceFn(tones, behaviour):\n",
    "    unique_tones = np.unique(tones)\n",
    "\n",
    "    tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "    for i_tone in range(len(unique_tones)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,0]==unique_tones[i_tone]])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,1]==unique_tones[i_tone]])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,2]==unique_tones[i_tone]])\n",
    "    mean_behaviour = (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3\n",
    "    return unique_tones, mean_behaviour    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noContextData(keys, corrans, tones):\n",
    "    trial_tones_expt, trial_behaviour_expt, corrans_expt = identifyResponseTrials(keysPressed = keys, \n",
    "                                                                                correctAns = corrans, \n",
    "                                                                                tonesPlayed = tones, \n",
    "                                                                                exptTotalLength = 600)\n",
    "\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_expt==corrans_expt)/len(trial_tones_expt))\n",
    "    return trial_tones_expt, trial_behaviour_expt\n",
    "\n",
    "\"\"\"\n",
    "Subsample the long context expt with a low category bias\n",
    "\"\"\"\n",
    "def subsampleLongContextLow(keyslc, corranslc, toneslc):\n",
    "    trial_tones_fulllc, trial_behaviour_fulllc, corrans_fulllc = identifyResponseTrials(keysPressed = keyslc, \n",
    "                                                                                        correctAns = corranslc, \n",
    "                                                                                        tonesPlayed = toneslc, \n",
    "                                                                                        exptTotalLength = 800)\n",
    "    idxHigh = np.arange(len(trial_behaviour_fulllc[0:]))[corrans_fulllc[0:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_fulllc[0:]))[corrans_fulllc[0:]==0]\n",
    "    idxOfSmallerCategory = np.random.choice(idxLow,size=len(idxHigh),replace=False)\n",
    "    idxToKeep = np.concatenate((idxHigh, idxOfSmallerCategory))\n",
    "    corrans_exptlc = corrans_fulllc[0:][idxToKeep]\n",
    "    trial_behaviour_exptlc = trial_behaviour_fulllc[0:][idxToKeep]\n",
    "    trial_tones_exptlc = trial_tones_fulllc[0:][idxToKeep,:]\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_exptlc==corrans_exptlc)/len(trial_tones_exptlc))\n",
    "    return trial_tones_exptlc, trial_behaviour_exptlc\n",
    "\n",
    "\"\"\"\n",
    "Subsample the long context expt with a high category bias\n",
    "\"\"\"\n",
    "def subsampleLongContextHigh(keyshc, corranshc, toneshc):\n",
    "    trial_tones_fullhc, trial_behaviour_fullhc, corrans_fullhc = identifyResponseTrials(keysPressed = keyshc, \n",
    "                                                                                        correctAns = corranshc, \n",
    "                                                                                        tonesPlayed = toneshc, \n",
    "                                                                                        exptTotalLength = 800)\n",
    "    idxHigh = np.arange(len(trial_behaviour_fullhc[0:]))[corrans_fullhc[0:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_fullhc[0:]))[corrans_fullhc[0:]==0]\n",
    "    idxOfSmallerCategory = np.random.choice(idxHigh,size=len(idxLow),replace=False)\n",
    "    idxToKeep = np.concatenate((idxLow, idxOfSmallerCategory))\n",
    "    corrans_expthc = corrans_fullhc[0:][idxToKeep]\n",
    "    trial_behaviour_expthc = trial_behaviour_fullhc[0:][idxToKeep]\n",
    "    trial_tones_expthc = trial_tones_fullhc[0:][idxToKeep,:]\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_expthc==corrans_expthc)/len(trial_tones_expthc))\n",
    "    return trial_tones_expthc, trial_behaviour_expthc\n",
    "\n",
    "\"\"\"\n",
    "Using raw data to calculate any inherent bias the subject develops\n",
    "\"\"\"\n",
    "def bias(trial_behaviour_longContext):\n",
    "    return np.mean(trial_behaviour_longContext)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-psychiatry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Subsampling n times to find mean and sem of bias (n=20).\n",
    "\"\"\"\n",
    "\n",
    "df_tones, df_toneKind, df_corrans, df_keys = extractData(csv_test=Test, \n",
    "                                                        csv_data=Data, \n",
    "                                                        exptTotalLength=600, \n",
    "                                                        exptLengthWithBreaks=603) \n",
    "\n",
    "df_toneslc, df_toneKindlc, df_corranslc, df_keyslc = extractData(csv_test=TestLc, \n",
    "                                                                csv_data=DataLc, \n",
    "                                                                exptTotalLength=800, \n",
    "                                                                exptLengthWithBreaks=804) \n",
    "\n",
    "\n",
    "df_toneshc, df_toneKindhc, df_corranshc, df_keyshc = extractData(csv_test=TestHc, \n",
    "                                                                csv_data=DataHc, \n",
    "                                                                exptTotalLength=800, \n",
    "                                                                exptLengthWithBreaks=804) \n",
    "\n",
    "meanBiaslc = 0; meanBiashc = 0\n",
    "numberOfSubsamples = 20\n",
    "unique_tones_played = np.empty((numberOfSubsamples,30))\n",
    "subjectBehaviour = np.empty((numberOfSubsamples,30))\n",
    "subjectBehaviourLC = np.empty((numberOfSubsamples,30))\n",
    "subjectBehaviourHC = np.empty((numberOfSubsamples,30))\n",
    "for iSubsample in range(numberOfSubsamples):\n",
    "    sampledlc_tones, sampledlc_behaviour = subsampleLongContextLow(toneslc=df_toneslc, \n",
    "                                                                   corranslc=df_corranslc, \n",
    "                                                                   keyslc=df_keyslc)\n",
    "    sampledhc_tones, sampledhc_behaviour = subsampleLongContextHigh(toneshc=df_toneshc, \n",
    "                                                                    corranshc=df_corranshc, \n",
    "                                                                    keyshc=df_keyshc)\n",
    "    noContext_tones, noContext_behaviour = noContextData(keys=df_keys,\n",
    "                                                         corrans=df_corrans,\n",
    "                                                         tones=df_tones)\n",
    "    \n",
    "    unique_tones_played[iSubsample,:], subjectBehaviour[iSubsample,:] = plottingInfluenceFn(noContext_tones, \n",
    "                                                                                        noContext_behaviour)   \n",
    "    \n",
    "    unique_tones_played[iSubsample,:], subjectBehaviourLC[iSubsample,:] = plottingInfluenceFn(sampledlc_tones, \n",
    "                                                                                            sampledlc_behaviour)    \n",
    "    meanBiaslc += bias(sampledlc_behaviour)\n",
    "    \n",
    "    unique_tones_played[iSubsample,:], subjectBehaviourHC[iSubsample,:] = plottingInfluenceFn(sampledhc_tones,\n",
    "                                                                                        sampledhc_behaviour)        \n",
    "    meanBiashc += bias(sampledhc_behaviour)\n",
    "    \n",
    "print(\"length of subsample dataset in low context\", len(sampledlc_tones))\n",
    "print(\"length of subsample dataset in high context\", len(sampledhc_tones))\n",
    "    \n",
    "print(\"p(L) given the no context is likely to be: \",np.mean(noContext_behaviour))\n",
    "print(\"P(L) given the long context data biased towards low is likely to be: \", meanBiaslc/numberOfSubsamples)\n",
    "print(\"P(L) given the long context data biased towards high is likely to be: \", meanBiashc/numberOfSubsamples)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,5))\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(subjectBehaviour,axis=0), \n",
    "            yerr=np.nanstd(subjectBehaviour)/numberOfSubsamples,color='k')\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(subjectBehaviourLC,axis=0), \n",
    "            yerr=np.nanstd(subjectBehaviourLC)/numberOfSubsamples,color='orange')\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(subjectBehaviourHC,axis=0), \n",
    "            yerr=np.nanstd(subjectBehaviourHC)/numberOfSubsamples,color='brown')\n",
    "ax.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "ax.set_xticklabels(labels=[100,1000,3000])\n",
    "ax.set_yticks(ticks=np.arange(0,1.1,0.2))\n",
    "ax.set_yticklabels(labels=np.around(np.arange(0,1.1,0.2),1))\n",
    "ax.tick_params(axis='both',labelsize=26,length=6,width=2)\n",
    "ax.set_xlabel('Frequency (Hz)',fontsize=26)\n",
    "ax.set_ylabel('p(High)',fontsize=26)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/7aed_pBHgivenT_forNoandLongContext.eps',\n",
    "           bbox_inches='tight')\n",
    "print(np.nanmean(subjectBehaviour,axis=0))\n",
    "print(np.mean(np.log10(unique_tones_played),axis=0), 10**2.72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36861909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Experiment design\n",
    "\"\"\"\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(10,5))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in [0,59,119,179,239,299,359,419,479,539,599]:\n",
    "    ax1.plot([cntSeq-1,cntSeq,cntSeq+1],np.log10(df_tones[sequence]),'.',markersize=3)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8    \n",
    "ax1.set_xticks(ticks=cntSeqArray);\n",
    "ax1.set_xticklabels(labels=[0,60,120,180,240,300,360,420,480,540,600],fontsize=23);\n",
    "ax1.set_yticks(ticks=np.log10([100,500,1000,1500,3000]));\n",
    "ax1.set_yticklabels(labels=[100,500,1000,1500,3000],fontsize=23);\n",
    "ax1.tick_params(axis='both',length=6,width=2)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax1.set_ylabel('$\\it{Unbiased}$ $\\it{Session}$ \\n Frequency (Hz)',fontsize=23)\n",
    "#plt.savefig('figures/FromProlific/illustrations/NoContext_trialsHeardBySubject7aed.eps', bbox_inches='tight')\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(10,5))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in [0,99,199,299,399,499,599,699,799]:\n",
    "    ax2.plot([cntSeq-1,cntSeq,cntSeq+1],np.log10(df_toneslc[sequence]),'.',markersize=5)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8\n",
    "ax2.set_xticks(ticks=cntSeqArray);\n",
    "ax2.set_xticklabels(labels=[0,100,200,300,400,500,600,700,800],fontsize=23);\n",
    "ax2.set_yticks(ticks=np.log10([100,500,1000,1500,3000]));\n",
    "ax2.set_yticklabels(labels=[100,500,1000,1500,3000],fontsize=23);\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax2.tick_params(axis='both',length=6,width=2)\n",
    "ax2.set_ylabel('$\\it{Biased}$ $\\it{Low}$ $\\it{Session}$  \\n Frequency (Hz)',fontsize=23)\n",
    "#plt.savefig('figures/FromProlific/illustrations/LowContext_trialsHeardBySubject7aed.eps', bbox_inches='tight')\n",
    "\n",
    "fig, ax3 = plt.subplots(1,1, figsize=(10,5))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in [0,99,199,299,399,499,599,699,799]:\n",
    "    ax3.plot([cntSeq-1,cntSeq,cntSeq+1],np.log10(df_toneshc[sequence]),'.',markersize=5)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8   \n",
    "ax3.set_xticks(ticks=cntSeqArray);\n",
    "ax3.set_xticklabels(labels=[0,100,200,300,400,500,600,700,800],fontsize=23);\n",
    "ax3.set_yticks(ticks=np.log10([100,500,1000,1500,3000]));\n",
    "ax3.set_yticklabels(labels=[100,500,1000,1500,3000],fontsize=23);\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax3.tick_params(axis='both',length=6,width=2)\n",
    "ax3.set_xlabel('Trial number',fontsize=23)\n",
    "ax3.set_ylabel('$\\it{Biased}$ $\\it{High}$ $\\it{Session}$ \\n Frequency (Hz)',fontsize=23)\n",
    "plt.savefig('figures/FromProlific/illustrations/HighContext_trialsHeardBySubject7aed.eps', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb98f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_corrans[[0,59,119,179,239,299,359,419,479,539,599]], \n",
    "      df_corranslc[[0,99,199,299,399,499,599,699,799]], \n",
    "      df_corranshc[[0,99,199,299,399,499,599,699,799]])\n",
    "\n",
    "#print(df_toneKind[[0,59,119,179,239,299,359,419,479,539,599]])\n",
    "#print(df_tones[[0,59,119,179,239,299,359,419,479,539,599]])\n",
    "#print(df_toneKindlc[[0,99,199,299,399,499,599,699,799]]) \n",
    "#print(df_toneslc[[0,99,199,299,399,499,599,699,799]])\n",
    "print(df_toneKindhc[[0,99,199,299,399,499,599,699,799]])\n",
    "print(df_toneshc[[0,99,199,299,399,499,599,699,799]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-mounting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: First figure - how does performance vary with number and frequency of distractors?\n",
    "\"\"\"\n",
    "meanPerformance_lowCategory = np.array([87.09,77.38,64.61,46.53])\n",
    "stdPerformance_lowCategory = np.array([9.11,9.65,8,12.9])\n",
    "meanPerformance_highCategory = np.array([87.96,78.18,67.29,65.95])\n",
    "stdPerformance_highCategory = np.array([6.48,7.93,5.87,15.32])\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(10,7))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "ax1.errorbar(np.arange(4),meanPerformance_lowCategory,\n",
    "             yerr=stdPerformance_lowCategory/np.sqrt(56),\n",
    "             color='orange',linestyle=\"None\",marker='o',markersize=12)\n",
    "ax1.errorbar(np.arange(4),meanPerformance_highCategory,\n",
    "             yerr=stdPerformance_highCategory/np.sqrt(56),\n",
    "             color='brown',linestyle=\"None\",marker='o',markersize=12)\n",
    "ax1.set_xlabel('Number of Distractors',fontsize=32)\n",
    "ax1.set_ylabel('Accuracy',fontsize=32)\n",
    "ax1.set_xticks([0,1,2,3])\n",
    "ax1.set_xticklabels(['0','1','2','3'])\n",
    "ax1.set_ylim(0,100)\n",
    "ax1.yaxis.labelpad = -8\n",
    "ax1.tick_params(axis='both',labelsize=28,length=10,width=2)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "plt.savefig('figures/FromProlific/illustrations/PerformanceAccuracyInThePresenceOfDistractors_partI.eps',\n",
    "            bbox_inches='tight',format='eps')\n",
    "\n",
    "import pickle\n",
    "with open('subjectDataForPlots/PerformanceAccuracyChangesWithFrequencyOfDistractor.pickle', 'rb') as handle:\n",
    "    dict_behaviour = pickle.load(handle)\n",
    "unique_distractors_low = dict_behaviour['unique_distractors_low']\n",
    "averageBehaviorAcrossSubjects_low = dict_behaviour['averageBehaviorAcrossSubjects_low']\n",
    "unique_distractors_high = dict_behaviour['unique_distractors_high']\n",
    "averageBehaviorAcrossSubjects_high = dict_behaviour['averageBehaviorAcrossSubjects_high']\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,7))\n",
    "ax.plot(unique_distractors_low, \n",
    "         np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "         color='orange',linewidth=2)\n",
    "ax.plot(unique_distractors_high, \n",
    "         np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "         color='brown',linewidth=2)\n",
    "ax.fill_between(unique_distractors_low, \n",
    "                 y1 = np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1) - np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),\n",
    "                 y2 = np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1) + np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),\n",
    "                 color='moccasin')\n",
    "ax.fill_between(unique_distractors_high, \n",
    "                 y1 = np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1) - np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),\n",
    "                 y2 = np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1) + np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),\n",
    "                 color='lightcoral')\n",
    "ax.set_xticks(np.array([np.log10(100),np.log10(500),np.log10(990),\n",
    "                        np.log10(1510),np.log10(3000)]))\n",
    "ax.set_xticklabels([100,500,1000,1500,3000])\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_xlabel('Frequency of Distractor Tone Burst (Hz)',fontsize=32)\n",
    "ax.set_ylabel('Accuracy',fontsize=32)\n",
    "ax.yaxis.labelpad = -8\n",
    "ax.tick_params(axis='both',labelsize=28,length=10,width=2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig('figures/FromProlific/illustrations/PerformanceAccuracyInThePresenceOfDistractors_partII.eps',\n",
    "            bbox_inches='tight',format='eps')\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(22,7))\n",
    "ax.bar(unique_distractors_low-0.02,np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "       width=0.03,yerr=np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),color='orange')\n",
    "ax.bar(unique_distractors_high+0.02,np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "       width=0.03,yerr=np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),color='brown')\n",
    "ax.set_xticks(np.arange(2,3.5,0.1))\n",
    "ax.set_xticklabels(np.ceil(10**np.arange(2,3.5,0.1)).astype(int))\n",
    "ax.set_xlabel('Frequency of Distractor Tone Burst (Hz)',fontsize=35)\n",
    "ax.set_ylabel('Accuracy',fontsize=35)\n",
    "ax.tick_params(axis='both',labelsize=30,length=10,width=2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "print(\"Correlation of low category accuracy with number of distractors\", pg.corr(np.arange(4),\n",
    "                                                                                meanPerformance_lowCategory,\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Correlation of high category accuracy with number of distractors\", pg.corr(np.arange(4),\n",
    "                                                                                meanPerformance_highCategory,\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Corelation of low category accuracy with distractor frequency\", pg.corr(unique_distractors_low,\n",
    "                                                                                np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Corelation of high category accuracy with distractor frequency\", pg.corr(unique_distractors_high,\n",
    "                                                                                np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Comparing central frequency using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[8],\n",
    "                                                                averageBehaviorAcrossSubjects_high[8]))\n",
    "print(\"Comparing one frequency before center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[7],\n",
    "                                                                            averageBehaviorAcrossSubjects_high[7]))\n",
    "print(\"Comparing one frequency after center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[9],\n",
    "                                                                            averageBehaviorAcrossSubjects_high[9]))\n",
    "print(\"Comparing two frequencies before center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[6],\n",
    "                                                                            averageBehaviorAcrossSubjects_high[6]))\n",
    "print(\"Comparing two frequencies after center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[10],\n",
    "                                                                            averageBehaviorAcrossSubjects_high[10]))\n",
    "print(\"Median and iqr of central frequency accuracies\", \"low\",\n",
    "      np.median(averageBehaviorAcrossSubjects_low[10]),scipy.stats.iqr(averageBehaviorAcrossSubjects_low[10]),\n",
    "      \"high\", np.median(averageBehaviorAcrossSubjects_high[10]),scipy.stats.iqr(averageBehaviorAcrossSubjects_high[10]))\n",
    "print(\"Average accuracy for first 7 distractor tone frequencies\", \n",
    "      np.nanmean(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[:8]),\n",
    "      np.nanstd(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[:8])/np.sqrt(7))\n",
    "print(\"Average accuracy for last 7 distractor tone frequencies\", \n",
    "      np.nanmean(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[9:]),\n",
    "      np.nanstd(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[9:])/np.sqrt(7))\n",
    "print(\"Median Accuracy for low category trials\",np.median(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1))\n",
    "print(\"Median Accuracy for high category trials\",np.median(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1))\n",
    "print(\"IQR Accuracy for low category trials\",scipy.stats.iqr(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1))\n",
    "print(\"IQR Accuracy for high category trials\",scipy.stats.iqr(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-technology",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Range of sigma sensory across the subject population. \n",
    "\"\"\"\n",
    "indices = np.arange(56)\n",
    "computedLikelihoods = pd.read_excel(xls,'NoContextModelFits')\n",
    "sigmaSensory = computedLikelihoods['ss'].values\n",
    "sigmaSensory = sigmaSensory[~numpy.isnan(sigmaSensory)]\n",
    "plt.hist(sigmaSensory)\n",
    "print(np.quantile(sigmaSensory, 0.5), np.quantile(sigmaSensory, 0.1), np.quantile(sigmaSensory, 0.90), \n",
    "      sum(sigmaSensory>np.quantile(sigmaSensory, 0.95)),indices[sigmaSensory>np.quantile(sigmaSensory, 0.95)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-child",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Can we split participants up into two categories as a first pass? And then everybody else goes on a continuum.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The following variables are for bic values of the subsampled dataset.\n",
    "\"\"\"\n",
    "nll_subsampledData_signalModel = computedLikelihoods['medianFull'].values\n",
    "nll_subsampledData_signalModel = nll_subsampledData_signalModel[~numpy.isnan(nll_subsampledData_signalModel)]\n",
    "\n",
    "nll_subsampledData_lesionModel = computedLikelihoods['medianLesion'].values\n",
    "nll_subsampledData_lesionModel = nll_subsampledData_lesionModel[~numpy.isnan(nll_subsampledData_lesionModel)]\n",
    "\n",
    "nll_subsampledData_randomModel = computedLikelihoods['medianRandom'].values\n",
    "nll_subsampledData_randomModel = nll_subsampledData_randomModel[~numpy.isnan(nll_subsampledData_randomModel)]\n",
    "\n",
    "nll_lowerErrorbars_signalModel = computedLikelihoods['5thPercentileFull'].values\n",
    "nll_lowerErrorbars_signalModel = nll_lowerErrorbars_signalModel[~numpy.isnan(nll_lowerErrorbars_signalModel)]\n",
    "\n",
    "nll_upperErrorbars_signalModel = computedLikelihoods['95thPercentileFull'].values\n",
    "nll_upperErrorbars_signalModel = nll_upperErrorbars_signalModel[~numpy.isnan(nll_upperErrorbars_signalModel)]\n",
    "\n",
    "nll_lowerErrorbars_lesionModel = computedLikelihoods['5thPercentileLesion'].values\n",
    "nll_lowerErrorbars_lesionModel = nll_lowerErrorbars_lesionModel[~numpy.isnan(nll_lowerErrorbars_lesionModel)]\n",
    "\n",
    "nll_upperErrorbars_lesionModel = computedLikelihoods['95thPercentileLesion'].values\n",
    "nll_upperErrorbars_lesionModel = nll_upperErrorbars_lesionModel[~numpy.isnan(nll_upperErrorbars_lesionModel)]\n",
    "\n",
    "nll_lowerErrorbars_randomModel = computedLikelihoods['5thPercentileRandom'].values\n",
    "nll_lowerErrorbars_randomModel = nll_lowerErrorbars_randomModel[~numpy.isnan(nll_lowerErrorbars_randomModel)]\n",
    "\n",
    "nll_upperErrorbars_randomModel = computedLikelihoods['95thPercentileRandom'].values\n",
    "nll_upperErrorbars_randomModel = nll_upperErrorbars_randomModel[~numpy.isnan(nll_upperErrorbars_randomModel)]\n",
    "\n",
    "\"\"\"\n",
    "Qs: Where do subjects lie in the one irrelevant tone and two irrelevant tones space?\n",
    "\"\"\"\n",
    "performance = pd.read_excel(xls,'Strategy_PerformanceAccuracies',nrows = 57)\n",
    "numberOfNoResponses = performance['NumberOfNoResponsesNoContext'].values\n",
    "numberOfNoResponses = numberOfNoResponses[~numpy.isnan(numberOfNoResponses)]\n",
    "distractorPerformance = pd.read_excel(xls,'PerformanceVsNumDistractors',nrows=56)\n",
    "OverallAccuracy = performance['OverallAccuracyNoContext'].values\n",
    "OverallAccuracy = OverallAccuracy[~numpy.isnan(OverallAccuracy)]\n",
    "OneIrrelevantToneAccuracy = np.ma.array(distractorPerformance['CombinedAcrossCategoriesOneDistractor'].values,mask=False)\n",
    "TwoIrrelevantTonesAccuracy = np.ma.array(distractorPerformance['CombinedAcrossCategoriesTwoDistractors'].values,mask=False)\n",
    "\n",
    "fig, (ax0,ax1) = plt.subplots(2,1,figsize=(5,10))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.305, hspace=None)\n",
    "for i in range(len(nll_subsampledData_signalModel)):\n",
    "    ax0.errorbar(2*nll_subsampledData_randomModel[i]+2*np.log(600),\n",
    "                 2*nll_subsampledData_signalModel[i]+6*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_randomModel[i]+2*nll_subsampledData_randomModel[i]],\n",
    "                      [2*nll_upperErrorbars_randomModel[i]-2*nll_subsampledData_randomModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_signalModel[i]+2*nll_subsampledData_signalModel[i]],\n",
    "                      [2*nll_upperErrorbars_signalModel[i]-2*nll_subsampledData_signalModel[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax1.errorbar(2*nll_subsampledData_lesionModel[i]+5*np.log(600),\n",
    "                 2*nll_subsampledData_signalModel[i]+6*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_lesionModel[i]+2*nll_subsampledData_lesionModel[i]],\n",
    "                      [2*nll_upperErrorbars_lesionModel[i]-2*nll_subsampledData_lesionModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_signalModel[i]+2*nll_subsampledData_signalModel[i]],\n",
    "                      [2*nll_upperErrorbars_signalModel[i]-2*nll_subsampledData_signalModel[i]]],\n",
    "                 color='k',marker='o')\n",
    "    \n",
    "ax0.plot(np.arange(300,900),np.arange(300,900),'k--')\n",
    "ax0.set_xticks([300,500,700,900]); ax0.set_yticks([300,500,700,900])\n",
    "ax1.plot(np.arange(300,1450),np.arange(300,1450),'k--')\n",
    "ax1.set_xticks([500,800,1100,1400]); ax1.set_yticks([500,800,1100,1400])\n",
    "\n",
    "ax0.set_xlabel('BIC of Random Choice Strategy', fontsize=20)\n",
    "ax0.set_ylabel('BIC of Probabilistic Strategy',fontsize=20)\n",
    "ax0.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax0.spines['top'].set_visible(False)\n",
    "ax0.spines['right'].set_visible(False)\n",
    "ax1.set_xlabel('BIC of Signal Strategy', fontsize=20)\n",
    "ax1.set_ylabel('BIC of Probabilistic Strategy',fontsize=20)\n",
    "ax1.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "plt.subplots_adjust(hspace=0.33)\n",
    "#plt.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategyLesionvsFull.eps',bbox_inches=\"tight\")\n",
    "    \n",
    "fig, (ax2,ax3) = plt.subplots(2,1,figsize=(5,10))    \n",
    "ax2.plot(OneIrrelevantToneAccuracy,TwoIrrelevantTonesAccuracy,'ko')\n",
    "ax2.plot(OneIrrelevantToneAccuracy[[40,19,36]],TwoIrrelevantTonesAccuracy[[40,19,36]],'ko')\n",
    "ax2.plot(OneIrrelevantToneAccuracy[[29,34,35]],TwoIrrelevantTonesAccuracy[[29,34,35]],\n",
    "         marker='o',linestyle='None',color='k')\n",
    "ax2.set_xlim([50,89.5])\n",
    "ax2.set_ylim([50,89.5])\n",
    "ax2.plot(np.arange(50,89.5),np.arange(50,89.5),'k--')\n",
    "ax2.set_xlabel('Accuracy for trials \\n with one distractor tone burst',fontsize=18)\n",
    "ax2.set_ylabel('Accuracy for trials with two \\n distractor tone bursts',fontsize=18)\n",
    "ax2.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "for i in range(len(nll_subsampledData_signalModel)):\n",
    "    if i in [19,36,40]:\n",
    "        ax3.errorbar(-2*nll_subsampledData_lesionModel[i]-5*np.log(600)\n",
    "                     +2*nll_subsampledData_signalModel[i]+6*np.log(600), \n",
    "                     OverallAccuracy[i],\n",
    "                     xerr=[[2*nll_upperErrorbars_lesionModel[i]-2*nll_lowerErrorbars_signalModel[i]\n",
    "                           -2*nll_subsampledData_lesionModel[i]+2*nll_subsampledData_signalModel[i]],\n",
    "                           [-2*nll_lowerErrorbars_lesionModel[i]+2*nll_upperErrorbars_signalModel[i]\n",
    "                           +2*nll_subsampledData_lesionModel[i]-2*nll_subsampledData_signalModel[i]]],\n",
    "                     yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                             /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                     color='k',marker='o',ecolor='k') \n",
    "    elif i in [29,34,35]: \n",
    "        ax3.errorbar(-2*nll_subsampledData_lesionModel[i]-5*np.log(600)\n",
    "                     +2*nll_subsampledData_signalModel[i]+6*np.log(600), \n",
    "                     OverallAccuracy[i],\n",
    "                     xerr=[[2*nll_upperErrorbars_lesionModel[i]-2*nll_lowerErrorbars_signalModel[i]\n",
    "                           -2*nll_subsampledData_lesionModel[i]+2*nll_subsampledData_signalModel[i]],\n",
    "                           [-2*nll_lowerErrorbars_lesionModel[i]+2*nll_upperErrorbars_signalModel[i]\n",
    "                           +2*nll_subsampledData_lesionModel[i]-2*nll_subsampledData_signalModel[i]]],\n",
    "                     yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                            /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                     color='k',marker='o',ecolor='k')       \n",
    "    else:\n",
    "        ax3.errorbar(-2*nll_subsampledData_lesionModel[i]-5*np.log(600)\n",
    "                     +2*nll_subsampledData_signalModel[i]+6*np.log(600), \n",
    "                     OverallAccuracy[i],\n",
    "                     xerr=[[2*nll_upperErrorbars_lesionModel[i]-2*nll_lowerErrorbars_signalModel[i]\n",
    "                           -2*nll_subsampledData_lesionModel[i]+2*nll_subsampledData_signalModel[i]],\n",
    "                           [-2*nll_lowerErrorbars_lesionModel[i]+2*nll_upperErrorbars_signalModel[i]\n",
    "                           +2*nll_subsampledData_lesionModel[i]-2*nll_subsampledData_signalModel[i]]],\n",
    "                     yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                            /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                     color='k',marker='o')\n",
    "\n",
    "ax3.set_xlabel('BIC of Probabilistic Strategy - \\n BIC of Signal Strategy', fontsize=18)\n",
    "ax3.set_ylabel('Accuracy',fontsize=18)\n",
    "ax3.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('figures/FromProlific/illustrations/variabilityInAccuracy.eps',bbox_inches=\"tight\")\n",
    "\n",
    "\"\"\"\n",
    "Qs: what is the distribution of the bic values of the subjects?\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.hist(nll_subsampledData_lesionModel-nll_subsampledData_signalModel,bins=20)\n",
    "print(np.quantile(-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600), 0.5), \n",
    "      np.quantile(-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600), 0.05), \n",
    "      np.quantile(-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600), 0.95))\n",
    "print(indices[-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600)<\n",
    "      np.quantile(-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600), 0.05)])\n",
    "print(indices[-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600)>\n",
    "      np.quantile(-2*nll_subsampledData_lesionModel-5*np.log(600)+2*nll_subsampledData_signalModel+6*np.log(600), 0.95)])\n",
    "print(np.quantile(OverallAccuracy,0.80))\n",
    "\n",
    "print(\"Median and IQR of random model BICs\",np.median(2*nll_subsampledData_randomModel+2*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_subsampledData_randomModel+2*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of lesion model BICs\",np.median(2*nll_subsampledData_lesionModel+5*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_subsampledData_lesionModel+5*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of signal model BICs\",np.median(2*nll_subsampledData_signalModel+6*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_subsampledData_signalModel+6*np.log(600)))\n",
    "\n",
    "print(\"Comparing random and signal model BICs using wilcoxon\", pg.wilcoxon(2*nll_subsampledData_randomModel+2*np.log(600),\n",
    "                                                                           2*nll_subsampledData_signalModel+6*np.log(600)))\n",
    "print(\"Comparing Lesion and signal model BICs using wilcoxon\", pg.wilcoxon(2*nll_subsampledData_lesionModel+5*np.log(600),\n",
    "                                                                           2*nll_subsampledData_signalModel+6*np.log(600)))\n",
    "print(\"Comparing BIC signal to overall accuracy\",pg.corr(2*nll_subsampledData_signalModel-2*nll_subsampledData_lesionModel+np.log(600),\n",
    "                                                         OverallAccuracy,method='spearman'))\n",
    "\n",
    "OneIrrelevantToneAccuracy.mask[[29,34]]=True\n",
    "TwoIrrelevantTonesAccuracy.mask[[29,34]]=True\n",
    "print(\"Median and IQR of one distractor tone trial accuracy\",np.median(OneIrrelevantToneAccuracy),\n",
    "     scipy.stats.iqr(OneIrrelevantToneAccuracy))\n",
    "print(\"Median and IQR of two distractor tones trial accuracy\",np.median(TwoIrrelevantTonesAccuracy),\n",
    "     scipy.stats.iqr(TwoIrrelevantTonesAccuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443dc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 4.\n",
    "Qs: how can we relate performance to metrics from the bayesian model?\n",
    "\"\"\"\n",
    "pbackCalcuatedOnSubsampledDataset = computedLikelihoods['medianPBack'].values\n",
    "pbackLowerErrorBars = computedLikelihoods['5thPercentilePBack'].values\n",
    "pbackUpperErrorBars = computedLikelihoods['95thPercentilePBack'].values\n",
    "pbackCalcuatedOnSubsampledDataset = pbackCalcuatedOnSubsampledDataset[~numpy.isnan(pbackCalcuatedOnSubsampledDataset)]\n",
    "pbackLowerErrorBars = pbackLowerErrorBars[~numpy.isnan(pbackLowerErrorBars)]\n",
    "pbackUpperErrorBars = pbackUpperErrorBars[~numpy.isnan(pbackUpperErrorBars)]\n",
    "\n",
    "posteriorNorm = computedLikelihoods['medianRelevanceMetric'].values\n",
    "posteriorNorm = posteriorNorm[~numpy.isnan(posteriorNorm)]\n",
    "posteriorNormLowerErrorBars = computedLikelihoods['5thPercentileRelevanceMetric'].values\n",
    "posteriorNormLowerErrorBars = posteriorNormLowerErrorBars[~numpy.isnan(posteriorNormLowerErrorBars)]\n",
    "posteriorNormUpperErrorBars = computedLikelihoods['95thPercentileRelevanceMetric'].values\n",
    "posteriorNormUpperErrorBars = posteriorNormUpperErrorBars[~numpy.isnan(posteriorNormUpperErrorBars)]\n",
    "\n",
    "print(f\"Sigma sensory mean is {np.mean(sigmaSensory)} and standard deviation is {np.std(sigmaSensory)}\")\n",
    "print(f\"Subjects with high sigma sensory are {np.argwhere(sigmaSensory>np.mean(sigmaSensory)+np.std(sigmaSensory))}\")\n",
    "\n",
    "OneIrrelevantToneAccuracy.mask[[29,34]]=False\n",
    "TwoIrrelevantTonesAccuracy.mask[[29,34]]=False\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1,3,figsize=(20,5))\n",
    "for iSubj in range(len(pbackCalcuatedOnSubsampledDataset)):\n",
    "    ax1.errorbar(pbackCalcuatedOnSubsampledDataset[iSubj], OneIrrelevantToneAccuracy[iSubj],\n",
    "                 xerr=[[-pbackLowerErrorBars[iSubj]+pbackCalcuatedOnSubsampledDataset[iSubj]],\n",
    "                      [pbackUpperErrorBars[iSubj]-pbackCalcuatedOnSubsampledDataset[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax2.errorbar(pbackCalcuatedOnSubsampledDataset[iSubj], TwoIrrelevantTonesAccuracy[iSubj],\n",
    "                 xerr=[[-pbackLowerErrorBars[iSubj]+pbackCalcuatedOnSubsampledDataset[iSubj]],\n",
    "                      [pbackUpperErrorBars[iSubj]-pbackCalcuatedOnSubsampledDataset[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax3.errorbar(pbackCalcuatedOnSubsampledDataset[iSubj],OverallAccuracy[iSubj],\n",
    "                xerr=[[-pbackLowerErrorBars[iSubj]+pbackCalcuatedOnSubsampledDataset[iSubj]],\n",
    "                      [pbackUpperErrorBars[iSubj]-pbackCalcuatedOnSubsampledDataset[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "ax1.set_xlabel('P$_{distractor}$',fontsize=22)\n",
    "ax1.set_ylabel('Accuracy for trials with \\n one distractor tone burst',fontsize=22)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.set_xlabel('P$_{distractor}$',fontsize=22)\n",
    "ax2.set_ylabel('Accuracy for trials with \\n two distractor tone bursts',fontsize=22)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax3.set_xlabel('P$_{distractor}$',fontsize=22)\n",
    "ax3.set_ylabel('Accuracy',fontsize=22)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax1.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax2.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax3.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.savefig('figures/FromProlific/illustrations/irrelevantToneAccuracyAndOverallAccuracyGivenPback.eps',bbox_inches=\"tight\")\n",
    "\n",
    "fig, [ax1,ax2,ax3] = plt.subplots(1,3,figsize=(20,5))\n",
    "for iSubj in range(len(pbackCalcuatedOnSubsampledDataset)):\n",
    "    ax1.errorbar(posteriorNorm[iSubj], OneIrrelevantToneAccuracy[iSubj],\n",
    "                 xerr=[[-posteriorNormLowerErrorBars[iSubj]+posteriorNorm[iSubj]],\n",
    "                      [posteriorNormUpperErrorBars[iSubj]-posteriorNorm[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax2.errorbar(posteriorNorm[iSubj], TwoIrrelevantTonesAccuracy[iSubj],\n",
    "                 xerr=[[-posteriorNormLowerErrorBars[iSubj]+posteriorNorm[iSubj]],\n",
    "                      [posteriorNormUpperErrorBars[iSubj]-posteriorNorm[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax3.errorbar(posteriorNorm[iSubj],OverallAccuracy[iSubj],\n",
    "                xerr=[[-posteriorNormLowerErrorBars[iSubj]+posteriorNorm[iSubj]],\n",
    "                      [posteriorNormUpperErrorBars[iSubj]-posteriorNorm[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "ax1.set_xlabel('Relevance Metric',fontsize=22)\n",
    "ax1.set_ylabel('Accuracy for trials with \\n one distractor tone burst',fontsize=22)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.set_xlabel('Relevance Metric',fontsize=22)\n",
    "ax2.set_ylabel('Accuracy for trials with \\n two distractor tone bursts',fontsize=22)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax3.set_xlabel('Relevance Metric',fontsize=22)\n",
    "ax3.set_ylabel('Accuracy',fontsize=22)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax1.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax2.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax3.tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.savefig('figures/FromProlific/illustrations/irrelevantToneAccuracyAndOverallAccuracyGivenPosteriorShape.eps',bbox_inches=\"tight\")\n",
    "\n",
    "print(\"Correlation of pback with accuracies\")\n",
    "print(pg.corr(pbackCalcuatedOnSubsampledDataset,TwoIrrelevantTonesAccuracy,method='spearman'))\n",
    "print(pg.corr(pbackCalcuatedOnSubsampledDataset,OneIrrelevantToneAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,TwoIrrelevantTonesAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,OneIrrelevantToneAccuracy,method='spearman'))\n",
    "print(pg.corr(pbackCalcuatedOnSubsampledDataset,OverallAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,OverallAccuracy,method='spearman'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-radical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 5.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Qs: are subjects equally biased in both the long context cases? \n",
    "This is the internalized bias which is obtained from the matched raw data for subjects that do both the long \n",
    "context expts.\n",
    "\"\"\"\n",
    "internalizedBias = pd.read_excel(xls,'internalizedBias',nrows=53)\n",
    "\n",
    "biasLowForSubjectsWithBothLongContexts = internalizedBias['biasLowContext']\n",
    "biasLowForSubjectsWithBothLongContexts = biasLowForSubjectsWithBothLongContexts[~numpy.isnan(internalizedBias['biasHighContext'])]\n",
    "biasHighForSubjectsWithBothLongContexts = internalizedBias['biasHighContext']\n",
    "biasHighForSubjectsWithBothLongContexts = biasHighForSubjectsWithBothLongContexts[~numpy.isnan(biasHighForSubjectsWithBothLongContexts)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot((0.5-biasLowForSubjectsWithBothLongContexts)*2,(biasHighForSubjectsWithBothLongContexts-0.5)*2,'k.')\n",
    "ax.plot(np.arange(-0.5,0.5,0.1),np.arange(-0.5,0.5,0.1),'k--')\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Internalized Bias in Low Context',fontsize=26)\n",
    "plt.ylabel('Internalized Bias in High Context',fontsize=26)\n",
    "plt.savefig('figures/FromProlific/illustrations/biasLowContextVsbiasHighContext',bbox_inches=\"tight\")\n",
    "\n",
    "\"\"\"\n",
    "Qs: are subjects also biased in the no context case? This is benchmarking internalized bias against \n",
    "the no context.  \n",
    "\"\"\"\n",
    "\n",
    "biasLowForSubjectsWithNoAndLowContexts = internalizedBias['biasLowContext']\n",
    "biasLowForSubjectsWithNoAndLowContexts = biasLowForSubjectsWithNoAndLowContexts[~numpy.isnan(biasLowForSubjectsWithNoAndLowContexts)]\n",
    "biasNoForSubjectsWithNoAndLowContexts = internalizedBias['biasNoContext']\n",
    "biasNoForSubjectsWithNoAndLowContexts = biasNoForSubjectsWithNoAndLowContexts[~numpy.isnan(internalizedBias['biasLowContext'])]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,(biasNoForSubjectsWithNoAndLowContexts-0.5)*2,'o',color='orange')\n",
    "ax.hlines(0,xmin=-0.5,xmax=0.5,color='k',linestyles='--')\n",
    "ax.vlines(0,ymin=-0.5,ymax=0.5,color='k',linestyles='--')\n",
    "\n",
    "biasHighForSubjectsWithNoAndHighContexts = internalizedBias['biasHighContext']\n",
    "biasHighForSubjectsWithNoAndHighContexts = biasHighForSubjectsWithNoAndHighContexts[~numpy.isnan(biasHighForSubjectsWithNoAndHighContexts)]\n",
    "biasNoForSubjectsWithNoAndHighContexts = internalizedBias['biasNoContext']\n",
    "biasNoForSubjectsWithNoAndHighContexts = biasNoForSubjectsWithNoAndHighContexts[~numpy.isnan(internalizedBias['biasHighContext'])]\n",
    "\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,(biasNoForSubjectsWithNoAndHighContexts-0.5)*2,'o',color='brown')\n",
    "ax.hlines(0,xmin=-0.4,xmax=0.8,color='k',linestyles='--')\n",
    "ax.vlines(0,ymin=-0.4,ymax=0.4,color='k',linestyles='--')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xlabel('Internalized Bias in the $\\it{biased}$ sessions',fontsize=23)\n",
    "plt.ylabel('Internalized Bias in \\n the $\\it{unbiased}$ session',fontsize=23)\n",
    "plt.savefig('figures/FromProlific/illustrations/biasNoContextVsbiasLongContext.eps',bbox_inches=\"tight\")\n",
    "\n",
    "\"\"\"\n",
    "Qs: how does internalized bias compare to performance accuracy?\n",
    "\"\"\"\n",
    "biasHighForAllSubjectsWithHighContext = internalizedBias['biasHighContext'].values\n",
    "biasHighForAllSubjectsWithHighContext = biasHighForAllSubjectsWithHighContext[~numpy.isnan(internalizedBias['biasHighContext'])]\n",
    "majorityVsMinorityCategoryAccuracyHighContext = internalizedBias['DifferenceInAccuracyOfCategoriesInHighContext']\n",
    "majorityVsMinorityCategoryAccuracyHighContext = majorityVsMinorityCategoryAccuracyHighContext[~numpy.isnan(majorityVsMinorityCategoryAccuracyHighContext)]\n",
    "\n",
    "biasLowForAllSubjectsWithLowContext = internalizedBias['biasLowContext'].values\n",
    "majorityVsMinorityCategoryAccuracyLowContext = internalizedBias['DifferenceInAccuracyOfCategoriesInLowContext']\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,majorityVsMinorityCategoryAccuracyLowContext*100,\n",
    "        'o',color='orange')\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,majorityVsMinorityCategoryAccuracyHighContext*100,\n",
    "        'o',color='brown')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=23)\n",
    "plt.ylabel('Accuracy of overrepresented \\n category - accuracy of \\n underrepresented category',fontsize=23)\n",
    "plt.savefig('figures/FromProlific/illustrations/accuracyInMajorityMinorityCategoriesExplainsInternalisedBias.eps',\n",
    "            bbox_inches=\"tight\")\n",
    "\n",
    "AccuracyHighContext = internalizedBias['accuracyHighContext']\n",
    "AccuracyHighContext = AccuracyHighContext[~numpy.isnan(AccuracyHighContext)]\n",
    "AccuracyLowContext = internalizedBias['accuracyLowContext']\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(7,5))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,AccuracyLowContext,\n",
    "        'o',color='orange')\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,AccuracyHighContext,\n",
    "        'o',color='brown')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=23)\n",
    "plt.ylabel('Accuracy',fontsize=23)\n",
    "plt.savefig('figures/FromProlific/illustrations/totalAccuracyVsInternalisedBias.eps',\n",
    "            bbox_inches=\"tight\")\n",
    "\n",
    "print(\"Median of internalized bias in the unbiased session for bias low subjects\", \n",
    "      np.median((0.5-biasNoForSubjectsWithNoAndLowContexts)*2), \n",
    "      scipy.stats.iqr((0.5-biasNoForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Median of internalized bias in the unbiased session for bias high subjects\", \n",
    "      np.median((0.5-biasNoForSubjectsWithNoAndHighContexts)*2), \n",
    "      scipy.stats.iqr((0.5-biasNoForSubjectsWithNoAndHighContexts)*2))\n",
    "print(\"Median of internalized bias in the biased low session\", \n",
    "      np.median((0.5-biasLowForSubjectsWithNoAndLowContexts)*2),\n",
    "      scipy.stats.iqr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Median of internalized bias in the biased high session\", \n",
    "     np.median((biasHighForSubjectsWithNoAndHighContexts-0.5)*2), \n",
    "      scipy.stats.iqr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2))\n",
    "print(\"Comparing bias in no and low sessions\",pg.wilcoxon((0.5-biasNoForSubjectsWithNoAndLowContexts)*2,\n",
    "                                                          (0.5-biasLowForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Comparing bias in no and high sessions\",pg.wilcoxon((0.5-biasNoForSubjectsWithNoAndHighContexts)*2,\n",
    "                                                           (biasHighForSubjectsWithNoAndHighContexts-0.5)*2))\n",
    "print(\"Correlation of bias with difference in accuracy in biased low session\",\n",
    "      pg.corr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,\n",
    "              majorityVsMinorityCategoryAccuracyLowContext,method='spearman'))\n",
    "print(\"Correlation of bias with difference in accuracy in biased high session\",\n",
    "      pg.corr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,\n",
    "              majorityVsMinorityCategoryAccuracyHighContext,method='spearman'))\n",
    "print(\"Correlation of bias and accuracy in biased low session\", pg.corr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,\n",
    "                                                                        AccuracyLowContext,method='spearman'))\n",
    "print(\"Correlation of bias and accuracy in biased high session\", pg.corr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,\n",
    "                                                                         AccuracyHighContext,method='spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-arizona",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 7.\n",
    "Qs: What are the trends in the long context? \n",
    "\"\"\"\n",
    "computedLikelihoodsLowContext = pd.read_excel(xls,'LowContextModelFits')\n",
    "\n",
    "bic_lowContext_signalModel = np.ma.array(computedLikelihoodsLowContext['medianFull'].values,mask=False)\n",
    "bic_lowContext_signalModel = bic_lowContext_signalModel[~numpy.isnan(bic_lowContext_signalModel)]\n",
    "\n",
    "bic_lowContext_randomModel = np.ma.array(computedLikelihoodsLowContext['medianRandom'].values,mask=False)\n",
    "bic_lowContext_randomModel = bic_lowContext_randomModel[~numpy.isnan(bic_lowContext_randomModel)]\n",
    "\n",
    "bic_lowContext_lesionModel = np.ma.array(computedLikelihoodsLowContext['medianLesion'].values,mask=False)\n",
    "bic_lowContext_lesionModel = bic_lowContext_lesionModel[~numpy.isnan(bic_lowContext_lesionModel)]\n",
    "\n",
    "bic_lowContext_lowerError_signalModel = np.ma.array(computedLikelihoodsLowContext['5thPercentileFull'].values,mask=False)\n",
    "bic_lowContext_lowerError_signalModel = bic_lowContext_lowerError_signalModel[~numpy.isnan(bic_lowContext_lowerError_signalModel)]\n",
    "\n",
    "bic_lowContext_upperError_signalModel = np.ma.array(computedLikelihoodsLowContext['95thPercentileFull'].values,mask=False)\n",
    "bic_lowContext_upperError_signalModel = bic_lowContext_upperError_signalModel[~numpy.isnan(bic_lowContext_upperError_signalModel)]\n",
    "\n",
    "bic_lowContext_lowerError_randomModel = np.ma.array(computedLikelihoodsLowContext['5thPercentileRandom'].values,mask=False)\n",
    "bic_lowContext_lowerError_randomModel = bic_lowContext_lowerError_randomModel[~numpy.isnan(bic_lowContext_lowerError_randomModel)]\n",
    "\n",
    "bic_lowContext_upperError_randomModel = np.ma.array(computedLikelihoodsLowContext['95thPercentileRandom'].values,mask=False)\n",
    "bic_lowContext_upperError_randomModel = bic_lowContext_upperError_randomModel[~numpy.isnan(bic_lowContext_upperError_randomModel)]\n",
    "\n",
    "bic_lowContext_lowerError_lesionModel = np.ma.array(computedLikelihoodsLowContext['5thPercentileLesion'].values,mask=False)\n",
    "bic_lowContext_lowerError_lesionModel = bic_lowContext_lowerError_lesionModel[~numpy.isnan(bic_lowContext_lowerError_lesionModel)]\n",
    "\n",
    "bic_lowContext_upperError_lesionModel = np.ma.array(computedLikelihoodsLowContext['95thPercentileLesion'].values,mask=False)\n",
    "bic_lowContext_upperError_lesionModel = bic_lowContext_upperError_lesionModel[~numpy.isnan(bic_lowContext_upperError_lesionModel)]\n",
    "\n",
    "size_lowContext = np.ma.array(computedLikelihoodsLowContext['sizeSubsampledDataset'].values,mask=False)\n",
    "size_lowContext = size_lowContext[~np.isnan(size_lowContext)]\n",
    "\n",
    "computedLikelihoodsHighContext = pd.read_excel(xls,'HighContextModelFits',nrows=158)\n",
    "\n",
    "bic_highContext_signalModel = np.ma.array(computedLikelihoodsHighContext['medianFull'].values,mask=False)\n",
    "bic_highContext_signalModel = bic_highContext_signalModel[~numpy.isnan(bic_highContext_signalModel)]\n",
    "\n",
    "bic_highContext_randomModel = np.ma.array(computedLikelihoodsHighContext['medianRandom'].values,mask=False)\n",
    "bic_highContext_randomModel = bic_highContext_randomModel[~numpy.isnan(bic_highContext_randomModel)]\n",
    "\n",
    "bic_highContext_lesionModel = np.ma.array(computedLikelihoodsHighContext['medianLesion'].values,mask=False)\n",
    "bic_highContext_lesionModel = bic_highContext_lesionModel[~numpy.isnan(bic_highContext_lesionModel)]\n",
    "\n",
    "bic_highContext_lowerError_signalModel = np.ma.array(computedLikelihoodsHighContext['5thPercentileFull'].values,mask=False)\n",
    "bic_highContext_lowerError_signalModel = bic_highContext_lowerError_signalModel[~numpy.isnan(bic_highContext_lowerError_signalModel)]\n",
    "\n",
    "bic_highContext_upperError_signalModel = np.ma.array(computedLikelihoodsHighContext['95thPercentileFull'].values,mask=False)\n",
    "bic_highContext_upperError_signalModel = bic_highContext_upperError_signalModel[~numpy.isnan(bic_highContext_upperError_signalModel)]\n",
    "\n",
    "bic_highContext_lowerError_randomModel = np.ma.array(computedLikelihoodsHighContext['5thPercentileRandom'].values,mask=False)\n",
    "bic_highContext_lowerError_randomModel = bic_highContext_lowerError_randomModel[~numpy.isnan(bic_highContext_lowerError_randomModel)]\n",
    "\n",
    "bic_highContext_upperError_randomModel = np.ma.array(computedLikelihoodsHighContext['95thPercentileRandom'].values,mask=False)\n",
    "bic_highContext_upperError_randomModel = bic_highContext_upperError_randomModel[~numpy.isnan(bic_highContext_upperError_randomModel)]\n",
    "\n",
    "bic_highContext_lowerError_lesionModel = np.ma.array(computedLikelihoodsHighContext['5thPercentileLesion'].values,mask=False)\n",
    "bic_highContext_lowerError_lesionModel = bic_highContext_lowerError_lesionModel[~numpy.isnan(bic_highContext_lowerError_lesionModel)]\n",
    "\n",
    "bic_highContext_upperError_lesionModel = np.ma.array(computedLikelihoodsHighContext['95thPercentileLesion'].values,mask=False)\n",
    "bic_highContext_upperError_lesionModel = bic_highContext_upperError_lesionModel[~numpy.isnan(bic_highContext_upperError_lesionModel)]\n",
    "\n",
    "size_highContext = np.ma.array(computedLikelihoodsHighContext['sizeSubsampledDataset'].values,mask=False)\n",
    "size_highContext = size_highContext[~np.isnan(size_highContext)]\n",
    "\n",
    "pcategory_lowContextWithNan = np.ma.array(computedLikelihoodsLowContext['medianPLow'].values,mask=False)\n",
    "pcategory_lowContext = pcategory_lowContextWithNan[~numpy.isnan(pcategory_lowContextWithNan)]\n",
    "\n",
    "pcategory_lowContextWithNan_lowerError = np.ma.array(computedLikelihoodsLowContext['5thPercentilePLow'].values,mask=False)\n",
    "pcategory_lowContext_lowerError = pcategory_lowContextWithNan_lowerError[~numpy.isnan(pcategory_lowContextWithNan_lowerError)]\n",
    "\n",
    "pcategory_lowContextWithNan_upperError = np.ma.array(computedLikelihoodsLowContext['95thPercentilePLow'].values,mask=False)\n",
    "pcategory_lowContext_upperError = pcategory_lowContextWithNan_upperError[~numpy.isnan(pcategory_lowContextWithNan_upperError)]\n",
    "\n",
    "pback_lowContextWithNan = np.ma.array(computedLikelihoodsLowContext['pback'].values,mask=False)\n",
    "pback_lowContext = pback_lowContextWithNan[~numpy.isnan(pback_lowContextWithNan)]\n",
    "\n",
    "posteriorNorm_lowContextWithNan = np.ma.array(computedLikelihoodsLowContext['medianRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_lowContext = posteriorNorm_lowContextWithNan[~numpy.isnan(posteriorNorm_lowContextWithNan)]\n",
    "\n",
    "posteriorNorm_lowContextWithNan_lowerError = np.ma.array(computedLikelihoodsLowContext['5thPercentileRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_lowContext_lowerError = posteriorNorm_lowContextWithNan_lowerError[~numpy.isnan(posteriorNorm_lowContextWithNan_lowerError)]\n",
    "\n",
    "posteriorNorm_lowContextWithNan_upperError = np.ma.array(computedLikelihoodsLowContext['95thPercentileRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_lowContext_upperError = posteriorNorm_lowContextWithNan_upperError[~numpy.isnan(posteriorNorm_lowContextWithNan_upperError)]\n",
    "\n",
    "pcategory_highContextWithNan = np.ma.array(computedLikelihoodsHighContext['medianPLow'].values,mask=False)\n",
    "pcategory_highContext = pcategory_highContextWithNan[~numpy.isnan(pcategory_highContextWithNan)]\n",
    "\n",
    "pcategory_highContextWithNan_lowerError = np.ma.array(computedLikelihoodsHighContext['5thPercentilePLow'].values,mask=False)\n",
    "pcategory_highContext_lowerError = pcategory_highContextWithNan_lowerError[~numpy.isnan(pcategory_highContextWithNan_lowerError)]\n",
    "\n",
    "pcategory_highContextWithNan_upperError = np.ma.array(computedLikelihoodsHighContext['95thPercentilePLow'].values,mask=False)\n",
    "pcategory_highContext_upperError = pcategory_highContextWithNan_upperError[~numpy.isnan(pcategory_highContextWithNan_upperError)]\n",
    "\n",
    "pback_highContextWithNan = np.ma.array(computedLikelihoodsHighContext['pback'].values,mask=False)\n",
    "pback_highContext = pback_highContextWithNan[~numpy.isnan(pback_highContextWithNan)]\n",
    "\n",
    "posteriorNorm_highContextWithNan = np.ma.array(computedLikelihoodsHighContext['medianRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_highContext = posteriorNorm_highContextWithNan[~numpy.isnan(posteriorNorm_highContextWithNan)]\n",
    "\n",
    "posteriorNorm_highContextWithNan_lowerError = np.ma.array(computedLikelihoodsHighContext['5thPercentileRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_highContext_lowerError = posteriorNorm_highContextWithNan_lowerError[~numpy.isnan(posteriorNorm_highContextWithNan_lowerError)]\n",
    "\n",
    "posteriorNorm_highContextWithNan_upperError = np.ma.array(computedLikelihoodsHighContext['95thPercentileRelevanceMetric'].values,mask=False)\n",
    "posteriorNorm_highContext_upperError = posteriorNorm_highContextWithNan_upperError[~numpy.isnan(posteriorNorm_highContextWithNan_upperError)]\n",
    "\n",
    "fig, ax = plt.subplots(2,3,figsize=(22,10))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.35, hspace=0.35)\n",
    "for i in np.arange(53):     \n",
    "    ax[0,0].errorbar(2*bic_lowContext_randomModel[i]+2*np.log(size_lowContext[i]),\n",
    "                 2*bic_lowContext_signalModel[i*2]+6*np.log(size_lowContext[i]), \n",
    "                 xerr=[[-2*bic_lowContext_lowerError_randomModel[i]+2*bic_lowContext_randomModel[i]],\n",
    "                      [2*bic_lowContext_upperError_randomModel[i]-2*bic_lowContext_randomModel[i]]],\n",
    "                 yerr=[[-2*bic_lowContext_lowerError_signalModel[i*2]+2*bic_lowContext_signalModel[i*2]],\n",
    "                      [2*bic_lowContext_upperError_signalModel[i*2]-2*bic_lowContext_signalModel[i*2]]],\n",
    "                 color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    ax[1,0].errorbar(2*bic_highContext_randomModel[i]+2*np.log(size_highContext[i]),\n",
    "                 2*bic_highContext_signalModel[i]+6*np.log(size_highContext[i]), \n",
    "                 xerr=[[-2*bic_highContext_lowerError_randomModel[i]+2*bic_highContext_randomModel[i]],\n",
    "                      [2*bic_highContext_upperError_randomModel[i]-2*bic_highContext_randomModel[i]]],\n",
    "                 yerr=[[-2*bic_highContext_lowerError_signalModel[i]+2*bic_highContext_signalModel[i]],\n",
    "                      [2*bic_highContext_upperError_signalModel[i]-2*bic_highContext_signalModel[i]]],\n",
    "                 color='brown',marker='o')\n",
    "    \n",
    "ax[0,0].plot(np.arange(200,800),np.arange(200,800),'k--')  \n",
    "ax[0,0].set_xticks([200,400,600,800])\n",
    "ax[0,0].set_xticklabels([200,400,600,800])\n",
    "ax[1,0].plot(np.arange(200,700),np.arange(200,700),'k--') \n",
    "ax[1,0].set_xticks([200,400,600])\n",
    "ax[1,0].set_xticklabels([200,400,600])\n",
    "ax[0,0].set_xlabel('BIC of Random Choice Strategy', fontsize=22)\n",
    "ax[0,0].set_ylabel('BIC of Probabilistic Strategy',fontsize=22)\n",
    "ax[0,0].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[0,0].spines['top'].set_visible(False)\n",
    "ax[0,0].spines['right'].set_visible(False)\n",
    "ax[1,0].set_xlabel('BIC of Random Choice Strategy', fontsize=22)\n",
    "ax[1,0].set_ylabel('BIC of Probabilistic Strategy',fontsize=22)\n",
    "ax[1,0].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[1,0].spines['top'].set_visible(False)\n",
    "ax[1,0].spines['right'].set_visible(False)\n",
    "\n",
    "for i in range(53):  \n",
    "    if i not in [25,45]:\n",
    "        ax[0,1].errorbar(2*bic_lowContext_lesionModel[i]+5*np.log(size_lowContext[i]),\n",
    "                         2*bic_lowContext_signalModel[i*2]+6*np.log(size_lowContext[i]), \n",
    "                         xerr=[[-2*bic_lowContext_lowerError_lesionModel[i]+2*bic_lowContext_lesionModel[i]],\n",
    "                              [2*bic_lowContext_upperError_lesionModel[i]-2*bic_lowContext_lesionModel[i]]],\n",
    "                         yerr=[[-2*bic_lowContext_lowerError_signalModel[i*2]+2*bic_lowContext_signalModel[i*2]],\n",
    "                              [2*bic_lowContext_upperError_signalModel[i*2]-2*bic_lowContext_signalModel[i*2]]],\n",
    "                         color='orange',marker='o')\n",
    "        ax[0,2].errorbar((pcategory_lowContext[i]-0.5)*2,posteriorNorm_lowContext[i], \n",
    "                         xerr=[[-pcategory_lowContext_lowerError[i]+pcategory_lowContext[i]],\n",
    "                              [pcategory_lowContext_upperError[i]-pcategory_lowContext[i]]],\n",
    "                         yerr=[[-posteriorNorm_lowContext_lowerError[i]+posteriorNorm_lowContext[i]],\n",
    "                              [posteriorNorm_lowContext_upperError[i]-posteriorNorm_lowContext[i]]],\n",
    "                         color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax[1,1].errorbar(2*bic_highContext_lesionModel[i]+5*np.log(size_highContext[i]),\n",
    "                         2*bic_highContext_signalModel[i]+6*np.log(size_highContext[i]), \n",
    "                         xerr=[[-2*bic_highContext_lowerError_lesionModel[i]+2*bic_highContext_lesionModel[i]],\n",
    "                              [2*bic_highContext_upperError_lesionModel[i]-2*bic_highContext_lesionModel[i]]],\n",
    "                         yerr=[[-2*bic_highContext_lowerError_signalModel[i]+2*bic_highContext_signalModel[i]],\n",
    "                              [2*bic_highContext_upperError_signalModel[i]-2*bic_highContext_signalModel[i]]],\n",
    "                         color='brown',marker='o')\n",
    "        ax[1,2].errorbar((0.5-pcategory_highContext[i])*2,posteriorNorm_highContext[i], \n",
    "                         xerr=[[-pcategory_highContext_lowerError[i]+pcategory_highContext[i]],\n",
    "                              [pcategory_highContext_upperError[i]-pcategory_highContext[i]]],\n",
    "                         yerr=[[-posteriorNorm_highContext_lowerError[i]+posteriorNorm_highContext[i]],\n",
    "                              [posteriorNorm_highContext_upperError[i]-posteriorNorm_highContext[i]]],\n",
    "                         color='brown',marker='o')\n",
    "        \n",
    "ax[0,1].plot(np.arange(200,1400),np.arange(200,1400),'k--') \n",
    "ax[0,1].set_xticks([300,600,900,1200])\n",
    "ax[0,1].set_xticklabels([300,600,900,1200])\n",
    "ax[0,1].set_yticks([300,600,900,1200])\n",
    "ax[0,1].set_yticklabels([300,600,900,1200])\n",
    "ax[1,1].plot(np.arange(200,1201),np.arange(200,1201),'k--')\n",
    "ax[1,1].set_xticks([300,600,900,1200])\n",
    "ax[1,1].set_xticklabels([300,600,900,1200])\n",
    "ax[1,1].set_yticks([300,600,900,1200])\n",
    "ax[1,1].set_yticklabels([300,600,900,1200])\n",
    "ax[0,1].set_xlabel('BIC of Signal Strategy', fontsize=22)\n",
    "ax[0,1].set_ylabel('BIC of Probabilistic Strategy',fontsize=22)\n",
    "ax[0,1].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[0,1].spines['top'].set_visible(False)\n",
    "ax[0,1].spines['right'].set_visible(False)\n",
    "ax[0,2].set_xlabel('Modeled Internalized Bias',fontsize=22)\n",
    "ax[0,2].set_ylabel('Relevance Metric',fontsize=22)\n",
    "ax[0,2].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[0,2].spines['top'].set_visible(False)\n",
    "ax[0,2].spines['right'].set_visible(False)\n",
    "ax[0,2].set_xticks([0,0.2,0.4,0.6])\n",
    "ax[1,1].set_xlabel('BIC of Signal Strategy', fontsize=22)\n",
    "ax[1,1].set_ylabel('BIC of Probabilistic Strategy',fontsize=22)\n",
    "ax[1,1].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[1,1].spines['top'].set_visible(False)\n",
    "ax[1,1].spines['right'].set_visible(False)\n",
    "ax[1,2].set_xlabel('Modeled Internalized Bias',fontsize=22)\n",
    "ax[1,2].set_ylabel('Relevance Metric',fontsize=22)\n",
    "ax[1,2].tick_params(axis='both',labelsize=20,length=6,width=2)\n",
    "ax[1,2].set_xticks([0,0.2,0.4,0.6])\n",
    "ax[1,2].spines['top'].set_visible(False)\n",
    "ax[1,2].spines['right'].set_visible(False)\n",
    "    \n",
    "plt.savefig('figures/FromProlific/illustrations/uncertaintyVsRelevance_longContext.eps',bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "pback_lowContextForcomparison = np.array([0.68,0.74,0.25,0.68,0.053,0.83,0.49,0.25,0.52,0.68,0.67,0.48,0.88,0.66,0.87,\n",
    "                                         0.69,0.88,0.67,0.25,0.93,0.46,0.74,0.26,0.4,0.68,0.67,0.25,0.53,0.85,0.69,0.48,\n",
    "                                         0.44,0.67,0.067,0.64,0.26,0.05,0.45,0.81,0.66,0.68,0.47,0.053,0.05])\n",
    "pback_noForComparisonToLowContext = np.array([0.24,0.91,0.56,0.74,0.26,0.65,0.88,0.48,0.27,0.73,0.49,0.92,0.84,0.65,0.77,\n",
    "                                              0.68,0.45,0.24,0.77,0.84,0.25,0.7,0.46,0.86,0.66,0.27,0.86,0.85,0.87,0.89,\n",
    "                                              0.67,0.66,0.88,0.89,0.67,0.89,0.26,0.66,0.85,0.69,0.46,0.46,0.67,0.47])\n",
    "\n",
    "bic_lowContext_randomModel.mask[[25,45]] = True\n",
    "bic_lowContext_lesionModel.mask[[25,45]] = True\n",
    "bic_lowContext_signalModel.mask[[25,45]] = True\n",
    "bic_highContext_randomModel.mask[[20,40]] = True\n",
    "bic_highContext_lesionModel.mask[[20,40]] = True\n",
    "bic_highContext_signalModel.mask[[20,40]] = True\n",
    "size_lowContext.mask[[25,45]] = True\n",
    "size_highContext.mask[[20,40]] = True\n",
    "\n",
    "pcategory_lowContext.mask[[25,45]] = True\n",
    "pback_lowContext.mask[[25,45]] = True\n",
    "posteriorNorm_lowContext.mask[[25,45]] = True\n",
    "pcategory_highContext.mask[[20,40]] = True\n",
    "pback_highContext.mask[[20,40]] = True\n",
    "posteriorNorm_highContext.mask[[20,40]] = True\n",
    "\n",
    "print(\"Median and IQR of random model BICs low context\", \n",
    "      np.median(2*bic_lowContext_randomModel+2*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_randomModel+2*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of random model BICs high context\", \n",
    "      np.median(2*bic_highContext_randomModel+2*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_randomModel+2*np.log(size_highContext)))\n",
    "print(\"Median and IQR of lesion model BICs low context\", \n",
    "      np.median(2*bic_lowContext_lesionModel+5*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_lesionModel+5*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of lesion model BICs high context\", \n",
    "      np.median(2*bic_highContext_lesionModel+5*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_lesionModel+5*np.log(size_highContext)))\n",
    "print(\"Median and IQR of signal model BICs low context\", \n",
    "      np.median(2*bic_lowContext_signalModel[::2]+6*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_signalModel[::2]+6*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of signal model BICs high context\", \n",
    "      np.median(2*bic_highContext_signalModel+6*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_signalModel+6*np.log(size_highContext)))\n",
    "print(\"Comparing random and signal model BICs low context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_lowContext_randomModel+2*np.log(size_lowContext), \n",
    "                  2*bic_lowContext_signalModel[::2]+6*np.log(size_lowContext)))\n",
    "print(\"Comparing random and signal model BICs high context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_highContext_randomModel+2*np.log(size_highContext),\n",
    "                  2*bic_highContext_signalModel+6*np.log(size_highContext)))\n",
    "print(\"Comparing Lesion and signal model BICs low context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_lowContext_lesionModel+5*np.log(size_lowContext),\n",
    "                  2*bic_lowContext_signalModel[::2]+6*np.log(size_lowContext)))\n",
    "print(\"Comparing Lesion and signal model BICs high context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_highContext_lesionModel+5*np.log(size_highContext),\n",
    "                  2*bic_highContext_signalModel+6*np.log(size_highContext)))\n",
    "\n",
    "print(\"Correlation between plow and pdistractor in low context\",\n",
    "      pg.corr((pcategory_lowContext-0.5)*2,pback_lowContext[::2],method='spearman'))\n",
    "print(\"Correlation between plow and pdistractor in high context\",\n",
    "      pg.corr((-pcategory_highContext+0.5)*2,pback_highContext,method='spearman'))\n",
    "print(\"Correlation between plow and relevance metric in low context\",\n",
    "      pg.corr((pcategory_lowContext-0.5)*2,posteriorNorm_lowContext,method='spearman'))\n",
    "print(\"Correlation between plow and relevance metric in high context\",\n",
    "      pg.corr((0.5-pcategory_highContext)*2,posteriorNorm_highContext,method='spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a829214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Model free analysis of short term effects of bias.\n",
    "\"\"\"\n",
    "\n",
    "fig,[[ax1,ax2],[ax3,ax4]] = plt.subplots(2,2,figsize=(15,15))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=None)\n",
    "\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext)*2,\n",
    "         (internalizedBias['ExpectationOfPriorCategoryLowContextLowGaussianTrials']*internalizedBias['NumberOfLowContextLowGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*internalizedBias['NumberOfSimpleTrialsLowContextLL'])*100/\n",
    "         (internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL']),\n",
    "         'go',markersize=10)\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextLH']*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext)*2,\n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext)*2,\n",
    "         (internalizedBias['ExpectationOfPriorCategoryLowContextHighGaussianTrials']*internalizedBias['NumberOfLowContextHighGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*internalizedBias['NumberOfSimpleTrialsLowContextHH'])*100/\n",
    "         (internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH']),\n",
    "         'go',markersize=10)\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextHL']*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "print(\"average accuracy in low context when both trials are low but current stimulus has one distractor\",\n",
    "     np.mean((internalizedBias['ExpectationOfPriorCategoryLowContextLowGaussianTrials']*internalizedBias['NumberOfLowContextLowGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*internalizedBias['NumberOfSimpleTrialsLowContextLL'])*100/\n",
    "         (internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL'])))\n",
    "print(\"std of accuracy in low context when both trials are low but current stimulus has one distractor\",\n",
    "     np.std((internalizedBias['ExpectationOfPriorCategoryLowContextLowGaussianTrials']*internalizedBias['NumberOfLowContextLowGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*internalizedBias['NumberOfSimpleTrialsLowContextLL'])*100/\n",
    "         (internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL'])))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL']),\n",
    "      np.std(internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL']))\n",
    "print(\"average accuracy in low context when both trials are high but current stimulus has one distractor\",\n",
    "     np.mean((internalizedBias['ExpectationOfPriorCategoryLowContextHighGaussianTrials']*internalizedBias['NumberOfLowContextHighGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*internalizedBias['NumberOfSimpleTrialsLowContextHH'])*100/\n",
    "         (internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH'])))\n",
    "print(\"std of accuracy in low context when both trials are high but current stimulus has one distractor\",\n",
    "     np.std((internalizedBias['ExpectationOfPriorCategoryLowContextHighGaussianTrials']*internalizedBias['NumberOfLowContextHighGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*internalizedBias['NumberOfSimpleTrialsLowContextHH'])*100/\n",
    "         (internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH'])))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH']),\n",
    "      np.std(internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH']))\n",
    "\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextLL'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryLowContextLowGaussianTrials']*internalizedBias['NumberOfLowContextLowGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*internalizedBias['NumberOfSimpleTrialsLowContextLL'])/\n",
    "         (internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL']),\n",
    "             method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextLH'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextHH'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryLowContextHighGaussianTrials']*internalizedBias['NumberOfLowContextHighGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*internalizedBias['NumberOfSimpleTrialsLowContextHH'])/\n",
    "         (internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH']),\n",
    "             method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextHL'], method='spearman'))\n",
    "\n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         (internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])]),\n",
    "         'go',markersize=10)\n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextHL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHL'])]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         (internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])]),\n",
    "         'go',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextLH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLH'])]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "print(\"average accuracy in high context when both trials are high but current stimulus has one distractor\",\n",
    "     np.mean((internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])))\n",
    "print(\"std of accuracy in high context when both trials are high but current stimulus has one distractor\",\n",
    "     np.std((internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "              -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])]),\n",
    "      np.std(internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "              -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])]))\n",
    "\n",
    "print(\"average accuracy in high context when both trials are low but current stimulus has one distractor\",\n",
    "     np.mean((internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])))\n",
    "print(\"std of accuracy in high context when both trials are low but current stimulus has one distractor\",\n",
    "     np.std((internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "              -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])]),\n",
    "      np.std(internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "              -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])]))\n",
    "\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextHighGaussianTrials'])]\n",
    "          *internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]\n",
    "          *internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])])*100/\n",
    "         (internalizedBias['NumberOfHighContextHighGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextHighGaussianTrials'])]\n",
    "          -internalizedBias['NumberOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextHH'])]),\n",
    "             method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextHL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHL'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'][~np.isnan(internalizedBias['ExpectationOfPriorCategoryHighContextLowGaussianTrials'])]\n",
    "              *internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "             -internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]\n",
    "              *internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])])*100/\n",
    "            (internalizedBias['NumberOfHighContextLowGaussianTrials'][~np.isnan(internalizedBias['NumberOfHighContextLowGaussianTrials'])]\n",
    "              -internalizedBias['NumberOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['NumberOfSimpleTrialsHighContextLL'])]),\n",
    "              method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextLH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLH'])], method='spearman'))\n",
    "\n",
    "\n",
    "ax1.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax1.set_ylabel('Accuracy',fontsize=25)\n",
    "ax1.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax1.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax1.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax2.set_ylabel('Accuracy',fontsize=25)\n",
    "ax2.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax2.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax2.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax3.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax3.set_ylabel('Accuracy',fontsize=25)\n",
    "ax3.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax3.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax3.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax4.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax4.set_ylabel('Accuracy',fontsize=25)\n",
    "ax4.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax4.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax4.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax4.spines['top'].set_visible(False)\n",
    "ax4.spines['right'].set_visible(False)\n",
    "\n",
    "plt.savefig('figures/FromProlific/illustrations/biasEffectsOnTrialPairs_longContext.eps',bbox_inches=\"tight\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-bargain",
   "metadata": {},
   "source": [
    "\n",
    "## From here on Discarded code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Qs: are accuracy and bias correlated in the no context case?\n",
    "\"\"\"\n",
    "accuracyNo = np.array([81.5,73.5,82.6,82.8,67.8,82.3,74.9,81.2,79.2,74.9,78.1,80.5,79.1,84.5,83,\n",
    "                       76.7,85.7,72.3,80.5,85.1,74.3,79.7,76.7,68.9,72,78.8,86,84.6,83.6])\n",
    "biasNo = np.array([0.49,0.57,0.49,0.49,0.49,0.48,0.51,0.53,0.49,0.51,0.53,0.48,0.53,0.48,\n",
    "                   0.46,0.49,0.5,0.46,0.51,0.47,0.54,0.52,0.47,0.44,0.44,0.46,0.48,0.49,0.49])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(biasNo,accuracyNo,'k.')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Internalized Bias',fontsize=14)\n",
    "plt.ylabel('Accuracy',fontsize=14)\n",
    "#plt.savefig('figures/FromProlific/illustrations/accuracyVsBiasNoContext',bbox_inches=\"tight\")\n",
    "\n",
    "\"\"\"\n",
    "Qs: Are strategies the same in no and long context?\n",
    "\"\"\"\n",
    "\n",
    "noContextSubjectsSigMean = ['735a','1c3f','1396','2099']\n",
    "noContextSubjectsVoting = ['5b34','c2e0','8de3','6b7f','1604','801d','54db','b4c7','a12e','c653','4b7f',\n",
    "                          'fc3a','1570','592d','a45d','0cf1','214b','b7cc','6ced','188a','e12e']\n",
    "noContextSubjectsMean = ['d619','1304','0d04','e453']\n",
    "\n",
    "lowContextSubjectsAll = ['d619','c2e0','735a','1304','6b7f','1c3f','1604','801d','1396','b4c7','a12e',\n",
    "                         'c653','4b7f','fc3a','0d04','2099','592d','a45d','0cf1','e453','214b',\n",
    "                            'b7cc','6ced','188a','e12e']\n",
    "lowContextSubjectsSigMean = ['d619','c2e0','735a','1304','1c3f','c653','0d04','2099','592d','214b','e12e']\n",
    "lowContextSubjectsVoting = ['6b7f','801d','1396','b4c7','a12e','4b7f','fc3a','1570','a45d','0cf1','e453',\n",
    "                            'b7cc','6ced','188a']\n",
    "lowContextSubjectsMean = ['1604']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for iSubject in noContextSubjectsSigMean:\n",
    "    if iSubject in lowContextSubjectsSigMean:\n",
    "        ax.plot(1,np.random.randn()*0.1+1,'b.')\n",
    "    elif iSubject in lowContextSubjectsVoting:\n",
    "        ax.plot(1,np.random.randn()*0.1+2,'b.')\n",
    "    elif iSubject in lowContextSubjectsMean:\n",
    "        ax.plot(1,np.random.randn()*0.1+3,'b.')\n",
    "        \n",
    "for iSubject in noContextSubjectsVoting:\n",
    "    if iSubject in lowContextSubjectsSigMean:\n",
    "        ax.plot(2,np.random.randn()*0.1+1,'b.')\n",
    "    elif iSubject in lowContextSubjectsVoting:\n",
    "        ax.plot(2,np.random.randn()*0.1+2,'b.')\n",
    "    elif iSubject in lowContextSubjectsMean:\n",
    "        ax.plot(2,np.random.randn()*0.1+3,'b.')\n",
    "        \n",
    "for iSubject in noContextSubjectsMean:\n",
    "    if iSubject in lowContextSubjectsSigMean:\n",
    "        ax.plot(3,np.random.randn()*0.1+1,'b.')\n",
    "    elif iSubject in lowContextSubjectsVoting:\n",
    "        ax.plot(3,np.random.randn()*0.1+2,'b.')\n",
    "    elif iSubject in lowContextSubjectsMean:\n",
    "        ax.plot(3,np.random.randn()*0.1+3,'b.')\n",
    "ax.set_xticks([1,2,3])\n",
    "ax.set_xticklabels(['Mean of Signal','Voting','Mean of all'],fontsize=14)\n",
    "ax.set_yticks([1,2,3])\n",
    "ax.set_yticklabels(['Mean of Signal','Voting','Mean of all'],fontsize=14)\n",
    "\n",
    "\"\"\"\n",
    "Qs:how do strategies and bias interplay in the long context scenarios?\n",
    "\"\"\"\n",
    "\n",
    "fig, [ax1,ax2] = plt.subplots(1,2,sharex=True,figsize=[20,5])\n",
    "for iSubject in range(len(lowContextSubjectsAll)):\n",
    "    if lowContextSubjectsAll[iSubject] in lowContextSubjectsSigMean:\n",
    "        ax1.plot(1,0.5-biasLow[iSubject],'b.')\n",
    "        ax2.plot(1,accuracyLow[iSubject],'b*')\n",
    "    elif lowContextSubjectsAll[iSubject] in lowContextSubjectsVoting:\n",
    "        ax1.plot(2,0.5-biasLow[iSubject],'b.')\n",
    "        ax2.plot(2,accuracyLow[iSubject],'b*')\n",
    "    elif lowContextSubjectsAll[iSubject] in lowContextSubjectsMean:\n",
    "        ax1.plot(3,0.5-biasLow[iSubject],'b.')\n",
    "        ax2.plot(3,accuracyLow[iSubject],'b*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following variables are for bic values of the entire dataset.\n",
    "\"\"\"\n",
    "bic_meanModel = 2*np.array([311.78,296,366.67,362.62,381.75,304.45,290.95,381.91,287.53,293.13,385,254.11,288.05,\n",
    "                 248.98,337.35,295.97,305.18,378.12,344.81,413.92,329.65,331.31,280.72,310.66,354.75,258.69,\n",
    "                318.33,312.08,401.44])\n",
    "bic_signalModel = 2*np.array([227.51, 277.09,222.37,203.26, 367.84,233.56,264.95,229.66,243.16,263.38,267.19,\n",
    "                           204.64,256.01,170.87,200.08,268.11,176.17,301.37,218.84,184.93,287.37,238.48,238.84,\n",
    "                           309.79,323.45,219.93,157.15,178.92,202.52])\n",
    "bic_votingModel = 2*np.array([233.22,280.1,245.63,233.77,369.2,234.79,265.85,243.82,249.72,265.95,285.31,203.48,\n",
    "                           258.35,173.28,220.8,267.46,185.68,315.17,232.01,230.61,286.29,249.47,249.77,309.9,\n",
    "                            322.7,217.33,152.39,171.85,195.91]) \n",
    "\n",
    "percentageDataExplainedByMean = np.array([68.5,73.6,67.7,64.2,64,70.1,74.9,62.4,75.3,76.8,61.3,79.4,72.5,\n",
    "                                          74.2,68.7,74.2,71.4,71.4,67.3,61.1,71.8,71.6,76.4,78.5,67.5,75.9,\n",
    "                                          71.7,74.1,66.8])\n",
    "percentageDataExplainedBySig = np.array([74.2,61.5,74.3,78,59.6,75.1,64.3,82.4,64.6,61,73.8,64.7,67.2,71.1,\n",
    "                                         73,63.8,75.5,57.9,76,82.1,66.4,63.5,64.2,53.1,68.1,66.8,73,70.5,77.2])\n",
    "percentageDataExplainedByVoting = np.array([80.5,73.5,79.4,76.9,61.5,82.2,81.9,79.2,78.2,79,72.5,83.8,79.9,\n",
    "                                            89.5,80.8,79.5,85.7,70.9,78.7,79.1,78.7,78.5,76.7,72.7,74,81.5,90.6,\n",
    "                                            89.8,88.5])\n",
    "\n",
    "fig,[ax1,ax2] = plt.subplots(1,2,figsize=(20,5))\n",
    "ax1.plot(bic_meanModel+5*np.log(600),bic_signalModel+6*np.log(600),'ko')\n",
    "ax1.plot(bic_meanModel[23]+5*np.log(600),bic_signalModel[23]+6*np.log(600),marker='$\\\\bigoplus$',color='royalblue')\n",
    "ax1.plot(bic_meanModel[19]+5*np.log(600),bic_signalModel[19]+6*np.log(600),marker='$\\\\bigoplus$',color='red')\n",
    "ax1.plot(bic_meanModel[3]+5*np.log(600),bic_signalModel[3]+6*np.log(600),marker='$\\\\bigoplus$',color='red')\n",
    "ax1.plot(np.arange(450,850),np.arange(450,850),'k--')\n",
    "#ax1.hlines(0,-5,250,'k',linestyles='--')\n",
    "#ax1.vlines(0,-30,30,'k',linestyles='--')\n",
    "ax1.set_xlabel('BIC Lesion Model', fontsize=20)\n",
    "ax1.set_ylabel('BIC Signal Model',fontsize=20)\n",
    "ax1.tick_params(axis='both',labelsize=20)\n",
    "\n",
    "ax2.plot(bic_votingModel+5*np.log(600),bic_signalModel+6*np.log(600),'ko')\n",
    "ax2.plot(bic_votingModel[-1]+5*np.log(600),bic_signalModel[-1]+6*np.log(600),marker='$\\\\bigoplus$',color='limegreen')\n",
    "ax2.plot(bic_votingModel[-2]+5*np.log(600),bic_signalModel[-2]+6*np.log(600),marker='$\\\\bigoplus$',color='limegreen')\n",
    "ax2.plot(bic_votingModel[-3]+5*np.log(600),bic_signalModel[-3]+6*np.log(600),marker='$\\\\bigoplus$',color='limegreen')\n",
    "ax2.plot(bic_votingModel[19]+5*np.log(600),bic_signalModel[19]+6*np.log(600),marker='$\\\\bigoplus$',color='red')\n",
    "ax2.plot(bic_votingModel[3]+5*np.log(600),bic_signalModel[3]+6*np.log(600),marker='$\\\\bigoplus$',color='red')\n",
    "ax2.plot(bic_votingModel[10]+5*np.log(600),bic_signalModel[10]+6*np.log(600),marker='$\\\\bigoplus$',color='red')\n",
    "\n",
    "ax2.plot(np.arange(300,800),np.arange(300,800),'k--')\n",
    "ax2.set_xlabel('BIC Voting Model', fontsize=20)\n",
    "ax2.set_ylabel('BIC Signal Model',fontsize=20)\n",
    "ax2.tick_params(axis='both',labelsize=20)\n",
    "#plt.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategy',bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: how is accuracy correlated with pback? For this I am using the pback calculated on the bootstrap sampled\n",
    "dataset.\n",
    "\"\"\"\n",
    "fig,ax = plt.subplots(1,1,figsize=(10,5))\n",
    "for iSubj in range(len(pbackCalcuatedOnSubsampledDataset)//2):\n",
    "    ax.plot(OneIrrelevantToneAccuracy[iSubj],TwoIrrelevantTonesAccuracy[iSubj],marker='o',\n",
    "            color=[pbackCalcuatedOnSubsampledDataset[iSubj*2],pbackCalcuatedOnSubsampledDataset[iSubj*2],\n",
    "                   pbackCalcuatedOnSubsampledDataset[iSubj*2]])\n",
    "ax.set_xlabel('Accuracy for trials with one irrelevant experimental tone',fontsize=18)\n",
    "ax.set_ylabel('Accuracy for trials with two \\n irrelevant experimental tones',fontsize=18)\n",
    "ax.tick_params(axis='both',labelsize=15)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.axes(projection='3d')\n",
    "for iSubj in range(len(pbackCalcuatedOnSubsampledDataset)//2):\n",
    "    ax.plot3D(OneIrrelevantToneAccuracy[iSubj],TwoIrrelevantTonesAccuracy[iSubj],\n",
    "              pbackCalcuatedOnSubsampledDataset[iSubj*2],'ko')\n",
    "ax.set_xlabel(\"Accuracy for trials with \\n one irrelevant experimental tone\",fontsize=10)\n",
    "ax.set_ylabel('Accuracy for trials with two \\n irrelevant experimental tones',fontsize=10)\n",
    "ax.set_zlabel('P$_{distractor}$',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcategory_lowContext.mask[[0,1,2,7,19]] = True\n",
    "print(pcategory_lowContext)\n",
    "fig, ax = plt.subplots(2,2,figsize=(22,20))\n",
    "ax[0,0].plot(pcategory_lowContext[~pcategory_lowContext.mask]-0.5, \n",
    "             0.5-pcategory_highContext[~pcategory_highContext.mask],'.')\n",
    "ax[0,0].plot(np.arange(0,0.4,0.05),np.arange(0,0.4,0.05),'k--')\n",
    "ax[0,0].set_xlabel(\"Low context bias\")\n",
    "ax[0,0].set_ylabel(\"High context bias\")\n",
    "ax[0,0].tick_params(axis='both',labelsize=23)\n",
    "\n",
    "ll_lowContext_signalModel = np.ma.array(computedLikelihoodsLowContext['likelihood'].values,mask=False)\n",
    "ll_lowContext_signalModel = ll_lowContext_signalModel[~numpy.isnan(ll_lowContext_signalModel)]\n",
    "\n",
    "plow_signalModel = np.ma.array(computedLikelihoodsLowContext['plow'].values,mask=False)\n",
    "plow_signalModel = plow_signalModel[~numpy.isnan(plow_signalModel)]\n",
    "\n",
    "ss_signalModel = np.ma.array(computedLikelihoodsLowContext['ss'].values,mask=False)\n",
    "ss_signalModel = ss_signalModel[~numpy.isnan(ss_signalModel)]\n",
    "\n",
    "ax[0,1].plot(ss_signalModel[::2], ss_signalModel[1::2], '.')\n",
    "ax[0,1].plot(np.arange(0.1,0.6,0.05),np.arange(0.1,0.6,0.05),'k--')\n",
    "ax[0,1].set_xlabel(\"ss constrained\")\n",
    "ax[0,1].set_ylabel(\"ss minimum\")\n",
    "ax[0,1].tick_params(axis='both',labelsize=23)\n",
    "\n",
    "ax[1,0].plot(plow_signalModel[::2]-0.5, 0.5-biasLowForSubjectsWithNoAndLowContexts,'.')\n",
    "ax[1,0].plot(np.arange(0,0.4,0.05),np.arange(0,0.4,0.05),'k--')\n",
    "ax[1,0].tick_params(axis='both',labelsize=23)\n",
    "\n",
    "ax[1,1].plot(plow_signalModel[1::2]-0.5, 0.5-biasLowForSubjectsWithNoAndLowContexts,'.')\n",
    "ax[1,1].plot(np.arange(0,0.4,0.05),np.arange(0,0.4,0.05),'k--')\n",
    "ax[1,1].tick_params(axis='both',labelsize=23)\n",
    "\n",
    "print(pg.wilcoxon(plow_signalModel[::2]-0.5, 0.5-biasLowForSubjectsWithNoAndLowContexts))\n",
    "print(pg.wilcoxon(plow_signalModel[1::2]-0.5, 0.5-biasLowForSubjectsWithNoAndLowContexts))\n",
    "print(\"median of difference in ss:\", np.mean(np.abs(ss_signalModel[::2]-ss_signalModel[1::2])))\n",
    "plt.figure()\n",
    "plt.hist(np.abs(ss_signalModel[::2]-ss_signalModel[1::2]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sklearn\n",
    "import pdb\n",
    "import scipy\n",
    "from scipy.optimize import minimize, fmin\n",
    "from scipy.stats import multivariate_normal\n",
    "import xlrd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "import pingouin as pg\n",
    "import statsmodels.api as sm\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.sans-serif'] = ['tahoma']\n",
    "def makeAxesPretty(ax):\n",
    "    for axis in ['bottom','left']:\n",
    "        ax.spines[axis].set_linewidth(2)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Obtaining data from a given long context expt\n",
    "\"\"\"\n",
    "Test = pd.read_csv('subjectDataForPlots/allTrials_noBias.csv')\n",
    "Data = pd.read_csv('subjectDataForPlots/noContextData/601306071a72651244570d04_categorization_task_2021-05-18_04h46.56.555.csv');\n",
    "\n",
    "TestLc = pd.read_csv('subjectDataForPlots/allTrials_lowContext.csv')\n",
    "DataLc = pd.read_csv('subjectDataForPlots/biasedLowContextData/601306071a72651244570d04_categorization_task_longLow_2021-06-09_03h41.33.989.csv');\n",
    "\n",
    "TestHc = pd.read_csv('subjectDataForPlots/allTrials_highContext.csv')\n",
    "DataHc = pd.read_csv('subjectDataForPlots/biasedHighContextData/601306071a72651244570d04_categorization_task_longHigh_2021-06-16_23h31.15.482.csv');\n",
    "\n",
    "xls = pd.ExcelFile('subjectDataForPlots/clusterResultsForSubsampledData.xls')\n",
    "computedLikelihoods = pd.read_excel(xls,'NoContextModelFits')\n",
    "computedLikelihoodsVeridical = pd.read_excel(xls,'NoContextModelFits_veridicalPar')\n",
    "computedLikelihoodsLowContext = pd.read_excel(xls,'LowContextModelFits')\n",
    "computedLikelihoodsLowContextVeridical = pd.read_excel(xls,'BiasedLowModelFits_veridicalPar')\n",
    "computedLikelihoodsHighContext = pd.read_excel(xls,'HighContextModelFits')\n",
    "computedLikelihoodsHighContextVeridical = pd.read_excel(xls,'BiasedHighModelFits_veridicalPa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_freq_seq = np.arange(90,3000,1) #array of possible true tones\n",
    "expt_log_freq_seq_array = np.arange(np.log10(expt_freq_seq[0]), np.log10(expt_freq_seq[-1]), \n",
    "                                    np.log10(1003/1000)*40)\n",
    "log_freq_seq_mid = np.median(expt_log_freq_seq_array)\n",
    "log_freq_percept = np.arange(0.6,4.7,0.1)\n",
    "log_freq_seq_array = np.arange(0.6,4.7,0.1)\n",
    "log_freq_low = [log_freq_seq_mid - 0.15,0.1]  #low freq condition is gaussian \n",
    "log_freq_high = [log_freq_seq_mid + 0.15,0.1] \n",
    "\n",
    "def extractData(csv_test, csv_data, exptTotalLength, exptLengthWithBreaks):  \n",
    "    n_tones=3\n",
    "    n_trials = csv_data.shape[0]-47\n",
    "\n",
    "    test_columns = list(csv_test.columns)\n",
    "    test_tones_name = test_columns.index('Name')\n",
    "    test_tones_col_idx = test_columns.index('Tones')\n",
    "    test_toneKind_col_idx = test_columns.index('Tonekind')\n",
    "    df_names = (csv_test.iloc[0:exptTotalLength,test_tones_name]).values\n",
    "    df_tones = (csv_test.iloc[0:exptTotalLength,test_tones_col_idx]).values\n",
    "    df_toneKind = (csv_test.iloc[0:exptTotalLength,test_toneKind_col_idx]).values\n",
    "\n",
    "    tones_array_orig = np.zeros((n_trials,n_tones))\n",
    "    toneKind_array_orig = np.zeros((n_trials,n_tones))\n",
    "    tones_array_idxs_keep = []\n",
    "\n",
    "    for i_wav in range(exptLengthWithBreaks):\n",
    "        if isinstance(csv_data['Name'][i_wav+46],str):\n",
    "            tones_array_orig[i_wav,:] = np.array(df_tones[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                              ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "            toneKind_array_orig[i_wav,:] = np.array(df_toneKind[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                              ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "            tones_array_idxs_keep += [i_wav]\n",
    "\n",
    "\n",
    "    exptTones = np.copy(tones_array_orig[tones_array_idxs_keep,:])\n",
    "    exptToneKind = np.copy(toneKind_array_orig[tones_array_idxs_keep,:])\n",
    "    exptCorrans = np.copy(csv_data['corrAns'][46:csv_data.shape[0]])[tones_array_idxs_keep]\n",
    "    exptKeys = np.copy(csv_data['test_resp.keys'][46:csv_data.shape[0]])[tones_array_idxs_keep]\n",
    "    \n",
    "    return exptTones, exptToneKind, exptCorrans, exptKeys\n",
    "\n",
    "def identifyResponseTrials(tonesPlayed,tonesSignalOrDistractor,correctAns,keysPressed,\n",
    "                           exptTotalLength):\n",
    "    no_response = np.intersect1d(np.where(keysPressed!='h')[0],\n",
    "                                 np.where(keysPressed!='l')[0])\n",
    "    #print(\"Did not respond to: \",no_response)\n",
    "\n",
    "    \"\"\"\n",
    "    Convert keys ['l','h'] to [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    corrans_num_orig = np.zeros_like(correctAns)\n",
    "    corrans_num_orig[correctAns == 'h'] = 1\n",
    "\n",
    "    keys_num_orig = np.zeros_like(keysPressed)\n",
    "    keys_num_orig[keysPressed == 'h'] = 1\n",
    "\n",
    "    corrans_num = corrans_num_orig[:exptTotalLength]\n",
    "    keys_num = keys_num_orig[:exptTotalLength]\n",
    "    tones_array = tonesPlayed[:exptTotalLength]\n",
    "    tonesType_array = tonesSignalOrDistractor[:exptTotalLength]\n",
    "\n",
    "    trial_tones = np.repeat(tones_array,1,axis = 0)\n",
    "    trial_behaviour = np.reshape(keys_num,np.prod(keys_num.shape)) \n",
    "    idxs_with_response = np.delete(np.arange(len(trial_tones)),no_response)\n",
    "    trialTonesResponded = trial_tones[idxs_with_response,:]\n",
    "    trialBehaviourResponded = trial_behaviour[idxs_with_response]\n",
    "    corransResponded = corrans_num[idxs_with_response]\n",
    "    tonesTypeResponded = tonesType_array[idxs_with_response]\n",
    "    #print(f\"Total trials played are {len(trial_tones)}, and total trials responded to are {len(trialTonesResponded)}\")\n",
    "    #print(\"Got correct: \", np.sum(trialBehaviourResponded==corransResponded)/len(trialTonesResponded))\n",
    "    #print(\"No. of minority category correct: \", np.sum(keys_num*corrans_num)/np.sum(corrans_num))\n",
    "    \n",
    "    return trialTonesResponded, tonesTypeResponded, corransResponded, trialBehaviourResponded \n",
    "\n",
    "def plottingInfluenceFn(tones, behaviour):\n",
    "    unique_tones = np.unique(tones)\n",
    "\n",
    "    tone1_prob_behaviour = np.zeros((len(unique_tones),1))\n",
    "    tone2_prob_behaviour = np.zeros((len(unique_tones),1))\n",
    "    tone3_prob_behaviour = np.zeros((len(unique_tones),1))\n",
    "\n",
    "    for i_tone in range(len(unique_tones)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,0]==unique_tones[i_tone]])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,1]==unique_tones[i_tone]])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(behaviour[tones[:,2]==unique_tones[i_tone]])\n",
    "    behaviour = np.concatenate((tone1_prob_behaviour,tone2_prob_behaviour,tone3_prob_behaviour),axis=1)\n",
    "    return unique_tones, behaviour    \n",
    "\n",
    "def gaussian(x, mean, sigma):\n",
    "    return np.exp(-(x-mean)**2/(2*sigma**2))\n",
    "\n",
    "def Tones3dgrid(latentTones, sigma):\n",
    "    \n",
    "    input_array_0 = np.expand_dims(gaussian(log_freq_percept, latentTones[0], sigma), axis = 1)\n",
    "    input_array_1 = np.expand_dims(gaussian(log_freq_percept, latentTones[1], sigma), axis = 1)\n",
    "    input_array_2 = np.expand_dims(gaussian(log_freq_percept, latentTones[2], sigma), axis = 1)\n",
    "    s0 = 1/np.sum(input_array_0); s1 = 1/np.sum(input_array_1); s2 = 1/np.sum(input_array_2)\n",
    "    input_array_0 *= s0; input_array_1 *= s1; input_array_2 *= s2; \n",
    "    \n",
    "    input_array_mat = np.expand_dims(input_array_0@input_array_1.T,axis=2)@(input_array_2.T) #p(T1,T2..|H)     \n",
    "    \n",
    "    return input_array_mat\n",
    "\n",
    "def posterior_array(freq_input, n_tones, p_back, log_prior):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    freq_input - range of all possible frequencies (percepts?)\n",
    "    p_back - prob of background\n",
    "    p_low - prob of low condition\n",
    "    log_prior - list of prior parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    log_prior_low_mean = log_prior[0]; log_prior_low_sigma = log_prior[2];\n",
    "    log_prior_high_mean = log_prior[1]; log_prior_high_sigma = log_prior[2];\n",
    "    likelihood_onetone_low = gaussian(x=freq_input, mean=log_prior_low_mean, sigma=log_prior_low_sigma)\n",
    "    likelihood_onetone_high = gaussian(x=freq_input, mean=log_prior_high_mean, sigma=log_prior_high_sigma)\n",
    "    likelihood_onetone_mixed_high = p_back*(1/len(freq_input)) + (1-p_back)*likelihood_onetone_high\n",
    "    #mixture model with p(T|B) = 1/no. of possible freqs\n",
    "    likelihood_onetone_mixed_high /= likelihood_onetone_mixed_high.sum() #normalizing\n",
    "    likelihood_onetone_mixed_high = np.expand_dims(likelihood_onetone_mixed_high, axis = 1)\n",
    "    likelihood_onetone_mixed_low = p_back*(1/len(freq_input)) + (1-p_back)*likelihood_onetone_low\n",
    "    #mixture model with p(T|B) = 1/no. of possible freqs\n",
    "    likelihood_onetone_mixed_low /= likelihood_onetone_mixed_low.sum() #normalizing\n",
    "    likelihood_onetone_mixed_low = np.expand_dims(likelihood_onetone_mixed_low, axis = 1)\n",
    "        \n",
    "    if n_tones == 3:\n",
    "        likelihood_alltones_low = (np.expand_dims(likelihood_onetone_mixed_low@np.transpose\n",
    "                                                 (likelihood_onetone_mixed_low),axis=2)\n",
    "                                   @np.transpose(likelihood_onetone_mixed_low))\n",
    "        #p(T1,T2..|L) \n",
    "        likelihood_alltones_high = (np.expand_dims(likelihood_onetone_mixed_high@np.transpose\n",
    "                                                 (likelihood_onetone_mixed_high),axis=2)\n",
    "                                    @np.transpose(likelihood_onetone_mixed_high))\n",
    "        #p(T1,T2..|H) \n",
    "    elif n_tones == 1:\n",
    "        likelihood_alltones_low = likelihood_onetone_mixed_low\n",
    "        likelihood_alltones_high = likelihood_onetone_mixed_high\n",
    "\n",
    "    return [likelihood_onetone_mixed_high, likelihood_onetone_mixed_low, \n",
    "            likelihood_alltones_high, likelihood_alltones_low]\n",
    "\n",
    "def task(n_trials = 10, n_tones = 3, p_low = 0.5, p_back = 0.3):\n",
    "    trial_tones = np.zeros((n_trials,n_tones))\n",
    "    dist_chosen = np.zeros((n_trials,))\n",
    "    kind_of_tones = np.zeros((n_trials,n_tones))\n",
    "    for trial in range(n_trials):\n",
    "        signal_rand = np.random.random()\n",
    "        low_dist = signal_rand < p_low #choosing true tone from either low or high condition\n",
    "        tones = np.zeros((n_tones,))\n",
    "        tone_kind = np.zeros((n_tones,))\n",
    "        for n_tone in range(n_tones):\n",
    "            signal_back = np.random.random()\n",
    "            background = signal_back < p_back #choosing background or true tone\n",
    "            if background:\n",
    "                nearest_log_tone = np.random.choice(expt_log_freq_seq_array) \n",
    "                tone_kind[n_tone] = 0\n",
    "            else: \n",
    "                if low_dist:\n",
    "                    tone = min(max(np.random.randn()*log_freq_low[1] + log_freq_low[0],\\\n",
    "                                   expt_log_freq_seq_array[0]),expt_log_freq_seq_array[-1])\n",
    "                    tone_kind[n_tone] = 1\n",
    "                else:\n",
    "                    tone = min(max(np.random.randn()*log_freq_high[1] + log_freq_high[0],\\\n",
    "                                   expt_log_freq_seq_array[0]),expt_log_freq_seq_array[-1])\n",
    "                    tone_kind[n_tone] = 2\n",
    "                nearest_log_tone = expt_log_freq_seq_array[np.argmin(np.abs(expt_log_freq_seq_array - tone))]\n",
    "            nearest_tone = expt_freq_seq[np.argmin(np.abs(expt_freq_seq - 10**nearest_log_tone))]\n",
    "            tones[n_tone] = nearest_tone\n",
    "        trial_tones[trial,:] = tones\n",
    "        dist_chosen[trial] = 1-int(low_dist)\n",
    "        kind_of_tones[trial,:] = tone_kind\n",
    "    return trial_tones, dist_chosen, kind_of_tones\n",
    "\n",
    "\n",
    "def generate_behaviour(sim_trial_tones, dist_chosen, reps, n_tones, \n",
    "                       log_prior_params,\n",
    "                       sigma_sensory, \n",
    "                       prob_back, \n",
    "                       Wconstant, W1, tau):    \n",
    "    all_trial_tones = np.empty((len(sim_trial_tones)*reps,n_tones))\n",
    "    all_trial_behaviour = np.zeros((len(sim_trial_tones)*reps,1))\n",
    "    prob_trial_behaviour = np.zeros((len(sim_trial_tones),1))\n",
    "    probability_sim_high = np.zeros((len(sim_trial_tones),1))\n",
    "\n",
    "    [_,_,LikelihoodLatentTonegivenHigh,LikelihoodLatentTonegivenLow] = \\\n",
    "    posterior_array(log_freq_seq_array, n_tones=len(sim_trial_tones[0]),\n",
    "                    p_back=prob_back, log_prior=log_prior_params)\n",
    "\n",
    "    LikelihoodPerceptgivenHigh = np.zeros((len(log_freq_percept),len(log_freq_percept),len(log_freq_percept)))\n",
    "    LikelihoodPerceptgivenLow = np.zeros((len(log_freq_percept),len(log_freq_percept),len(log_freq_percept)))\n",
    "\n",
    "    for itrue1 in range(len(log_freq_percept)):\n",
    "        for itrue2 in range(len(log_freq_percept)):\n",
    "            for itrue3 in range(len(log_freq_percept)):\n",
    "                probPerceptgivenLatentTones = Tones3dgrid([log_freq_percept[itrue1],\n",
    "                                                           log_freq_percept[itrue2],\n",
    "                                                           log_freq_percept[itrue3]],sigma=sigma_sensory)\n",
    "                LikelihoodPerceptgivenHigh \\\n",
    "                += probPerceptgivenLatentTones * LikelihoodLatentTonegivenHigh[itrue1,itrue2,itrue3]\n",
    "                LikelihoodPerceptgivenLow \\\n",
    "                += probPerceptgivenLatentTones * LikelihoodLatentTonegivenLow[itrue1,itrue2,itrue3]\n",
    "                \n",
    "    all_trial_tones[0,:] = sim_trial_tones[0]  \n",
    "    for i_stim in range(1,len(sim_trial_tones)):\n",
    "        arePrevTrialsLow = 1-2*dist_chosen[:i_stim]\n",
    "        prob_low = np.clip(Wconstant + W1*np.sum(np.flip(arePrevTrialsLow)*np.exp(-(np.arange(i_stim)+1)/tau)),\n",
    "                           a_min=0,a_max=1)\n",
    "        probHighgivenPercept = LikelihoodPerceptgivenHigh*(1-prob_low)/\\\n",
    "        (LikelihoodPerceptgivenHigh*(1-prob_low) + LikelihoodPerceptgivenLow*prob_low)\n",
    "        \n",
    "        input_array = np.random.normal(loc=np.log10(sim_trial_tones[i_stim]),\n",
    "                                       scale=sigma_sensory,\n",
    "                                       size=(reps,1,n_tones)) \n",
    "        for i_tperc in range(reps):\n",
    "            perc_tone_idxs = np.zeros((n_tones,1),dtype=int)\n",
    "            for i in range(n_tones):\n",
    "                perc_tone_idxs[i] = np.argmin(np.abs(log_freq_percept-input_array[i_tperc][0][i]))\n",
    "                # find relevant adjacent freq percepts   \n",
    "            posterior_perc_tone = probHighgivenPercept[perc_tone_idxs[0],perc_tone_idxs[1],perc_tone_idxs[2]]\n",
    "            trial_behaviour = np.squeeze(posterior_perc_tone) > 0.5\n",
    "            all_trial_behaviour[i_stim*reps+i_tperc,:] = trial_behaviour\n",
    "        all_trial_tones[i_stim*reps:(i_stim+1)*reps,:] = sim_trial_tones[i_stim]    \n",
    "        prob_trial_behaviour[i_stim] = np.mean(all_trial_behaviour[i_stim*reps:(i_stim+1)*reps])\n",
    "\n",
    "        gaussian_array_mat = Tones3dgrid(np.array([np.log10(sim_trial_tones[i_stim][0]),\n",
    "                                                   np.log10(sim_trial_tones[i_stim][1]),\n",
    "                                                   np.log10(sim_trial_tones[i_stim][2])]),sigma=sigma_sensory)         \n",
    "        probability_sim_high[i_stim] = np.sum(np.multiply(probHighgivenPercept>0.5, gaussian_array_mat))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Shuffling the tones and the behaviour to simluate an experiment\n",
    "\n",
    "    s = np.arange(all_trial_tones.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    all_trial_tones = all_trial_tones[s]\n",
    "    all_trial_behaviour = all_trial_behaviour[s]\n",
    "    \"\"\"\n",
    "    return all_trial_tones, prob_trial_behaviour\n",
    "\n",
    "def generate_behaviourRandomChoice(params, trial_tones):\n",
    "    sigma_sensory, prob_low = params[0], params[1] # inputs are guesses at our parameters  \n",
    "    \n",
    "    prob_trial_behaviour = np.zeros((len(trial_tones),1))\n",
    "    for i_trial in range(len(trial_tones)):\n",
    "        gaussian_array_mat = Tones3dgrid(np.array([np.log10(trial_tones[i_trial][0]),\n",
    "                                                   np.log10(trial_tones[i_trial][1]),\n",
    "                                                   np.log10(trial_tones[i_trial][2])]),sigma=sigma_sensory)\n",
    "        prob_trial_behaviour[i_trial] = np.sum(np.multiply((1-prob_low), gaussian_array_mat))\n",
    "            \n",
    "    return trial_tones, prob_trial_behaviour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noContextData(tones,toneTypes,corrans,keys):\n",
    "    [trial_tones_expt, trial_tonesType_expt, \n",
    "    corrans_expt, trial_behaviour_expt] = identifyResponseTrials(keysPressed = keys,\n",
    "                                                                 correctAns = corrans, \n",
    "                                                                 tonesPlayed = tones, \n",
    "                                                                 tonesSignalOrDistractor=toneTypes,\n",
    "                                                                 exptTotalLength = 600)\n",
    "\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_expt==corrans_expt)/len(trial_tones_expt))\n",
    "    return trial_tones_expt, trial_tonesType_expt, corrans_expt, trial_behaviour_expt\n",
    "\n",
    "\"\"\"\n",
    "Subsample the long context expt with a low category bias\n",
    "\"\"\"\n",
    "def subsampleLongContextLow(toneslc,toneTypeslc,corranslc,keyslc):\n",
    "    [trial_tones_fulllc, trial_tonesType_fulllc, \n",
    "    corrans_fulllc, trial_behaviour_fulllc] = identifyResponseTrials(keysPressed = keyslc, \n",
    "                                                                     correctAns = corranslc, \n",
    "                                                                     tonesPlayed = toneslc, \n",
    "                                                                     tonesSignalOrDistractor=toneTypeslc,\n",
    "                                                                     exptTotalLength = 800)\n",
    "    idxHigh = np.arange(len(trial_behaviour_fulllc[0:]))[corrans_fulllc[0:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_fulllc[0:]))[corrans_fulllc[0:]==0]\n",
    "    idxOfSmallerCategory = np.random.choice(idxLow,size=len(idxHigh),replace=False)\n",
    "    idxToKeep = np.concatenate((idxHigh, idxOfSmallerCategory))\n",
    "    corrans_exptlc = corrans_fulllc[idxToKeep]\n",
    "    trial_behaviour_exptlc = trial_behaviour_fulllc[idxToKeep]\n",
    "    trial_tones_exptlc = trial_tones_fulllc[idxToKeep,:]\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_exptlc==corrans_exptlc)/len(trial_tones_exptlc))\n",
    "    return trial_tones_exptlc, trial_behaviour_exptlc\n",
    "\n",
    "\"\"\"\n",
    "Subsample the long context expt with a high category bias\n",
    "\"\"\"\n",
    "def subsampleLongContextHigh(toneshc,toneTypeshc,corranshc,keyshc):\n",
    "    [trial_tones_fullhc, trial_tonesType_fullhc, \n",
    "     corrans_fullhc, trial_behaviour_fullhc] = identifyResponseTrials(keysPressed = keyshc, \n",
    "                                                                      correctAns = corranshc,\n",
    "                                                                      tonesPlayed = toneshc,\n",
    "                                                                      tonesSignalOrDistractor=toneTypeshc,\n",
    "                                                                      exptTotalLength = 800)\n",
    "    idxHigh = np.arange(len(trial_behaviour_fullhc[0:]))[corrans_fullhc[0:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_fullhc[0:]))[corrans_fullhc[0:]==0]\n",
    "    idxOfSmallerCategory = np.random.choice(idxHigh,size=len(idxLow),replace=False)\n",
    "    idxToKeep = np.concatenate((idxLow, idxOfSmallerCategory))\n",
    "    corrans_expthc = corrans_fullhc[idxToKeep]\n",
    "    trial_behaviour_expthc = trial_behaviour_fullhc[idxToKeep]\n",
    "    trial_tones_expthc = trial_tones_fullhc[idxToKeep,:]\n",
    "    #print(\"Got correct: \", np.sum(trial_behaviour_expthc==corrans_expthc)/len(trial_tones_expthc))\n",
    "    return trial_tones_expthc, trial_behaviour_expthc\n",
    "\n",
    "\"\"\"\n",
    "Using raw data to calculate any inherent bias the subject develops\n",
    "\"\"\"\n",
    "def bias(trial_behaviour_longContext):\n",
    "    return np.mean(trial_behaviour_longContext)\n",
    "\n",
    "\"\"\"\n",
    "Fitting experimental psychometric curves by generating simulated data and behaviour\n",
    "\"\"\"\n",
    "def fittingPsychometricCurve(PLowExpt,subjectFitParams,ifRandomChoice=0):\n",
    "    moreTrainingTrials, corransOfTrials, _ = task(n_trials = 2000, n_tones = 3, \n",
    "                                                  p_back=0.3, p_low=PLowExpt)\n",
    "    if ifRandomChoice:\n",
    "        all_trial_tones, behaviourOfTrials = generate_behaviourRandomChoice(trial_tones=moreTrainingTrials,\n",
    "                                                                            params=[subjectFitParams[3],\n",
    "                                                                                    subjectFitParams[5]])\n",
    "    else:        \n",
    "        all_trial_tones, behaviourOfTrials = generate_behaviour(sim_trial_tones=moreTrainingTrials, \n",
    "                                                                dist_chosen=corransOfTrials,\n",
    "                                                                reps=1, n_tones=3, \n",
    "                                                                log_prior_params = subjectFitParams[0:3], \n",
    "                                                                sigma_sensory = subjectFitParams[3],\n",
    "                                                                prob_back = subjectFitParams[4],\n",
    "                                                                Wconstant = subjectFitParams[5],\n",
    "                                                                W1 = subjectFitParams[6],\n",
    "                                                                tau = subjectFitParams[7])\n",
    "\n",
    "    unique_tones_played, subjectBehaviourModelled = plottingInfluenceFn(all_trial_tones, \n",
    "                                                                        behaviourOfTrials)\n",
    "    \n",
    "    return unique_tones_played, subjectBehaviourModelled\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-psychiatry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fig. 5: individual participant curves -- check parameter values from excel sheet attached\n",
    "\"\"\"\n",
    "\n",
    "df_tones, df_toneKind, df_corrans, df_keys = extractData(csv_test=Test, \n",
    "                                                        csv_data=Data, \n",
    "                                                        exptTotalLength=600, \n",
    "                                                        exptLengthWithBreaks=603) \n",
    "\n",
    "df_toneslc, df_toneKindlc, df_corranslc, df_keyslc = extractData(csv_test=TestLc, \n",
    "                                                                csv_data=DataLc, \n",
    "                                                                exptTotalLength=800, \n",
    "                                                                exptLengthWithBreaks=804) \n",
    "\n",
    "\n",
    "df_toneshc, df_toneKindhc, df_corranshc, df_keyshc = extractData(csv_test=TestHc, \n",
    "                                                                csv_data=DataHc, \n",
    "                                                                exptTotalLength=800, \n",
    "                                                                exptLengthWithBreaks=804) \n",
    "\n",
    "meanBiaslc = 0; meanBiashc = 0\n",
    "numberOfSubsamples = 1\n",
    "unique_tones_played = np.empty((numberOfSubsamples,30))\n",
    "subjectBehaviourLC = np.empty((numberOfSubsamples,30))\n",
    "subjectBehaviourHC = np.empty((numberOfSubsamples,30))\n",
    "for iSubsample in range(numberOfSubsamples):\n",
    "    sampledlc_tones, sampledlc_behaviour = subsampleLongContextLow(toneslc=df_toneslc, \n",
    "                                                                   toneTypeslc=df_toneKindlc,\n",
    "                                                                   corranslc=df_corranslc, \n",
    "                                                                   keyslc=df_keyslc)\n",
    "    sampledhc_tones, sampledhc_behaviour = subsampleLongContextHigh(toneshc=df_toneshc, \n",
    "                                                                    toneTypeshc=df_toneKindhc,\n",
    "                                                                    corranshc=df_corranshc, \n",
    "                                                                    keyshc=df_keyshc)\n",
    "    noContext_tones, _, _, noContext_behaviour = noContextData(tones=df_tones,\n",
    "                                                               toneTypes=df_toneKind,\n",
    "                                                               corrans=df_corrans,\n",
    "                                                               keys=df_keys)\n",
    "    _, allPositionssubjectBehaviour = plottingInfluenceFn(noContext_tones,\n",
    "                                                          noContext_behaviour)\n",
    "    \n",
    "    unique_tones_played[iSubsample,:], allPositionssubjectBehaviourLC = plottingInfluenceFn(sampledlc_tones, \n",
    "                                                                                            sampledlc_behaviour)  \n",
    "    subjectBehaviourLC[iSubsample,:] = np.mean(allPositionssubjectBehaviourLC,axis=1)\n",
    "    meanBiaslc += bias(sampledlc_behaviour)\n",
    "    \n",
    "    unique_tones_played[iSubsample,:], allPositionssubjectBehaviourHC = plottingInfluenceFn(sampledhc_tones,\n",
    "                                                                                            sampledhc_behaviour)  \n",
    "    subjectBehaviourHC[iSubsample,:] = np.mean(allPositionssubjectBehaviourHC,axis=1)\n",
    "    meanBiashc += bias(sampledhc_behaviour)\n",
    "    \n",
    "print(\"length of subsample dataset in low context\", len(sampledlc_tones))\n",
    "print(\"length of subsample dataset in high context\", len(sampledhc_tones))\n",
    "    \n",
    "print(\"p(L) given the no context is likely to be: \",np.mean(noContext_behaviour))\n",
    "print(\"P(L) given the long context data biased towards low is likely to be: \", meanBiaslc/numberOfSubsamples)\n",
    "print(\"P(L) given the long context data biased towards high is likely to be: \", meanBiashc/numberOfSubsamples)\n",
    "\n",
    "print(\"Average deviation from 0.5 for frequencies that are likely distractors\",\n",
    "      round(np.mean([0.5-np.nanmean(allPositionssubjectBehaviour[:7,:]), \n",
    "               np.nanmean(allPositionssubjectBehaviour[-8:,:])-0.5])\n",
    "            /np.mean(abs(allPositionssubjectBehaviour-0.5)), 2))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(allPositionssubjectBehaviour,axis=1), \n",
    "            yerr=np.nanstd(allPositionssubjectBehaviour,axis=1)/np.sqrt(3),color='k',linewidth=2)\n",
    "\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(subjectBehaviourLC,axis=0), \n",
    "            yerr=np.nanstd(allPositionssubjectBehaviourLC,axis=1)/np.sqrt(3),color='orange',linewidth=2)\n",
    "ax.errorbar(np.mean(np.log10(unique_tones_played),axis=0), np.nanmean(subjectBehaviourHC,axis=0), \n",
    "            yerr=np.nanstd(allPositionssubjectBehaviourHC,axis=1)/np.sqrt(3),color='brown',linewidth=2)\n",
    "\n",
    "simulatedTrials, simulatedBehaviourUnbiased = fittingPsychometricCurve(PLowExpt=0.5, \n",
    "                                                                       subjectFitParams=[2.55,2.85,0.1,0.15,0.79,\n",
    "                                                                                        0.52,0,0],\n",
    "                                                                       ifRandomChoice=0)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourUnbiased,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourUnbiased,axis=1)/np.sqrt(3),\n",
    "             color='k',linestyle='dotted',linewidth=2)\n",
    "\n",
    "simulatedTrials, simulatedBehaviourBiasedLow = fittingPsychometricCurve(PLowExpt=0.7, \n",
    "                                                                        subjectFitParams=[2.55,2.85,0.1,0.19,0.89,\n",
    "                                                                                         0.61,0,0],\n",
    "                                                                        ifRandomChoice=1)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourBiasedLow,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourBiasedLow,axis=1)/np.sqrt(3),\n",
    "             color='orange',linestyle='dotted',linewidth=2)\n",
    "\n",
    "simulatedTrials, simulatedBehaviourBiasedHigh = fittingPsychometricCurve(PLowExpt=0.3, \n",
    "                                                                        subjectFitParams=[2.55,2.85,0.1,0.19,0.81,\n",
    "                                                                                          0.22,0,0],\n",
    "                                                                        ifRandomChoice=1)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourBiasedHigh,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourBiasedHigh,axis=1)/np.sqrt(3),\n",
    "             color='brown',linestyle='dotted',linewidth=2)\n",
    "\n",
    "ax.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "ax.set_xticklabels(labels=[100,1000,3000])\n",
    "ax.set_yticks(ticks=np.arange(0,1.1,0.2))\n",
    "ax.set_yticklabels(labels=np.around(np.arange(0,1.1,0.2),1))\n",
    "ax.tick_params(axis='both',labelsize=30,length=6,width=2)\n",
    "ax.set_xlabel('Frequency (Hz)',fontsize=32)\n",
    "ax.set_ylabel(r'p($\\rm{\\widehat{High}}$)',fontsize=32)\n",
    "makeAxesPretty(ax)\n",
    "\n",
    "#plt.savefig('figures/FromProlific/illustrations/e21e_pBHgivenT_forNoandLongContext.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2bc3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figs. 2A-D, extended data 1: Behavioural analysis across all subjects\n",
    "Qs: How important are the different tone positions and what is the behavioural relevance of signals vs distractors?\n",
    "\"\"\"\n",
    "\n",
    "Test = pd.read_csv('subjectDataForPlots/allTrials_noBias.csv')\n",
    "SubjectFiles = os.listdir('subjectDataForPlots/noContextData')\n",
    "averageSubjectBehaviour = np.empty(shape=(56,30))\n",
    "weightsOfTonesAllTrials = np.zeros(shape=(56,3))\n",
    "meanWeightsOfTonesTwoDistractors = np.zeros(shape=(56,3))\n",
    "meanWeightsOfTonesOneDistractor = np.zeros(shape=(56,3))\n",
    "weightsOfTonesNoDistractorTrials = np.zeros(shape=(56,3))\n",
    "subjectBehaviourTwoDistractors = np.zeros(shape=(56,30,3))\n",
    "subjectBehaviourOneDistractor = np.zeros(shape=(56,30,2))\n",
    "subjectBehaviourNoDistractors = np.zeros(shape=(56,18))\n",
    "\n",
    "def meanWeightsOfTonePositions(tones, behaviour, accuracy):\n",
    "    weightsOfTonePositions = np.zeros(shape=(3,))\n",
    "    for toneIdx in range(3):\n",
    "        unique_tones = np.unique(tones[:,toneIdx])\n",
    "        meanPHighPerTone = np.zeros(shape=(len(unique_tones),))\n",
    "        for i_uniqueTone in range(len(unique_tones)):\n",
    "            meanPHighPerTone[i_uniqueTone] = np.mean(behaviour[tones[:,toneIdx]==\n",
    "                                                               unique_tones[i_uniqueTone]])\n",
    "        weightsOfTonePositions[toneIdx] = np.corrcoef(np.log10(unique_tones), meanPHighPerTone)[1,0]\n",
    "    return weightsOfTonePositions\n",
    "\n",
    "def meanWeightsOfTonesRelevance(tones, behaviour, accuracy):\n",
    "    weightsOfTonePositions = np.zeros(shape=(3,))\n",
    "    for toneIdx in range(3):\n",
    "        unique_tones = np.unique(tones[:,toneIdx])\n",
    "        unique_tones_center = unique_tones[np.logical_and(np.log10(unique_tones) < 3.05, \n",
    "                                                          np.log10(unique_tones) > 2.35)]\n",
    "        meanPHighPerTone = np.zeros(shape=(len(unique_tones_center),))\n",
    "        for i_uniqueTone in range(len(unique_tones_center)):\n",
    "            meanPHighPerTone[i_uniqueTone] = np.mean(behaviour[tones[:,toneIdx]==\n",
    "                                                               unique_tones_center[i_uniqueTone]])\n",
    "        weightsOfTonePositions[toneIdx] = np.corrcoef(np.log10(unique_tones_center), meanPHighPerTone)[1,0]\n",
    "    return weightsOfTonePositions\n",
    "\n",
    "def swapPositions(tones, toneTypes, oneDistractor=1):\n",
    "    if oneDistractor:\n",
    "        swappedTones = np.copy(tones)\n",
    "        swappedToneTypes = np.copy(toneTypes)\n",
    "        for itrial in range(len(tones)):\n",
    "            if toneTypes[itrial,1]==0:\n",
    "                swappedToneTypes[itrial,0] = np.copy(toneTypes[itrial,1])\n",
    "                swappedTones[itrial,0] = np.copy(tones[itrial,1])\n",
    "                swappedToneTypes[itrial,1] = np.copy(toneTypes[itrial,0])\n",
    "                swappedTones[itrial,1] = np.copy(tones[itrial,0])\n",
    "            if toneTypes[itrial,2]==0:\n",
    "                swappedToneTypes[itrial,0] = np.copy(toneTypes[itrial,2])\n",
    "                swappedTones[itrial,0] = np.copy(tones[itrial,2])\n",
    "                swappedToneTypes[itrial,2] = np.copy(toneTypes[itrial,0])\n",
    "                swappedTones[itrial,2] = np.copy(tones[itrial,0])\n",
    "    else:\n",
    "        swappedTones = np.copy(tones)\n",
    "        swappedToneTypes = np.copy(toneTypes)\n",
    "        for itrial in range(len(tones)):\n",
    "            if toneTypes[itrial,1]>0:\n",
    "                swappedToneTypes[itrial,2] = np.copy(toneTypes[itrial,1])\n",
    "                swappedTones[itrial,2] = np.copy(tones[itrial,1])\n",
    "                swappedToneTypes[itrial,1] = np.copy(toneTypes[itrial,2])\n",
    "                swappedTones[itrial,1] = np.copy(tones[itrial,2])\n",
    "            if toneTypes[itrial,0]>0:\n",
    "                swappedToneTypes[itrial,0] = np.copy(toneTypes[itrial,2])\n",
    "                swappedTones[itrial,0] = np.copy(tones[itrial,2])\n",
    "                swappedToneTypes[itrial,2] = np.copy(toneTypes[itrial,0])\n",
    "                swappedTones[itrial,2] = np.copy(tones[itrial,0])\n",
    "    return swappedTones, swappedToneTypes\n",
    "\n",
    "def trialsWithIncongruentDistractor(tones, toneTypes, corrAns, trialIdxs):\n",
    "    numelTrialIdxs = np.where(trialIdxs)[0]\n",
    "    tonesOfTrialsWithDistractors = tones[numelTrialIdxs,:]\n",
    "    toneTypesOfTrialsWithDistractors = toneTypes[numelTrialIdxs,:]\n",
    "    corrAnsOfTrialsWithDistractors = corrAns[numelTrialIdxs]\n",
    "    chosenTrialIdxs = np.array([])\n",
    "    for trial in range(len(numelTrialIdxs)):\n",
    "        if corrAnsOfTrialsWithDistractors[trial] == 1:\n",
    "            if any(np.log10(tonesOfTrialsWithDistractors[trial,:]\n",
    "                            [toneTypesOfTrialsWithDistractors[trial,:]==0]) > 2.7):\n",
    "                chosenTrialIdxs = np.append(chosenTrialIdxs, numelTrialIdxs[trial])\n",
    "        else:\n",
    "            if any(np.log10(tonesOfTrialsWithDistractors[trial,:]\n",
    "                            [toneTypesOfTrialsWithDistractors[trial,:]==0]) < 2.7):\n",
    "                chosenTrialIdxs = np.append(chosenTrialIdxs, numelTrialIdxs[trial])\n",
    "    return chosenTrialIdxs\n",
    "\n",
    "for subjectIdx in range(56):  \n",
    "    filename = 'subjectDataForPlots/noContextData/'+SubjectFiles[subjectIdx]  \n",
    "    Data = pd.read_csv(filename)\n",
    "    \n",
    "    df_tones, df_toneKind, df_corrans, df_keys = extractData(csv_test=Test, \n",
    "                                                             csv_data=Data, \n",
    "                                                             exptTotalLength=600,\n",
    "                                                             exptLengthWithBreaks=603) \n",
    "    \n",
    "    [noContext_tones, noContext_toneTypes, \n",
    "    noContext_corrAns, noContext_behaviour] = noContextData(keys=df_keys,\n",
    "                                                            toneTypes=df_toneKind,\n",
    "                                                            corrans=df_corrans,\n",
    "                                                            tones=df_tones)\n",
    "    \n",
    "    noContext_accuracy = noContext_behaviour==noContext_corrAns\n",
    "    unique_tonesPlayed_overall, subjectBehaviour = plottingInfluenceFn(noContext_tones, noContext_behaviour)\n",
    "    averageSubjectBehaviour[subjectIdx,:] = np.nanmean(subjectBehaviour,axis=1)    \n",
    "    \n",
    "    allTwoToneDistractorTrialsIdx = np.sum(noContext_toneTypes==0,axis=1)==2\n",
    "    # select only those trials who distractor frequency is incongruent with trial category\n",
    "    onlyIncongruentTrials = 0;\n",
    "    if onlyIncongruentTrials:\n",
    "        twoToneDistractorTrialsIdx = trialsWithIncongruentDistractor(tones = noContext_tones,\n",
    "                                                                     toneTypes = noContext_toneTypes,\n",
    "                                                                     corrAns = noContext_corrAns,\n",
    "                                                                     trialIdxs = allTwoToneDistractorTrialsIdx).astype(int)\n",
    "    else:\n",
    "        twoToneDistractorTrialsIdx = np.copy(allTwoToneDistractorTrialsIdx)\n",
    "    twoToneDistractorTrials, _ = swapPositions(tones=noContext_tones[twoToneDistractorTrialsIdx],\n",
    "                                                toneTypes=noContext_toneTypes[twoToneDistractorTrialsIdx],\n",
    "                                                oneDistractor=0)\n",
    "    [unique_tonesPlayed_twoDistractors, tempTwo] = plottingInfluenceFn(twoToneDistractorTrials,\n",
    "                                                                        noContext_behaviour[twoToneDistractorTrialsIdx])\n",
    "    #subjectBehaviourTwoDistractors[subjectIdx,:,1] = np.copy(tempTwo[:,2])\n",
    "    #subjectBehaviourTwoDistractors[subjectIdx,:,0] = np.nanmean(tempTwo[:,:2],axis=1)\n",
    "    subjectBehaviourTwoDistractors[subjectIdx,:,:] = np.copy(tempTwo)\n",
    "        \n",
    "    meanWeightsOfTonesTwoDistractors[subjectIdx,:] = meanWeightsOfTonesRelevance(tones=twoToneDistractorTrials,\n",
    "                                                                                behaviour=noContext_behaviour[twoToneDistractorTrialsIdx],\n",
    "                                                                                accuracy=noContext_accuracy[twoToneDistractorTrialsIdx])\n",
    "    \n",
    "    allOneToneDistractorTrialsIdx = np.sum(noContext_toneTypes==0,axis=1)==1\n",
    "    \n",
    "    if onlyIncongruentTrials:\n",
    "        oneToneDistractorTrialsIdx = trialsWithIncongruentDistractor(tones = noContext_tones,\n",
    "                                                                     toneTypes = noContext_toneTypes,\n",
    "                                                                     corrAns = noContext_corrAns,\n",
    "                                                                     trialIdxs = allOneToneDistractorTrialsIdx).astype(int)\n",
    "    else:\n",
    "        oneToneDistractorTrialsIdx = np.copy(allOneToneDistractorTrialsIdx)\n",
    "    oneToneDistractorTrials, _ = swapPositions(tones=noContext_tones[oneToneDistractorTrialsIdx],\n",
    "                                                toneTypes=noContext_toneTypes[oneToneDistractorTrialsIdx],\n",
    "                                                oneDistractor=1)\n",
    "    [unique_tonesPlayed_oneDistractor, tempOne] = plottingInfluenceFn(oneToneDistractorTrials,\n",
    "                                                                      noContext_behaviour[oneToneDistractorTrialsIdx])    \n",
    "    subjectBehaviourOneDistractor[subjectIdx,:,0] = np.copy(tempOne[:,0])\n",
    "    subjectBehaviourOneDistractor[subjectIdx,:,1] = np.nanmean(tempOne[:,1:],axis=1)\n",
    "    \n",
    "    meanWeightsOfTonesOneDistractor[subjectIdx,:] = meanWeightsOfTonesRelevance(tones=oneToneDistractorTrials,\n",
    "                                                                                behaviour=noContext_behaviour[oneToneDistractorTrialsIdx],\n",
    "                                                                                accuracy=noContext_accuracy[oneToneDistractorTrialsIdx])\n",
    "\n",
    "    \n",
    "    noDistractorTrialsIdx = np.sum(noContext_toneTypes==0,axis=1)==0\n",
    "    [unique_tonesPlayed_noDistractors, tempNo] = plottingInfluenceFn(noContext_tones[noDistractorTrialsIdx],\n",
    "                                                                    noContext_behaviour[noDistractorTrialsIdx])\n",
    "    subjectBehaviourNoDistractors[subjectIdx,:] = np.nanmean(tempNo,axis=1) \n",
    "     \n",
    "    weightsOfTonesNoDistractorTrials[subjectIdx,:] = meanWeightsOfTonesRelevance(tones=noContext_tones[noDistractorTrialsIdx],\n",
    "                                                                                behaviour=noContext_behaviour[noDistractorTrialsIdx],\n",
    "                                                                                accuracy=noContext_accuracy[noDistractorTrialsIdx])\n",
    "\n",
    "    weightsOfTonesAllTrials[subjectIdx,:] = meanWeightsOfTonePositions(tones=noContext_tones,\n",
    "                                                                        behaviour=noContext_behaviour,\n",
    "                                                                        accuracy=noContext_accuracy)\n",
    "\n",
    "\"\"\"\n",
    "Average plots across all subjects\n",
    "\"\"\"\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "ax1.errorbar(np.log10(unique_tonesPlayed_noDistractors), \n",
    "             np.mean(subjectBehaviourNoDistractors,axis=0),\n",
    "             yerr=np.std(subjectBehaviourNoDistractors,axis=0)/np.sqrt(56),color='k')\n",
    "ax1.axvline(x=np.log10(unique_tonesPlayed_noDistractors[2]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "ax1.axvline(x=np.log10(unique_tonesPlayed_noDistractors[-3]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "for tonePosition in [0,1]:\n",
    "    ax2.errorbar(np.log10(unique_tonesPlayed_oneDistractor), \n",
    "                 np.mean(subjectBehaviourOneDistractor[:,:,tonePosition],axis=0),\n",
    "                 yerr=np.std(subjectBehaviourOneDistractor[:,:,tonePosition],axis=0)/np.sqrt(56),\n",
    "                 color=[(1-tonePosition)/1.5,(1-tonePosition)/1.5,(1-tonePosition)/1.5])\n",
    "ax3.errorbar(np.log10(unique_tonesPlayed_twoDistractors), \n",
    "             np.mean(np.mean(subjectBehaviourTwoDistractors[:,:,:2],axis=2),axis=0),\n",
    "             yerr=np.std(np.mean(subjectBehaviourTwoDistractors[:,:,:2],axis=2),axis=0)/np.sqrt(56),\n",
    "             color=[1/1.5,1/1.5,1/1.5])\n",
    "ax3.errorbar(np.log10(unique_tonesPlayed_twoDistractors), \n",
    "             np.mean(subjectBehaviourTwoDistractors[:,:,2],axis=0),\n",
    "             yerr=np.std(subjectBehaviourTwoDistractors[:,:,2],axis=0)/np.sqrt(56),\n",
    "             color=[0,0,0])  \n",
    "ax2.axvline(x=np.log10(unique_tonesPlayed_oneDistractor[8]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "ax2.axvline(x=np.log10(unique_tonesPlayed_oneDistractor[22]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "ax3.axvline(x=np.log10(unique_tonesPlayed_twoDistractors[8]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "ax3.axvline(x=np.log10(unique_tonesPlayed_twoDistractors[22]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "for axes in [ax1,ax2,ax3]:\n",
    "    axes.set_ylim([0,1])\n",
    "    axes.set_xlim([1.9,3.6])\n",
    "    axes.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "    axes.set_xticklabels(labels=[100,1000,3000])\n",
    "    axes.tick_params(axis='both',labelsize=30,length=6,width=2)\n",
    "    axes.set_xlabel('Frequency (Hz)',fontsize=32)\n",
    "    axes.set_ylabel(r'p($\\rm{\\widehat{High}}$)',fontsize=32)\n",
    "    makeAxesPretty(axes)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/variationOfBehaviourWithToneFrequenciesNoDistractors.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/variationOfBehaviourWithToneFrequenciesOneDistractor.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "#fig3.savefig('figures/FromProlific/illustrations/variationOfBehaviourWithToneFrequenciesTwoDistractors.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "\"\"\"\n",
    "Plot per subject\n",
    "\"\"\"\n",
    "for iSubj in [0,8,14,15,16,39]:\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    for tonePosition in [0,1,2]:\n",
    "        ax.plot(np.log10(unique_tonesPlayed_twoDistractors), \n",
    "                subjectBehaviourTwoDistractors[iSubj,:,tonePosition],\n",
    "                color=[(2-tonePosition)/2.5,(2-tonePosition)/2.5,(2-tonePosition)/2.5]) \n",
    "    ax.axvline(x=np.log10(unique_tonesPlayed_twoDistractors[8]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "    ax.axvline(x=np.log10(unique_tonesPlayed_twoDistractors[22]), ymin=0, ymax=1, color='k', linestyle='--')\n",
    "    ax.set_ylim([0,1.1])\n",
    "    ax.set_xlim([1.9,3.6])\n",
    "    ax.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "    ax.set_xticklabels(labels=[100,1000,3000])\n",
    "    ax.tick_params(axis='both',labelsize=30,length=6,width=2)\n",
    "    ax.set_xlabel('Frequency (Hz)',fontsize=32)\n",
    "    ax.set_ylabel(r'p($\\rm{\\widehat{High}}$)',fontsize=32)\n",
    "    makeAxesPretty(ax)\n",
    "    #fig.savefig('figures/FromProlific/illustrations/subject-'+str(iSubj)+\n",
    "    #            'variationOfBehaviourWithToneFrequenciesTwoDistractors.pdf',\n",
    "    #            bbox_inches='tight',transparent=True)\n",
    "\n",
    "\"\"\"\n",
    "Weights of the three tone positions\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.errorbar(np.arange(3), \n",
    "            np.nanmean(weightsOfTonesAllTrials,axis=0)/sum(np.nanmean(weightsOfTonesAllTrials,axis=0)),\n",
    "            yerr=np.nanstd(weightsOfTonesAllTrials,axis=0)/np.sqrt(56),linestyle=None,color='k',marker='o')\n",
    "ax.set_xticks(ticks=[0,1,2])\n",
    "ax.set_xticklabels(labels=['First\\n Tone', 'Second \\n Tone', 'Third \\n Tone'])\n",
    "ax.set_ylim([0,1.1])\n",
    "ax.tick_params(axis='both',labelsize=36,length=6,width=2)\n",
    "ax.set_xlabel('Tone Position',fontsize=38)\n",
    "ax.set_ylabel('Normalized Correlation of \\n Tone Frequency and \\n Category Choice Probability',fontsize=38)\n",
    "makeAxesPretty(ax)\n",
    "#plt.savefig('figures/FromProlific/illustrations/weightsOfTonePositionsForAllTrials.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "\n",
    "\"\"\"\n",
    "Tone relevance of signals vs distractors across three trial conditions\n",
    "\"\"\"\n",
    "normalizedWeightsOfTonesNoDistractorTrials = weightsOfTonesNoDistractorTrials/sum(np.nanmean(weightsOfTonesNoDistractorTrials,axis=0))\n",
    "normalizedWeightsOfTonesOneDistractor = meanWeightsOfTonesOneDistractor/sum(np.nanmean(meanWeightsOfTonesOneDistractor,axis=0))\n",
    "normalizedWeightsOfTonesTwoDistractors = meanWeightsOfTonesTwoDistractors/sum(np.nanmean(meanWeightsOfTonesTwoDistractors,axis=0))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.errorbar([2], np.nanmean(weightsOfTonesNoDistractorTrials),\n",
    "            yerr= np.nanstd(weightsOfTonesNoDistractorTrials)/np.sqrt(3*56),linestyle=None,color='k',marker='o')\n",
    "ax.errorbar([9.5,10.5], [np.nanmean(meanWeightsOfTonesOneDistractor[:,0]),\n",
    "                         np.nanmean(meanWeightsOfTonesOneDistractor[:,1:])],\n",
    "            yerr=[np.nanstd(meanWeightsOfTonesOneDistractor[:,0])/np.sqrt(56),\n",
    "                  np.nanstd(meanWeightsOfTonesOneDistractor[:,1:])/np.sqrt(2*56)],\n",
    "            linestyle=None,color='k',marker='o')\n",
    "ax.errorbar([17.5,18.5], [np.nanmean(meanWeightsOfTonesTwoDistractors[:,0:2]),\n",
    "                          np.nanmean(meanWeightsOfTonesTwoDistractors[:,2])],\n",
    "            yerr=[np.nanstd(meanWeightsOfTonesTwoDistractors[:,0:2])/np.sqrt(2*56),\n",
    "                  np.nanstd(meanWeightsOfTonesTwoDistractors[:,2])/np.sqrt(56)],\n",
    "            linestyle=None,color='k',marker='o')\n",
    "ax.set_xticks(ticks=[2,10,18])\n",
    "ax.set_xticklabels(labels=['No \\n Distractors', 'One \\n Distractor', 'Two \\n Distractors'])\n",
    "ax.set_xlim([0,20])\n",
    "ax.set_ylim([0,1.1])\n",
    "ax.tick_params(axis='both',labelsize=36,length=6,width=2)\n",
    "ax.set_xlabel('Trial type',fontsize=38)\n",
    "ax.set_ylabel('Correlation of Tone \\n Frequency and Category \\n Choice Probability',fontsize=38)\n",
    "makeAxesPretty(ax)\n",
    "#plt.savefig('figures/FromProlific/illustrations/weightsOfTonePositionsForDifferentTrials.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "ax1.errorbar(meanWeightsOfTonesOneDistractor[:,0],\n",
    "             np.nanmean(meanWeightsOfTonesOneDistractor[:,1:],axis=1), \n",
    "             yerr = np.nanstd(meanWeightsOfTonesOneDistractor[:,1:],axis=1)/np.sqrt(2), \n",
    "             color='k', linestyle='', marker='o')\n",
    "ax2.errorbar(np.nanmean(meanWeightsOfTonesTwoDistractors[:,0:2],axis=1),\n",
    "             meanWeightsOfTonesTwoDistractors[:,2],\n",
    "             xerr = np.nanstd(meanWeightsOfTonesTwoDistractors[:,0:2],axis=1)/np.sqrt(2), \n",
    "             color='k', linestyle='', marker='o')\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.plot(np.arange(-0.25,1,0.01),np.arange(-0.25,1,0.01),'k--')\n",
    "    ax.set_xlim([-0.25,1])\n",
    "    ax.set_ylim([-0.25,1])\n",
    "    ax.tick_params(axis='both',labelsize=36,length=6,width=2)\n",
    "    ax.set_xlabel('Correlation of Distractor \\n Tone Frequency and \\n Category Choice Probability',fontsize=38)\n",
    "    ax.set_ylabel('Correlation of Signal \\n Tone Frequency and \\n Category Choice Probability',fontsize=38)\n",
    "    makeAxesPretty(ax)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/scatterPlotWeightsOfTonePositionsForTrialsWithOneDistractor.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/scatterPlotWeightsOfTonePositionsForTrialsWithTwoDistractors.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "\n",
    "print(\"Mean correlation tone position\",  \n",
    "      np.nanmean(weightsOfTonesAllTrials,axis=0)/np.sum(np.nanmean(weightsOfTonesAllTrials,axis=0)))\n",
    "print(\"SEM correlation tone position\",  \n",
    "      np.nanstd(weightsOfTonesAllTrials,axis=0)/np.sqrt(3*56))\n",
    "print(\"Mean and SEM correlation signal vs distractor tones for no distractor trials\",\n",
    "      np.mean(weightsOfTonesNoDistractorTrials),\n",
    "      np.nanstd(weightsOfTonesNoDistractorTrials)/np.sqrt(56))\n",
    "print(\"Mean and SEM correlation signal vs distractor tones for one distractor trials\",\n",
    "      np.mean(meanWeightsOfTonesOneDistractor[:,0]), np.nanstd(meanWeightsOfTonesOneDistractor[:,0])/np.sqrt(56),\n",
    "      np.mean(meanWeightsOfTonesOneDistractor[:,1:]), np.nanstd(meanWeightsOfTonesOneDistractor[:,1:])/np.sqrt(2*56))\n",
    "print(\"Mean and SEM correlation signal vs distractor tones for two distractor trials\",\n",
    "      np.mean(meanWeightsOfTonesTwoDistractors[:,0:2]), np.nanstd(meanWeightsOfTonesTwoDistractors[:,0:2])/np.sqrt(2*56),\n",
    "      np.mean(meanWeightsOfTonesTwoDistractors[:,2]), np.nanstd(meanWeightsOfTonesTwoDistractors[:,2])/np.sqrt(56))\n",
    "\n",
    "df = pd.DataFrame(columns=['Subject','TonePosition','Weight'])\n",
    "df['Weight'] = np.concatenate([weightsOfTonesAllTrials[:,0],weightsOfTonesAllTrials[:,1],weightsOfTonesAllTrials[:,2]])\n",
    "df['TonePosition'] = [0]*56 + [1]*56 + [2]*56\n",
    "subjectList = []\n",
    "for isubj in range(1,57):\n",
    "    subjectList += [isubj]\n",
    "df['Subject'] = subjectList*3\n",
    "df.index += 1\n",
    "print(pg.rm_anova(dv='Weight',within=['TonePosition'],\n",
    "                  subject='Subject', data=df, effsize='np2'))\n",
    "print(pg.ttest(meanWeightsOfTonesOneDistractor[:,0],np.mean(meanWeightsOfTonesOneDistractor[:,1:],axis=1),\n",
    "              paired=True))\n",
    "print(pg.ttest(np.mean(meanWeightsOfTonesTwoDistractors[:,0:2],axis=1),meanWeightsOfTonesTwoDistractors[:,2],\n",
    "              paired=True))\n",
    "\n",
    "df = pd.DataFrame(columns=['Subject','SignalToneFreq','Weight'])\n",
    "df['Weight'] = np.concatenate([np.nanmean(weightsOfTonesNoDistractorTrials,axis=1),\n",
    "                               np.nanmean(meanWeightsOfTonesOneDistractor[:,1:],axis=1),\n",
    "                               meanWeightsOfTonesTwoDistractors[:,2]])\n",
    "df['SignalToneFreq'] = [0]*56 + [1]*56 + [2]*56\n",
    "subjectList = []\n",
    "for isubj in range(1,57):\n",
    "    subjectList += [isubj]\n",
    "df['Subject'] = subjectList*3\n",
    "df.index += 1\n",
    "print(pg.rm_anova(dv='Weight',within=['SignalToneFreq'],\n",
    "                  subject='Subject', data=df, effsize='np2'))\n",
    "\n",
    "df = pd.DataFrame(columns=['Subject','DistractorToneFreq','Weight'])\n",
    "df['Weight'] = np.concatenate([meanWeightsOfTonesOneDistractor[:,0],\n",
    "                               np.nanmean(meanWeightsOfTonesTwoDistractors[:,0:2],axis=1)])\n",
    "df['DistractorToneFreq'] = [1]*56 + [2]*56\n",
    "subjectList = []\n",
    "for isubj in range(1,57):\n",
    "    subjectList += [isubj]\n",
    "df['Subject'] = subjectList*2\n",
    "df.index += 1\n",
    "print(pg.rm_anova(dv='Weight',within=['DistractorToneFreq'],\n",
    "                  subject='Subject', data=df, effsize='np2'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c2b9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 3D\n",
    "Qs: How does fitting look like across all subjects in the population in the unbiased experiment?\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.errorbar(np.log10(unique_tonesPlayed_overall), np.nanmean(averageSubjectBehaviour,axis=0), \n",
    "            yerr=np.nanstd(averageSubjectBehaviour,axis=0)/np.sqrt(56),color='k',linewidth=2)\n",
    "\n",
    "\"\"\"\n",
    "Loading no context parameter data \n",
    "\"\"\"\n",
    "averageSubjectParametersProb = np.zeros((8,))\n",
    "averageSubjectParametersProb[0] = 2.55\n",
    "averageSubjectParametersProb[1] = 2.85\n",
    "averageSubjectParametersProb[2] = 0.1\n",
    "averageSubjectParametersProb[3] = np.nanmean(computedLikelihoodsVeridical['ss'].values)\n",
    "averageSubjectParametersProb[4] = np.nanmean(computedLikelihoodsVeridical['medianPBack'].values)\n",
    "averageSubjectParametersProb[5] = np.nanmean(computedLikelihoodsVeridical['plow'].values)\n",
    "averageSubjectParametersProb[6] = 0\n",
    "averageSubjectParametersProb[7] = 0\n",
    "\n",
    "averageSubjectParametersSignal = np.zeros((8,))\n",
    "averageSubjectParametersSignal[0] = 2.55\n",
    "averageSubjectParametersSignal[1] = 2.85\n",
    "averageSubjectParametersSignal[2] = 0.1\n",
    "averageSubjectParametersSignal[3] = np.nanmean(computedLikelihoodsVeridical['ssSignal'].values)\n",
    "averageSubjectParametersSignal[4] = 0\n",
    "averageSubjectParametersSignal[5] = np.nanmean(computedLikelihoodsVeridical['plowSignal'].values)\n",
    "averageSubjectParametersSignal[6] = 0\n",
    "averageSubjectParametersSignal[7] = 0\n",
    "\n",
    "simulatedTrials, simulatedBehaviourProbabilistic = fittingPsychometricCurve(PLowExpt=0.5, \n",
    "                                                                           subjectFitParams=averageSubjectParametersProb,\n",
    "                                                                           ifRandomChoice=0)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourProbabilistic,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourProbabilistic,axis=1)/np.sqrt(3),\n",
    "             color='r',linestyle='--',linewidth=2)\n",
    "\n",
    "simulatedTrials, simulatedBehaviourSignal = fittingPsychometricCurve(PLowExpt=0.5, \n",
    "                                                                    subjectFitParams=averageSubjectParametersSignal,\n",
    "                                                                    ifRandomChoice=0)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourSignal,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourSignal,axis=1)/np.sqrt(3),\n",
    "             color='lightsalmon',linestyle='dotted',linewidth=2)\n",
    "\n",
    "simulatedTrials, simulatedBehaviourRandom = fittingPsychometricCurve(PLowExpt=0.5, \n",
    "                                                                    subjectFitParams=[0,0,0,\n",
    "                                                                                      averageSubjectParametersProb[3],0,0.5],\n",
    "                                                                    ifRandomChoice=1)\n",
    "\n",
    "plt.errorbar(np.log10(simulatedTrials), \n",
    "             np.mean(simulatedBehaviourRandom,axis=1),\n",
    "             yerr = np.std(simulatedBehaviourRandom,axis=1)/np.sqrt(3),\n",
    "             color='teal',linestyle='dashdot',linewidth=2)\n",
    "\n",
    "ax.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "ax.set_xticklabels(labels=[100,1000,3000])\n",
    "ax.set_yticks(ticks=np.arange(0,1.1,0.2))\n",
    "ax.set_yticklabels(labels=np.around(np.arange(0,1.1,0.2),1))\n",
    "ax.tick_params(axis='both',labelsize=28,length=6,width=2)\n",
    "ax.set_xlabel('Frequency (Hz)',fontsize=30)\n",
    "ax.set_ylabel(r'p($\\rm{\\widehat{High}}$)',fontsize=30)\n",
    "makeAxesPretty(ax)\n",
    "#plt.savefig('figures/FromProlific/illustrations/AverageSubject_pBHgivenT_forNoContext.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-andrews",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: How does fitting look like across all subjects in the biased experiment?\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Loading no context parameter data \n",
    "\"\"\"\n",
    "mean_lowLC = np.ma.array(computedLikelihoodsLowContext['mean_low'].values,mask=False)\n",
    "mean_highLC = np.ma.array(computedLikelihoodsLowContext['mean_high'].values,mask=False)\n",
    "sigmaLC = np.ma.array(computedLikelihoodsLowContext['sigma'].values,mask=False)\n",
    "ssLC = np.ma.array(computedLikelihoodsLowContext['ss'].values,mask=False)\n",
    "pbackLC = np.ma.array(computedLikelihoodsLowContext['pback'].values,mask=False)\n",
    "plowLC = np.ma.array(computedLikelihoodsLowContext['plow'].values,mask=False)\n",
    "mean_lowHC = np.ma.array(computedLikelihoodsHighContext['mean_low'].values,mask=False)\n",
    "mean_highHC = np.ma.array(computedLikelihoodsHighContext['mean_high'].values,mask=False)\n",
    "sigmaHC = np.ma.array(computedLikelihoodsHighContext['sigma'].values,mask=False)\n",
    "ssHC = np.ma.array(computedLikelihoodsHighContext['ss'].values,mask=False)\n",
    "pbackHC = np.ma.array(computedLikelihoodsHighContext['pback'].values,mask=False)\n",
    "plowHC = np.ma.array(computedLikelihoodsHighContext['plow'].values,mask=False)\n",
    "\n",
    "mean_lowHC.mask[[20,40]] = True\n",
    "mean_highHC.mask[[20,40]] = True\n",
    "sigmaHC.mask[[20,40]] = True\n",
    "ssHC.mask[[20,40]] = True\n",
    "pbackHC.mask[[20,40]] = True\n",
    "plowHC.mask[[20,40]] = True\n",
    "mean_lowLC.mask[[25,45]] = True\n",
    "mean_highLC.mask[[25,45]] = True\n",
    "sigmaLC.mask[[25,45]] = True\n",
    "ssLC.mask[[25,45]] = True\n",
    "pbackLC.mask[[25,45]] = True\n",
    "plowLC.mask[[25,45]] = True\n",
    "\n",
    "averageSubjectParametersProbLC = np.zeros((6,))\n",
    "averageSubjectParametersProbLC[0] = np.nanmean(mean_lowLC)\n",
    "averageSubjectParametersProbLC[1] = np.nanmean(mean_highLC)\n",
    "averageSubjectParametersProbLC[2] = np.nanmean(sigmaLC)\n",
    "averageSubjectParametersProbLC[3] = np.nanmean(ssLC)\n",
    "averageSubjectParametersProbLC[4] = np.nanmean(pbackLC)\n",
    "averageSubjectParametersProbLC[5] = np.nanmean(plowLC)\n",
    "\n",
    "averageSubjectParametersProbHC = np.zeros((6,))\n",
    "averageSubjectParametersProbHC[0] = np.nanmean(mean_lowHC)\n",
    "averageSubjectParametersProbHC[1] = np.nanmean(mean_highHC)\n",
    "averageSubjectParametersProbHC[2] = np.nanmean(sigmaHC)\n",
    "averageSubjectParametersProbHC[3] = np.nanmean(ssHC)\n",
    "averageSubjectParametersProbHC[4] = np.nanmean(pbackHC)\n",
    "averageSubjectParametersProbHC[5] = np.nanmean(plowHC)\n",
    "\n",
    "averageSubjectBehaviourLC = np.empty(shape=(53,30))\n",
    "averageSubjectBehaviourHC = np.empty(shape=(48,30))\n",
    "numberOfSubsamples=20\n",
    "Test = pd.read_csv('subjectDataForPlots/allTrials_lowContext.csv')\n",
    "SubjectFiles = os.listdir('subjectDataForPlots/biasedLowContextData')\n",
    "for subjectIdx in range(53):  \n",
    "    filename = 'subjectDataForPlots/biasedLowContextData/'+SubjectFiles[subjectIdx]  \n",
    "    print(filename, subjectIdx)\n",
    "    Data = pd.read_csv(filename);\n",
    "    subjectBehaviourLC = np.empty((numberOfSubsamples,30))    \n",
    "    df_tones, df_toneKind, df_corrans, df_keys = extractData(csv_test=Test, \n",
    "                                                             csv_data=Data, \n",
    "                                                             exptTotalLength=800,\n",
    "                                                             exptLengthWithBreaks=804) \n",
    "\n",
    "    for iSubsample in range(numberOfSubsamples):\n",
    "        sampledlc_tones, sampledlc_behaviour = subsampleLongContextLow(toneslc=df_tones, \n",
    "                                                                       toneTypeslc=df_toneKind,\n",
    "                                                                       corranslc=df_corrans, \n",
    "                                                                       keyslc=df_keys)        \n",
    "        unique_tones_played, allPositionssubjectBehaviourLC = plottingInfluenceFn(sampledlc_tones, \n",
    "                                                                                  sampledlc_behaviour)  \n",
    "        subjectBehaviourLC[iSubsample,:] = np.mean(allPositionssubjectBehaviourLC,axis=1)\n",
    "    averageSubjectBehaviourLC[subjectIdx,:] = np.nanmean(subjectBehaviourLC,axis=0)\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.errorbar(np.log10(unique_tones_played), np.nanmean(averageSubjectBehaviour,axis=0), \n",
    "            yerr=np.nanstd(averageSubjectBehaviour,axis=0)/np.sqrt(53),color='k',linewidth=2)\n",
    "\n",
    "ax.errorbar(np.log10(unique_tones_played), np.nanmean(averageSubjectBehaviourLC,axis=0), \n",
    "            yerr=np.nanstd(averageSubjectBehaviourLC,axis=0)/np.sqrt(53),color='orange',linewidth=2)\n",
    "\n",
    "Test = pd.read_csv('subjectDataForPlots/allTrials_highContext.csv')\n",
    "SubjectFiles = os.listdir('subjectDataForPlots/biasedHighContextData')\n",
    "for subjectIdx in range(48): \n",
    "    filename = 'subjectDataForPlots/biasedHighContextData/'+SubjectFiles[subjectIdx] \n",
    "    print(filename, subjectIdx)\n",
    "    Data = pd.read_csv(filename);\n",
    "    subjectBehaviourHC = np.empty((numberOfSubsamples,30))\n",
    "    df_tones, df_toneKind, df_corrans, df_keys = extractData(csv_test=Test, \n",
    "                                                             csv_data=Data, \n",
    "                                                             exptTotalLength=800,\n",
    "                                                             exptLengthWithBreaks=804) \n",
    "\n",
    "    for iSubsample in range(numberOfSubsamples):\n",
    "        sampledhc_tones, sampledhc_behaviour = subsampleLongContextHigh(toneshc=df_tones, \n",
    "                                                                       toneTypeshc=df_toneKind,\n",
    "                                                                       corranshc=df_corrans, \n",
    "                                                                       keyshc=df_keys)        \n",
    "        unique_tones_played, allPositionssubjectBehaviourHC = plottingInfluenceFn(sampledhc_tones, \n",
    "                                                                                  sampledhc_behaviour)  \n",
    "        subjectBehaviourHC[iSubsample,:] = np.mean(allPositionssubjectBehaviourHC,axis=1)\n",
    "    averageSubjectBehaviourHC[subjectIdx,:] = np.nanmean(subjectBehaviourHC,axis=0)\n",
    "    \n",
    "ax.errorbar(np.log10(unique_tones_played), np.nanmean(averageSubjectBehaviourHC,axis=0), \n",
    "            yerr=np.nanstd(averageSubjectBehaviourHC,axis=0)/np.sqrt(48),color='brown',linewidth=2)\n",
    "\n",
    "ax.set_xticks(ticks=np.log10([100,1000,3000]))\n",
    "ax.set_xticklabels(labels=[100,1000,3000])\n",
    "ax.set_yticks(ticks=np.arange(0,1.1,0.2))\n",
    "ax.set_yticklabels(labels=np.around(np.arange(0,1.1,0.2),1))\n",
    "ax.tick_params(axis='both',labelsize=30,length=6,width=2)\n",
    "ax.set_xlabel('Frequency (Hz)',fontsize=32)\n",
    "ax.set_ylabel(r'p($\\rm{\\widehat{High}}$)',fontsize=32)\n",
    "makeAxesPretty(ax)\n",
    "\n",
    "#plt.savefig('figures/FromProlific/illustrations/AverageSubject_pBHgivenT_acrossAllSessions.pdf',\n",
    "#           bbox_inches='tight',transparent=True)\n",
    "\n",
    "longTermLearning_dict = {\"behaviourSubsampledHC\": averageSubjectBehaviourHC,\n",
    "                          \"behaviourSubsampledLC\": averageSubjectBehaviourLC,\n",
    "                          \"behaviourSubsampledNC\": averageSubjectBehaviour}\n",
    "\n",
    "scipy.io.savemat(\"longTermLearning_behaviour.mat\", longTermLearning_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36861909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figs. 1, 3A and 4D\n",
    "Qs: Experiment design and cartoon of psychometric curves\n",
    "\"\"\"\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(15,6))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in range(5,13):\n",
    "    ax1.plot([cntSeq-1,cntSeq,cntSeq+1],np.log10(df_tones[sequence]),'.',markersize=3)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8    \n",
    "ax1.set_xticks(ticks=cntSeqArray);\n",
    "ax1.set_xticklabels(labels=range(1,9),fontsize=33);\n",
    "ax1.set_yticks(ticks=np.log10([100,500,1000,1500,3000]));\n",
    "ax1.set_yticklabels(labels=[100,500,1000,1500,3000],fontsize=33);\n",
    "ax1.tick_params(axis='both',length=6,width=2)\n",
    "makeAxesPretty(ax1)\n",
    "ax1.set_xlabel('Trial number',fontsize=35)\n",
    "ax1.set_ylabel('$\\it{Unbiased}$ $\\it{Session}$ \\n Frequency (Hz)',fontsize=35)\n",
    "#plt.savefig('figures/FromProlific/illustrations/NoContextCompressed_trialsHeardBySubjectb52f.pdf', \n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(15,1))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in range(5,13):\n",
    "    ax1.plot([cntSeq-1,cntSeq,cntSeq+1],[1,1,1],'.',markersize=3)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8    \n",
    "ax1.set_xticks(ticks=cntSeqArray);\n",
    "ax1.set_xticklabels(labels=range(1,9),fontsize=33);\n",
    "ax1.set_yticks(ticks=[]);\n",
    "ax1.tick_params(axis='both',length=6,width=2)\n",
    "makeAxesPretty(ax1)\n",
    "print(df_toneKind[5:13,:],df_corrans[5:13])\n",
    "#plt.savefig('figures/FromProlific/illustrations/NoContextCompressed_trialsHeardBySubjectb52f.pdf', \n",
    "#             bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig, ax2 = plt.subplots(1,1, figsize=(15,1))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in range(5,13):\n",
    "    ax2.plot([cntSeq-1,cntSeq,cntSeq+1],[1,1,1],'.',markersize=3)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8\n",
    "ax2.set_xticks(ticks=cntSeqArray)\n",
    "ax2.set_xticklabels(labels=range(1,9),fontsize=33)\n",
    "ax2.set_yticks(ticks=[])\n",
    "ax2.tick_params(axis='both',length=6,width=2)\n",
    "makeAxesPretty(ax2)\n",
    "print(df_toneKindlc[5:13,:], df_corranslc[5:13])\n",
    "#plt.savefig('figures/FromProlific/illustrations/LowContextCompressed_trialsHeardBySubjectb52f.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig, ax3 = plt.subplots(1,1, figsize=(15,1))\n",
    "cntSeq = 1\n",
    "cntSeqArray = []\n",
    "for sequence in range(5,13):\n",
    "    ax3.plot([cntSeq-1,cntSeq,cntSeq+1],[1,1,1],'.',markersize=5)\n",
    "    cntSeqArray += [cntSeq]\n",
    "    cntSeq += 8   \n",
    "ax3.set_xticks(ticks=cntSeqArray);\n",
    "ax3.set_xticklabels(labels=range(1,9),fontsize=33)\n",
    "ax3.set_yticks(ticks=[])\n",
    "makeAxesPretty(ax3)\n",
    "ax3.tick_params(axis='both',length=6,width=2)\n",
    "ax3.set_xlabel('Trial number',fontsize=35)\n",
    "print(df_toneKindhc[5:13,:], df_corranshc[5:13])\n",
    "#plt.savefig('figures/FromProlific/illustrations/HighContextCompressed_trialsHeardBySubjectb52f.pdf', \n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "def psychometricCurve(tones, category):\n",
    "    unique_tones = np.unique(tones)\n",
    "    probabilityArray = np.zeros(len(unique_tones))\n",
    "    for toneIdx in range(len(unique_tones)):\n",
    "        probabilityArray[toneIdx] = np.mean(category[tones==unique_tones[toneIdx]])\n",
    "    return unique_tones, probabilityArray\n",
    "\n",
    "makeSchematicOfPsychometricCurve = 1\n",
    "if makeSchematicOfPsychometricCurve:\n",
    "    trial_list, dist_list, tone_kind_list = task(n_trials = 5000000, n_tones = 1, \n",
    "                                                                      p_low=0.5, p_back=0.3)\n",
    "    [uniqueTones, probCategories] = psychometricCurve(trial_list, dist_list)\n",
    "    tone_category_Spline = make_interp_spline(np.log10(uniqueTones), probCategories)\n",
    "    tones_ = np.linspace(np.log10(uniqueTones).min(), np.log10(uniqueTones).max(), 50)\n",
    "    category_ = tone_category_Spline(tones_)\n",
    "    # Plotting the Graph\n",
    "    fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "    ax.plot(tones_, category_, 'black', linewidth=2)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks(ticks=[0,0.5,1])\n",
    "    ax.set_yticklabels([0,0.5,1.0],fontsize=42)\n",
    "    makeAxesPretty(ax)\n",
    "    ax.set_xlabel('Frequency (Hz)',fontsize=45)\n",
    "    ax.set_ylabel('p(High)',fontsize=45)\n",
    "    plt.savefig('figures/FromProlific/illustrations/psychometricCurve_oneTone_allDistributions.pdf', \n",
    "                bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-mounting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Figs. 2E,G, extended Fig. 1H - how does performance vary with number and frequency of distractors?\n",
    "\"\"\"\n",
    "meanPerformance_lowCategory = np.array([87.09,77.38,64.61,46.53])\n",
    "stdPerformance_lowCategory = np.array([9.11,9.65,8,12.9])\n",
    "meanPerformance_highCategory = np.array([87.96,78.18,67.29,65.95])\n",
    "stdPerformance_highCategory = np.array([6.48,7.93,5.87,15.32])\n",
    "meanPerformance_bothCategories = np.array([87.75,78.06,65.98,56.22])\n",
    "stdPerformance_bothCategories = np.array([6.78,7.77,4.43,9.53])\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1, figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1, figsize=(8,6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n",
    "ax1.errorbar(np.arange(4),meanPerformance_lowCategory,\n",
    "             yerr=stdPerformance_lowCategory/np.sqrt(56),\n",
    "             color='orange',linestyle=\"None\",marker='o',markersize=12)\n",
    "ax1.errorbar(np.arange(4),meanPerformance_highCategory,\n",
    "             yerr=stdPerformance_highCategory/np.sqrt(56),\n",
    "             color='brown',linestyle=\"None\",marker='o',markersize=12)\n",
    "ax2.errorbar(np.arange(4),meanPerformance_bothCategories,\n",
    "             yerr=stdPerformance_bothCategories/np.sqrt(56),\n",
    "             color='black',linestyle=\"None\",marker='o',markersize=12)\n",
    "\n",
    "reg = LinearRegression().fit(np.reshape(np.arange(4),(-1,1)),meanPerformance_bothCategories)\n",
    "ax2.plot(np.arange(4), reg.predict(np.reshape(np.arange(4),(-1,1))), 'k--')\n",
    "\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.set_xlabel('Number of Distractors',fontsize=38)\n",
    "    ax.set_ylabel('Accuracy',fontsize=38)\n",
    "    ax.set_xticks([0,1,2,3])\n",
    "    ax.set_xticklabels(['0','1','2','3'])\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.yaxis.labelpad = -8\n",
    "    ax.tick_params(axis='both',labelsize=36,length=10,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "\n",
    "#fig1.savefig('figures/FromProlific/illustrations/PerformanceAccuracyInThePresenceOfDistractors_categoryDependence.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/PerformanceAccuracyInThePresenceOfDistractors_bothCategories.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "import pickle\n",
    "with open('subjectDataForPlots/PerformanceAccuracyChangesWithFrequencyOfDistractor.pickle', 'rb') as handle:\n",
    "    dict_behaviour = pickle.load(handle)\n",
    "unique_distractors_low = dict_behaviour['unique_distractors_low']\n",
    "averageBehaviorAcrossSubjects_low = dict_behaviour['averageBehaviorAcrossSubjects_low']\n",
    "unique_distractors_high = dict_behaviour['unique_distractors_high']\n",
    "averageBehaviorAcrossSubjects_high = dict_behaviour['averageBehaviorAcrossSubjects_high']\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "ax.plot(unique_distractors_low, \n",
    "         np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "         color='orange',linewidth=2)\n",
    "ax.plot(unique_distractors_high, \n",
    "         np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "         color='brown',linewidth=2)\n",
    "ax.fill_between(unique_distractors_low, \n",
    "                 y1 = np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1) - np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),\n",
    "                 y2 = np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1) + np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),\n",
    "                 color='moccasin',alpha=0.7)\n",
    "ax.fill_between(unique_distractors_high, \n",
    "                 y1 = np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1) - np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),\n",
    "                 y2 = np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1) + np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),\n",
    "                 color='lightcoral',alpha=0.7)\n",
    "ax.set_xticks(np.array([np.log10(100),np.log10(1000),np.log10(3000)]))\n",
    "ax.set_xticklabels([100,1000,3000])\n",
    "ax.set_ylim(0,100)\n",
    "ax.set_xlabel('Frequency of Distractor Tone(Hz)',fontsize=38)\n",
    "ax.set_ylabel('Accuracy',fontsize=38)\n",
    "ax.yaxis.labelpad = -8\n",
    "ax.tick_params(axis='both',labelsize=36,length=10,width=2)\n",
    "makeAxesPretty(ax)\n",
    "#plt.savefig('figures/FromProlific/illustrations/PerformanceAccuracyInThePresenceOfDistractors_partII.pdf',\n",
    "#            bbox_inches='tight',transparent=True)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(22,7))\n",
    "ax.bar(unique_distractors_low-0.02,np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "       width=0.03,yerr=np.std(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)/np.sqrt(56),color='orange')\n",
    "ax.bar(unique_distractors_high+0.02,np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "       width=0.03,yerr=np.std(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1)/np.sqrt(56),color='brown')\n",
    "ax.set_xticks(np.arange(2,3.5,0.1))\n",
    "ax.set_xticklabels(np.ceil(10**np.arange(2,3.5,0.1)).astype(int))\n",
    "ax.set_xlabel('Frequency of Distractor Tone Burst (Hz)',fontsize=32)\n",
    "ax.set_ylabel('Accuracy',fontsize=32)\n",
    "ax.tick_params(axis='both',labelsize=30,length=10,width=2)\n",
    "makeAxesPretty(ax)\n",
    "\n",
    "\n",
    "print(\"Correlation of low category accuracy with number of distractors\", pg.corr(np.arange(4),\n",
    "                                                                                meanPerformance_lowCategory,\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Correlation of high category accuracy with number of distractors\", pg.corr(np.arange(4),\n",
    "                                                                                meanPerformance_highCategory,\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Correlation of both categories accuracy with number of distractors\", pg.corr(np.arange(4),\n",
    "                                                                                    meanPerformance_bothCategories,\n",
    "                                                                                    method = 'spearman'))\n",
    "print(\"Correlation of low category accuracy with distractor frequency\", pg.corr(unique_distractors_low,\n",
    "                                                                                np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1),\n",
    "                                                                                method = 'spearman'))\n",
    "print(\"Correlation of high category accuracy with distractor frequency\", pg.corr(unique_distractors_high,\n",
    "                                                                                np.mean(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1),\n",
    "                                                                                method = 'spearman'))\n",
    "\n",
    "print(\"Comparing central frequency using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[8],\n",
    "                                                                averageBehaviorAcrossSubjects_high[8]))\n",
    "print(\"Comparing one frequency before center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[7],\n",
    "                                                                          averageBehaviorAcrossSubjects_high[7]))\n",
    "print(\"Comparing one frequency after center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[9],\n",
    "                                                                         averageBehaviorAcrossSubjects_high[9]))\n",
    "print(\"Comparing two frequencies before center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[6],\n",
    "                                                                            averageBehaviorAcrossSubjects_high[6]))\n",
    "print(\"Comparing two frequencies after center using wilcoxon\", pg.wilcoxon(averageBehaviorAcrossSubjects_low[10],\n",
    "                                                                           averageBehaviorAcrossSubjects_high[10]))\n",
    "print(\"Median and iqr of central frequency accuracies\", \"low\",\n",
    "      np.median(averageBehaviorAcrossSubjects_low[10]),scipy.stats.iqr(averageBehaviorAcrossSubjects_low[10]),\n",
    "      \"high\", np.median(averageBehaviorAcrossSubjects_high[10]),scipy.stats.iqr(averageBehaviorAcrossSubjects_high[10]))\n",
    "print(\"Average accuracy for first 7 distractor tone frequencies\", \n",
    "      np.nanmean(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[:8]),\n",
    "      np.nanstd(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[:8])/np.sqrt(7))\n",
    "print(\"Average accuracy for last 7 distractor tone frequencies\", \n",
    "      np.nanmean(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[9:]),\n",
    "      np.nanstd(np.mean(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1)[9:])/np.sqrt(7))\n",
    "print(\"Median Accuracy for low category trials\",np.median(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1))\n",
    "print(\"Median Accuracy for high category trials\",np.median(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1))\n",
    "print(\"IQR Accuracy for low category trials\",scipy.stats.iqr(np.array(averageBehaviorAcrossSubjects_low)*100,axis=1))\n",
    "print(\"IQR Accuracy for high category trials\",scipy.stats.iqr(np.array(averageBehaviorAcrossSubjects_high)*100,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-child",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Range of sigma sensory across the subject population. \n",
    "\"\"\"\n",
    "indices = np.arange(56)\n",
    "sigmaSensory = computedLikelihoods['ss'].values\n",
    "sigmaSensory = sigmaSensory[~np.isnan(sigmaSensory)]\n",
    "plt.hist(sigmaSensory)\n",
    "print(np.quantile(sigmaSensory, 0.5), np.quantile(sigmaSensory, 0.1), np.quantile(sigmaSensory, 0.90), \n",
    "      sum(sigmaSensory>np.quantile(sigmaSensory, 0.95)),indices[sigmaSensory>np.quantile(sigmaSensory, 0.95)])\n",
    "\n",
    "\"\"\"\n",
    "Qs: Can we split participants up into two categories as a first pass? And then everybody else goes on a continuum.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The following variables are for bic values of the subsampled dataset.\n",
    "\"\"\"\n",
    "nll_median_probModel = computedLikelihoods['medianProb'].values\n",
    "nll_median_probModel = nll_median_probModel[~np.isnan(nll_median_probModel)]\n",
    "\n",
    "nll_median_signalModel = computedLikelihoods['medianSignal'].values\n",
    "nll_median_signalModel = nll_median_signalModel[~np.isnan(nll_median_signalModel)]\n",
    "\n",
    "nll_median_randomModel = computedLikelihoods['medianRandom'].values\n",
    "nll_median_randomModel = nll_median_randomModel[~np.isnan(nll_median_randomModel)]\n",
    "\n",
    "nll_lowerErrorbars_probModel = computedLikelihoods['5thPercentileProb'].values\n",
    "nll_lowerErrorbars_probModel = nll_lowerErrorbars_probModel[~np.isnan(nll_lowerErrorbars_probModel)]\n",
    "\n",
    "nll_upperErrorbars_probModel = computedLikelihoods['95thPercentileProb'].values\n",
    "nll_upperErrorbars_probModel = nll_upperErrorbars_probModel[~np.isnan(nll_upperErrorbars_probModel)]\n",
    "\n",
    "nll_lowerErrorbars_signalModel = computedLikelihoods['5thPercentileSignal'].values\n",
    "nll_lowerErrorbars_signalModel = nll_lowerErrorbars_signalModel[~np.isnan(nll_lowerErrorbars_signalModel)]\n",
    "\n",
    "nll_upperErrorbars_signalModel = computedLikelihoods['95thPercentileSignal'].values\n",
    "nll_upperErrorbars_signalModel = nll_upperErrorbars_signalModel[~np.isnan(nll_upperErrorbars_signalModel)]\n",
    "\n",
    "nll_lowerErrorbars_randomModel = computedLikelihoods['5thPercentileRandom'].values\n",
    "nll_lowerErrorbars_randomModel = nll_lowerErrorbars_randomModel[~np.isnan(nll_lowerErrorbars_randomModel)]\n",
    "\n",
    "nll_upperErrorbars_randomModel = computedLikelihoods['95thPercentileRandom'].values\n",
    "nll_upperErrorbars_randomModel = nll_upperErrorbars_randomModel[~np.isnan(nll_upperErrorbars_randomModel)]\n",
    "\n",
    "\"\"\"\n",
    "Fig. 2F\n",
    "Qs: Where do subjects lie in the one irrelevant tone and two irrelevant tones space?\n",
    "\"\"\"\n",
    "performance = pd.read_excel(xls,'Strategy_PerformanceAccuracies',nrows = 57)\n",
    "numberOfNoResponses = performance['NumberOfNoResponsesNoContext'].values\n",
    "numberOfNoResponses = numberOfNoResponses[~np.isnan(numberOfNoResponses)]\n",
    "distractorPerformance = pd.read_excel(xls,'PerformanceVsNumDistractors',nrows=56)\n",
    "OverallAccuracy = performance['OverallAccuracyNoContext'].values\n",
    "OverallAccuracy = OverallAccuracy[~np.isnan(OverallAccuracy)]\n",
    "OneIrrelevantToneAccuracy = np.ma.array(distractorPerformance['CombinedAcrossCategoriesOneDistractor'].values,mask=False)\n",
    "TwoIrrelevantTonesAccuracy = np.ma.array(distractorPerformance['CombinedAcrossCategoriesTwoDistractors'].values,mask=False)\n",
    "\n",
    "fig0, ax0 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.35, hspace=None)\n",
    "for i in range(len(nll_median_probModel)):\n",
    "    ax0.errorbar(2*nll_median_randomModel[i]+2*np.log(600),\n",
    "                 2*nll_median_probModel[i]+6*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_randomModel[i]+2*nll_median_randomModel[i]],\n",
    "                      [2*nll_upperErrorbars_randomModel[i]-2*nll_median_randomModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModel[i]+2*nll_median_probModel[i]],\n",
    "                      [2*nll_upperErrorbars_probModel[i]-2*nll_median_probModel[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax1.errorbar(2*nll_median_signalModel[i]+5*np.log(600),\n",
    "                 2*nll_median_probModel[i]+6*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_signalModel[i]+2*nll_median_signalModel[i]],\n",
    "                      [2*nll_upperErrorbars_signalModel[i]-2*nll_median_signalModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModel[i]+2*nll_median_probModel[i]],\n",
    "                      [2*nll_upperErrorbars_probModel[i]-2*nll_median_probModel[i]]],\n",
    "                 color='k',marker='o')\n",
    "    \n",
    "ax0.plot(np.arange(300,900),np.arange(300,900),'k--')\n",
    "ax0.set_xticks([300,500,700,900]); ax0.set_yticks([300,500,700,900])\n",
    "ax1.plot(np.arange(300,1450),np.arange(300,1450),'k--')\n",
    "ax1.set_xticks([500,800,1100,1400]); ax1.set_yticks([500,800,1100,1400])\n",
    "\n",
    "ax0.set_xlabel('BIC of Random Choice Strategy', fontsize=34)\n",
    "ax1.set_xlabel('BIC of Signal Strategy', fontsize=34)\n",
    "for ax in [ax0,ax1]:\n",
    "    ax.set_ylabel('BIC of Probabilistic Strategy',fontsize=34)\n",
    "    ax.tick_params(axis='both',labelsize=32,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "    \n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6)) \n",
    "ax2.plot(OneIrrelevantToneAccuracy,TwoIrrelevantTonesAccuracy,'ko')\n",
    "ax2.set_xlim([50,89.5])\n",
    "ax2.set_ylim([50,89.5])\n",
    "ax2.plot(np.arange(50,89.5),np.arange(50,89.5),'k--')\n",
    "ax2.set_xlabel('Accuracy for Trials \\n with One Distractor Tone',fontsize=38)\n",
    "ax2.set_ylabel('Accuracy for Trials with \\n Two Distractor Tones',fontsize=38)\n",
    "ax2.tick_params(axis='both',labelsize=36,length=6,width=2)\n",
    "makeAxesPretty(ax2)\n",
    "\n",
    "#fig2.savefig('figures/FromProlific/illustrations/variabilityInAccuracy.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "for i in range(len(nll_median_probModel)):      \n",
    "    ax3.errorbar(-2*nll_median_signalModel[i]-5*np.log(600)\n",
    "                 +2*nll_median_probModel[i]+6*np.log(600), \n",
    "                 OverallAccuracy[i],\n",
    "                 xerr=[[2*nll_upperErrorbars_signalModel[i]-2*nll_lowerErrorbars_probModel[i]\n",
    "                       -2*nll_median_signalModel[i]+2*nll_median_probModel[i]],\n",
    "                       [-2*nll_lowerErrorbars_signalModel[i]+2*nll_upperErrorbars_probModel[i]\n",
    "                       +2*nll_median_signalModel[i]-2*nll_median_probModel[i]]],\n",
    "                 yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                        /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                 color='k',marker='o',ecolor='k')\n",
    "\n",
    "ax3.set_xlabel('BIC of Probabilistic Strategy - \\n BIC of Signal Strategy', fontsize=26)\n",
    "ax3.set_ylabel('Accuracy',fontsize=26)\n",
    "ax3.tick_params(axis='both',labelsize=24,length=6,width=2)\n",
    "makeAxesPretty(ax3)\n",
    "\n",
    "OneIrrelevantToneAccuracy.mask[[29,34]]=True\n",
    "TwoIrrelevantTonesAccuracy.mask[[29,34]]=True\n",
    "print(\"Median and IQR of one distractor tone trial accuracy\",np.median(OneIrrelevantToneAccuracy),\n",
    "     scipy.stats.iqr(OneIrrelevantToneAccuracy))\n",
    "print(\"Median and IQR of two distractor tones trial accuracy\",np.median(TwoIrrelevantTonesAccuracy),\n",
    "     scipy.stats.iqr(TwoIrrelevantTonesAccuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d889c34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figs. 4A-C, extended data Figs. 4F,G: comparison of models\n",
    "\"\"\"\n",
    "nll_median_probModelVeridical = computedLikelihoodsVeridical['medianProb'].values\n",
    "nll_median_probModelVeridical = nll_median_probModelVeridical[~np.isnan(nll_median_probModelVeridical)]\n",
    "\n",
    "nll_lowerErrorbars_probModelVeridical = computedLikelihoodsVeridical['5thPercentileProb'].values\n",
    "nll_lowerErrorbars_probModelVeridical = nll_lowerErrorbars_probModelVeridical[~np.isnan(nll_lowerErrorbars_probModelVeridical)]\n",
    "\n",
    "nll_upperErrorbars_probModelVeridical = computedLikelihoodsVeridical['95thPercentileProb'].values\n",
    "nll_upperErrorbars_probModelVeridical = nll_upperErrorbars_probModelVeridical[~np.isnan(nll_upperErrorbars_probModelVeridical)]\n",
    "\n",
    "nll_median_signalModelVeridical = computedLikelihoodsVeridical['medianSignal'].values\n",
    "nll_median_signalModelVeridical = nll_median_signalModelVeridical[~np.isnan(nll_median_signalModelVeridical)]\n",
    "\n",
    "nll_lowerErrorbars_signalModelVeridical = computedLikelihoodsVeridical['5thPercentileSignal'].values\n",
    "nll_lowerErrorbars_signalModelVeridical = nll_lowerErrorbars_signalModelVeridical[~np.isnan(nll_lowerErrorbars_signalModelVeridical)]\n",
    "\n",
    "nll_upperErrorbars_signalModelVeridical = computedLikelihoodsVeridical['95thPercentileSignal'].values\n",
    "nll_upperErrorbars_signalModelVeridical = nll_upperErrorbars_signalModelVeridical[~np.isnan(nll_upperErrorbars_signalModelVeridical)]\n",
    "\n",
    "nll_median_probModelPBackVeridical = computedLikelihoodsVeridical['medianProbPBackVeridical'].values\n",
    "nll_median_probModelPBackVeridical = nll_median_probModelPBackVeridical[~np.isnan(nll_median_probModelPBackVeridical)]\n",
    "\n",
    "nll_lowerErrorbars_probModelPBackVeridical = computedLikelihoodsVeridical['5thPercentileProbPBackVeridical'].values\n",
    "nll_lowerErrorbars_probModelPBackVeridical = nll_lowerErrorbars_probModelPBackVeridical[~np.isnan(nll_lowerErrorbars_probModelPBackVeridical)]\n",
    "\n",
    "nll_upperErrorbars_probModelPBackVeridical = computedLikelihoodsVeridical['95thPercentileProbPBackveridical'].values\n",
    "nll_upperErrorbars_probModelPBackVeridical = nll_upperErrorbars_probModelPBackVeridical[~np.isnan(nll_upperErrorbars_probModelPBackVeridical)]\n",
    "\n",
    "pback_median_Veridical = computedLikelihoodsVeridical['medianPBack'].values\n",
    "pback_median_Veridical = pback_median_Veridical[~np.isnan(pback_median_Veridical)]\n",
    "\n",
    "pback_lowerErrorbars_Veridical = computedLikelihoodsVeridical['5thPercentilePBack'].values\n",
    "pback_lowerErrorbars_Veridical = pback_lowerErrorbars_Veridical[~np.isnan(pback_lowerErrorbars_Veridical)]\n",
    "\n",
    "pback_upperErrorbars_Veridical = computedLikelihoodsVeridical['95thPercentilePBack'].values\n",
    "pback_upperErrorbars_Veridical = pback_upperErrorbars_Veridical[~np.isnan(pback_upperErrorbars_Veridical)]\n",
    "\n",
    "ss_median_Veridical = computedLikelihoodsVeridical['ss'].values\n",
    "ss_median_Veridical = ss_median_Veridical[~np.isnan(ss_median_Veridical)]\n",
    "\n",
    "sigmoidicity_median_Veridical = computedLikelihoodsVeridical['medianRelevanceMetric'].values\n",
    "sigmoidicity_median_Veridical = sigmoidicity_median_Veridical[~np.isnan(sigmoidicity_median_Veridical)]\n",
    "\n",
    "sigmoidicity_lowerErrorbars_Veridical = computedLikelihoodsVeridical['5thPercentileRelevanceMetric'].values\n",
    "sigmoidicity_lowerErrorbars_Veridical = sigmoidicity_lowerErrorbars_Veridical[~np.isnan(sigmoidicity_lowerErrorbars_Veridical)]\n",
    "\n",
    "sigmoidicity_upperErrorbars_Veridical = computedLikelihoodsVeridical['95thPercentileRelevanceMetric'].values\n",
    "sigmoidicity_upperErrorbars_Veridical = sigmoidicity_upperErrorbars_Veridical[~np.isnan(sigmoidicity_upperErrorbars_Veridical)]\n",
    "\n",
    "fig0, ax0 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig4, ax4 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig5, ax5 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig6, ax6 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(len(nll_median_probModelVeridical)):\n",
    "    ax0.errorbar(2*nll_median_probModel[i]+6*np.log(600),\n",
    "                 2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_probModel[i]+2*nll_median_probModel[i]],\n",
    "                      [2*nll_upperErrorbars_probModel[i]-2*nll_median_probModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_probModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax1.errorbar(2*nll_median_signalModel[i]+5*np.log(600),\n",
    "                 2*nll_median_signalModelVeridical[i]+2*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_signalModel[i]+2*nll_median_signalModel[i]],\n",
    "                      [2*nll_upperErrorbars_signalModel[i]-2*nll_median_signalModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_median_signalModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_median_signalModelVeridical[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax2.errorbar(2*nll_median_randomModel[i]+2*np.log(600),\n",
    "                 2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_randomModel[i]+2*nll_median_randomModel[i]],\n",
    "                      [2*nll_upperErrorbars_randomModel[i]-2*nll_median_randomModel[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_probModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax3.errorbar(2*nll_median_signalModelVeridical[i]+2*np.log(600),\n",
    "                 2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_median_signalModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_median_signalModelVeridical[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_probModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 color='k',marker='o')\n",
    "    ax4.errorbar(-2*nll_median_signalModelVeridical[i]-2*np.log(600)\n",
    "                 +2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 OverallAccuracy[i],\n",
    "                 xerr=[[2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_lowerErrorbars_probModelVeridical[i]\n",
    "                       -2*nll_median_signalModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                       [-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_upperErrorbars_probModelVeridical[i]\n",
    "                       +2*nll_median_signalModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                        /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                 color='k',marker='o',ecolor='k')\n",
    "    ax5.errorbar(-2*nll_median_signalModelVeridical[i]-2*np.log(600)\n",
    "                 +2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 pback_median_Veridical[i],\n",
    "                 xerr=[[2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_lowerErrorbars_probModelVeridical[i]\n",
    "                       -2*nll_median_signalModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                       [-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_upperErrorbars_probModelVeridical[i]\n",
    "                       +2*nll_median_signalModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 color='k',marker='o',ecolor='k')\n",
    "for i in [36,25,29]:\n",
    "    ax6.errorbar(-2*nll_median_signalModelVeridical[i]-2*np.log(600)\n",
    "                 +2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 OverallAccuracy[i],\n",
    "                 xerr=[[2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_lowerErrorbars_probModelVeridical[i]\n",
    "                       -2*nll_median_signalModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                       [-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_upperErrorbars_probModelVeridical[i]\n",
    "                       +2*nll_median_signalModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                        /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                 color='k',marker='o',ecolor='k')\n",
    "    \n",
    "for i in [23,35]:\n",
    "    ax3.errorbar(2*nll_median_signalModelVeridical[i]+2*np.log(600),\n",
    "                 2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 xerr=[[-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_median_signalModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_median_signalModelVeridical[i]]],\n",
    "                 yerr=[[-2*nll_lowerErrorbars_probModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                      [2*nll_upperErrorbars_probModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 color='deepskyblue',marker='o')\n",
    "    ax4.errorbar(-2*nll_median_signalModelVeridical[i]-2*np.log(600)\n",
    "                 +2*nll_median_probModelVeridical[i]+3*np.log(600), \n",
    "                 OverallAccuracy[i],\n",
    "                 xerr=[[2*nll_upperErrorbars_signalModelVeridical[i]-2*nll_lowerErrorbars_probModelVeridical[i]\n",
    "                       -2*nll_median_signalModelVeridical[i]+2*nll_median_probModelVeridical[i]],\n",
    "                       [-2*nll_lowerErrorbars_signalModelVeridical[i]+2*nll_upperErrorbars_probModelVeridical[i]\n",
    "                       +2*nll_median_signalModelVeridical[i]-2*nll_median_probModelVeridical[i]]],\n",
    "                 yerr = [np.sqrt((600-numberOfNoResponses[i])*OverallAccuracy[i]/100*(1-OverallAccuracy[i]/100))\n",
    "                        /np.sqrt(600-numberOfNoResponses[i])],\n",
    "                 color='deepskyblue',marker='o',ecolor='deepskyblue')\n",
    "    \n",
    "ax0.plot(np.arange(300,800),np.arange(300,800),'k--')\n",
    "ax0.set_xticks([300,500,800])\n",
    "ax0.set_yticks([300,500,800,])\n",
    "ax0.set_xlabel('BIC of full Bayesian model \\n fitting all parameters', fontsize=35) \n",
    "ax0.set_ylabel('BIC of full Bayesian model \\n with fixed Gaussian parameters', fontsize=35)  \n",
    "ax1.plot(np.arange(300,1700),np.arange(300,1700),'k--')\n",
    "ax1.set_xticks([300,500,800,1100,1400,1700])\n",
    "ax1.set_yticks([300,500,800,1100,1400,1700])\n",
    "ax1.set_xlabel('BIC of no-distractor model \\n fitting all parameters', fontsize=35)\n",
    "ax1.set_ylabel('BIC of no-distractor model \\n with fixed Gaussian parameters',fontsize=35)\n",
    "ax2.plot(np.arange(300,900),np.arange(300,900),'k--')\n",
    "ax2.set_xticks([300,500,700,900])\n",
    "ax2.set_yticks([300,500,700,900])\n",
    "ax2.set_xlabel('BIC of random-guess model', fontsize=30)\n",
    "ax3.plot(np.arange(300,1400),np.arange(300,1400),'k--')\n",
    "ax3.set_xticks([300,700,1100,1500])\n",
    "ax3.set_yticks([300,700,1100,1500])\n",
    "ax3.set_xlabel('BIC of no-distractor model', fontsize=30)\n",
    "for ax in [ax2,ax3]:\n",
    "    ax.set_ylabel('BIC of full Bayesian model',fontsize=30)\n",
    "for ax in [ax4,ax5,ax6]:\n",
    "    ax.set_xlabel('BIC of full Bayesian model - \\n BIC of no-distractor model', fontsize=30)\n",
    "for ax in [ax4, ax6]:\n",
    "        ax.set_ylabel('Accuracy',fontsize=30)\n",
    "ax5.set_ylabel('p$_{distractor}$',fontsize=30)\n",
    "for ax in [ax0,ax1]:\n",
    "    ax.tick_params(axis='both',labelsize=33,length=6,width=2)   \n",
    "    makeAxesPretty(ax)\n",
    "for ax in [ax2,ax3,ax4,ax5,ax6]:\n",
    "    ax.tick_params(axis='both',labelsize=28,length=6,width=2)   \n",
    "    makeAxesPretty(ax)\n",
    "\n",
    "\n",
    "#fig0.savefig('figures/FromProlific/illustrations/comparingUnbiasedContextFittingProceduresProb.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)  \n",
    "#fig1.savefig('figures/FromProlific/illustrations/comparingUnbiasedContextFittingProceduresSignal.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategyRandomvsProb.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "#fig3.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategySignalvsProb.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "#fig4.savefig('figures/FromProlific/illustrations/correlationOfVariabilityInAccuracyWithDiffInStategyBICs.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "#fig6.savefig('figures/FromProlific/illustrations/correlationOfVariabilityInAccuracyWithDiffInStategyBICsRepresentativeSubjects.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "print(\"Average difference in nll \", np.mean(nll_median_probModelPBackVeridical-nll_median_probModelVeridical),\n",
    "     np.max(nll_median_probModelPBackVeridical-nll_median_probModelVeridical),\n",
    "     np.min(nll_median_probModelPBackVeridical-nll_median_probModelVeridical))\n",
    "plt.figure()\n",
    "plt.hist(nll_median_probModelPBackVeridical-nll_median_probModelVeridical)\n",
    "\n",
    "\"\"\"\n",
    "Qs: what is the distribution of the bic values of the subjects?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Median and IQR of random model BICs\",np.median(2*nll_median_randomModel+2*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_median_randomModel+2*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of signal model veridical BICs\",np.median(2*nll_median_signalModelVeridical+2*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_median_signalModelVeridical+2*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of probabilistic model veridical BICs\",np.median(2*nll_median_probModelVeridical+3*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_median_probModelVeridical+3*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of signal model BICs\",np.median(2*nll_median_signalModel+5*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_median_signalModel+5*np.log(600)))\n",
    "\n",
    "print(\"Median and IQR of probabilistic model BICs\",np.median(2*nll_median_probModel+6*np.log(600)),\n",
    "      scipy.stats.iqr(2*nll_median_probModel+6*np.log(600)))\n",
    "\n",
    "print(\"Comparing random and probabilistic model BICs using wilcoxon\", \n",
    "      pg.wilcoxon(2*nll_median_randomModel+2*np.log(600),\n",
    "                  2*nll_median_probModelVeridical+3*np.log(600)))\n",
    "print(\"Comparing signal and probabilistic model BICs using wilcoxon\", \n",
    "      pg.wilcoxon(2*nll_median_signalModelVeridical+2*np.log(600),\n",
    "                  2*nll_median_probModelVeridical+3*np.log(600)))\n",
    "print(\"Comparing probabilistic model veridical and otherwise BICs using wilcoxon\", \n",
    "      pg.wilcoxon(2*nll_median_probModelVeridical+3*np.log(600),\n",
    "                  2*nll_median_probModel+6*np.log(600)))\n",
    "print(\"Comparing signal model veridical and otherwise BICs using wilcoxon\", \n",
    "      pg.wilcoxon(2*nll_median_signalModelVeridical+2*np.log(600),\n",
    "                  2*nll_median_signalModel+5*np.log(600)))\n",
    "print(\"Correlation of difference in BICs with accuracy\",\n",
    "      pg.corr(-2*nll_median_signalModelVeridical-2*np.log(600)\n",
    "              +2*nll_median_probModelVeridical+3*np.log(600), OverallAccuracy,method='spearman'))\n",
    "print(\"Correlation of difference in BICs with pdistractor\",\n",
    "      pg.corr(-2*nll_median_signalModelVeridical-2*np.log(600)\n",
    "              +2*nll_median_probModelVeridical+3*np.log(600), pback_median_Veridical,method='spearman'))\n",
    "\n",
    "print(\"Correlation of pdistractor with accuracy for veridical fit\",\n",
    "      pg.corr(pback_median_Veridical, OverallAccuracy,method='spearman'))\n",
    "\n",
    "print(\"Correlation of sigma sensory with accuracy\",\n",
    "      pg.corr(ss_median_Veridical, OverallAccuracy,method='spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-terrain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figs. 4E,F\n",
    "Testing effect of sigma sensory on category choice and accuracy given p_distractor value.\n",
    "\"\"\"\n",
    "for iSubj in [23,35]:\n",
    "    pback_median_Veridical[iSubj] = 0\n",
    "\n",
    "print(\"Average pdistractor value\", np.mean(pback_median_Veridical), np.std(pback_median_Veridical))\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "for iSubject in np.arange(len(ss_median_Veridical)):\n",
    "    ax1.errorbar(ss_median_Veridical[iSubject], sigmoidicity_median_Veridical[iSubject], marker='o',\n",
    "             yerr=[[-sigmoidicity_lowerErrorbars_Veridical[iSubject]+sigmoidicity_median_Veridical[iSubject]],\n",
    "                   [sigmoidicity_upperErrorbars_Veridical[iSubject]-sigmoidicity_median_Veridical[iSubject]]],\n",
    "             color=[pback_median_Veridical[iSubject], 0, 0])\n",
    "    ax2.plot(ss_median_Veridical[iSubject], OverallAccuracy[iSubject], 'o',\n",
    "             color=[pback_median_Veridical[iSubject], 0, 0])\n",
    "    \n",
    "for ax in [ax1,ax2]:\n",
    "    ax.set_xticks([0,0.1,0.2,0.3,0.4,0.5])\n",
    "    ax.set_xlabel('Sensory Uncertainty', fontsize=30)\n",
    "    ax.tick_params(axis='both',labelsize=28,length=6,width=2)   \n",
    "    makeAxesPretty(ax)\n",
    "ax1.set_ylim([-0.02,1])\n",
    "ax1.set_yticks([0,0.2,0.4,0.6,0.8,1])\n",
    "ax1.set_ylabel('Sigmoidicity', fontsize=30)\n",
    "\n",
    "ax2.set_ylim([58,90])\n",
    "ax2.set_yticks([60,70,80,90])\n",
    "ax2.set_ylabel('Accuracy', fontsize=30) \n",
    "#fig1.savefig('figures/FromProlific/illustrations/effectOfSensoryAndRelevanceUncertaintyOnCategoryChoice.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True) \n",
    "#fig2.savefig('figures/FromProlific/illustrations/effectOfSensoryAndRelevanceUncertaintyOnAccuracy.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True) \n",
    "\n",
    "\"\"\"\n",
    "Anova analysis for effect of p_distractor and sigma sensory on accuracy and on category choice\n",
    "\"\"\" \n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=2,interaction_only=True,include_bias = False)\n",
    "X_forPoly = np.array([ss_median_Veridical, pback_median_Veridical]).T \n",
    "X_withInteraction = poly.fit_transform(X_forPoly)\n",
    "\n",
    "X_withInteraction = sm.add_constant(X_withInteraction)\n",
    "model_withInteraction = sm.OLS(OverallAccuracy, X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=2,interaction_only=True,include_bias = False)\n",
    "X_forPoly = np.array([ss_median_Veridical, pback_median_Veridical]).T \n",
    "X_withInteraction = poly.fit_transform(X_forPoly)\n",
    "\n",
    "X_withInteraction = sm.add_constant(X_withInteraction)\n",
    "model_withInteraction = sm.OLS(sigmoidicity_median_Veridical, X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443dc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extended data Figs. 4A-E.\n",
    "Qs: how can we relate performance to metrics from the bayesian model?\n",
    "\"\"\"\n",
    "pbackMedian = computedLikelihoods['medianPBack'].values\n",
    "pbackLowerErrorBars = computedLikelihoods['5thPercentilePBack'].values\n",
    "pbackUpperErrorBars = computedLikelihoods['95thPercentilePBack'].values\n",
    "pbackMedian = pbackMedian[~np.isnan(pbackMedian)]\n",
    "pbackLowerErrorBars = pbackLowerErrorBars[~np.isnan(pbackLowerErrorBars)]\n",
    "pbackUpperErrorBars = pbackUpperErrorBars[~np.isnan(pbackUpperErrorBars)]\n",
    "\n",
    "ssMedian = computedLikelihoods['ss'].values\n",
    "ssMedian = ssMedian[~np.isnan(ssMedian)]\n",
    "sigmaMedian = computedLikelihoods['sigma'].values\n",
    "sigmaMedian = sigmaMedian[~np.isnan(sigmaMedian)]\n",
    "meanLowMedian = computedLikelihoods['mean_low'].values\n",
    "meanLowMedian = meanLowMedian[~np.isnan(meanLowMedian)]\n",
    "meanHighMedian = computedLikelihoods['mean_high'].values\n",
    "meanHighMedian = meanHighMedian[~np.isnan(meanHighMedian)]\n",
    "\n",
    "posteriorNorm = computedLikelihoods['medianRelevanceMetric'].values\n",
    "posteriorNorm = posteriorNorm[~np.isnan(posteriorNorm)]\n",
    "posteriorNormLowerErrorBars = computedLikelihoods['5thPercentileRelevanceMetric'].values\n",
    "posteriorNormLowerErrorBars = posteriorNormLowerErrorBars[~np.isnan(posteriorNormLowerErrorBars)]\n",
    "posteriorNormUpperErrorBars = computedLikelihoods['95thPercentileRelevanceMetric'].values\n",
    "posteriorNormUpperErrorBars = posteriorNormUpperErrorBars[~np.isnan(posteriorNormUpperErrorBars)]\n",
    "\n",
    "print(f\"Sigma sensory mean is {np.mean(sigmaSensory)} and standard deviation is {np.std(sigmaSensory)}\")\n",
    "print(f\"Subjects with high sigma sensory are {np.argwhere(sigmaSensory>np.mean(sigmaSensory)+np.std(sigmaSensory))}\")\n",
    "\n",
    "OneIrrelevantToneAccuracy.mask[[29,34]]=False\n",
    "TwoIrrelevantTonesAccuracy.mask[[29,34]]=False\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "for iSubj in range(len(posteriorNorm)):\n",
    "    ax1.errorbar(posteriorNorm[iSubj],OverallAccuracy[iSubj],\n",
    "                xerr=[[-posteriorNormLowerErrorBars[iSubj]+posteriorNorm[iSubj]],\n",
    "                      [posteriorNormUpperErrorBars[iSubj]-posteriorNorm[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "ax1.set_ylabel('Accuracy',fontsize=30)\n",
    "ax1.set_xlabel('Relevance Metric',fontsize=30)\n",
    "ax1.tick_params(axis='both',labelsize=28,length=6,width=2)\n",
    "makeAxesPretty(ax1)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/overallAccuracyGivenPosteriorShape.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "print(\"Correlation of pback and relevance metric with accuracies (overall and across trial type)\")\n",
    "print(pg.corr(pbackMedian,TwoIrrelevantTonesAccuracy,method='spearman'))\n",
    "print(pg.corr(pbackMedian,OneIrrelevantToneAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,TwoIrrelevantTonesAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,OneIrrelevantToneAccuracy,method='spearman'))\n",
    "print(pg.corr(pbackMedian,OverallAccuracy,method='spearman'))\n",
    "print(pg.corr(posteriorNorm,OverallAccuracy,method='spearman'))\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "for iSubj in range(len(ssMedian)):\n",
    "    ax1.plot(ssMedian[iSubj], OverallAccuracy[iSubj],\n",
    "            color='k',marker='o')\n",
    "ax1.set_xlabel('Sensory Uncertainty ($\\\\sigma_{sensory}$)',fontsize=30)\n",
    "ax1.set_ylabel('Accuracy',fontsize=30)\n",
    "ax1.tick_params(axis='both',labelsize=28,length=6,width=2)\n",
    "makeAxesPretty(ax1)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/overallAccuracyGivenSensoryUncertainty.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig4, ax4 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig5, ax5 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig6, ax6 = plt.subplots(1,1,figsize=(8,6))\n",
    "for iSubj in range(len(OverallAccuracy)):\n",
    "    ax1.errorbar(meanLowMedian[iSubj], OverallAccuracy[iSubj], color='k',marker='o')\n",
    "    ax2.errorbar(meanHighMedian[iSubj], OverallAccuracy[iSubj], color='k',marker='o')\n",
    "    ax3.errorbar(sigmaMedian[iSubj], OverallAccuracy[iSubj], color='k',marker='o')\n",
    "    ax4.errorbar(pbackMedian[iSubj], OverallAccuracy[iSubj], \n",
    "                 xerr=[[-pbackLowerErrorBars[iSubj]+pbackMedian[iSubj]],\n",
    "                      [pbackUpperErrorBars[iSubj]-pbackMedian[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax5.errorbar(pback_median_Veridical[iSubj], OverallAccuracy[iSubj], \n",
    "                 xerr=[[-pback_lowerErrorbars_Veridical[iSubj]+pback_median_Veridical[iSubj]],\n",
    "                      [pback_upperErrorbars_Veridical[iSubj]-pback_median_Veridical[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "    ax6.errorbar(pback_median_Veridical[iSubj], ss_median_Veridical[iSubj], \n",
    "                 xerr=[[-pback_lowerErrorbars_Veridical[iSubj]+pback_median_Veridical[iSubj]],\n",
    "                      [pback_upperErrorbars_Veridical[iSubj]-pback_median_Veridical[iSubj]]],\n",
    "                 color='k',marker='o')\n",
    "ax1.plot(np.ones((7,1))*2.55,np.arange(60,91,5),'k--')\n",
    "ax1.set_xlabel('$\\mu_{low}$(Hz)',fontsize=35)\n",
    "ax1.set_xticks(ticks=np.log10([100,250,500]))\n",
    "ax1.set_xticklabels(labels=[100,250,500])\n",
    "ax2.plot(np.ones((7,1))*2.85,np.arange(60,91,5),'k--')\n",
    "ax2.set_xlabel('$\\mu_{high}$(Hz)',fontsize=35)\n",
    "ax2.set_xticks(ticks=np.log10([500,750,1000,1500]))\n",
    "ax2.set_xticklabels(labels=[500,750,1000,1500])\n",
    "ax3.plot(np.ones((7,1))*0.1,np.arange(60,91,5),'k--')\n",
    "ax3.set_xlabel('$\\sigma$',fontsize=35)\n",
    "for ax in [ax4, ax5]:\n",
    "    ax.plot(np.ones((7,1))*0.3,np.arange(60,91,5),'k--')\n",
    "    ax.set_xlabel('p$_{distractor}$',fontsize=35)\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5]:\n",
    "    ax.set_ylabel('Accuracy',fontsize=35)\n",
    "ax6.set_xlabel('p$_{distractor}$',fontsize=35)\n",
    "ax6.set_ylabel('Sensory Uncertainty',fontsize=35)\n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6]:\n",
    "    ax.tick_params(axis='both',labelsize=33,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/accuracyGivenMeanLow.pdf', bbox_inches=\"tight\", transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/accuracyGivenMeanHigh.pdf', bbox_inches=\"tight\", transparent=True)\n",
    "#fig3.savefig('figures/FromProlific/illustrations/accuracyGivenSigma.pdf', bbox_inches=\"tight\", transparent=True)\n",
    "#fig4.savefig('figures/FromProlific/illustrations/accuracyGivenMeanPBack.pdf', bbox_inches=\"tight\", transparent=True)\n",
    "#fig5.savefig('figures/FromProlific/illustrations/accuracyGivenMeanPBackVeridical.pdf', bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "print(\"Correlation of variables with accuracy\")\n",
    "print(\"mu low\",pg.corr(meanLowMedian,OverallAccuracy,method='spearman'))\n",
    "print(\"mu high\", pg.corr(meanHighMedian,OverallAccuracy,method='spearman'))\n",
    "print(\"sigma\", pg.corr(sigmaMedian,OverallAccuracy,method='spearman'))\n",
    "print(\"sigma sensory\", pg.corr(ssMedian,OverallAccuracy,method='spearman'))\n",
    "print(\"pdistractor\", pg.corr(pbackMedian,OverallAccuracy,method='spearman'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31791b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 6A-C: short term context in the unbiased data and its relation to other terms\n",
    "\"\"\"\n",
    "weightPreceedingTone_noContext = computedLikelihoodsVeridical['W1*e^(-1/tau)'].values\n",
    "W1_noContext = computedLikelihoodsVeridical['W1'].values\n",
    "WC_noContext = computedLikelihoodsVeridical['WC'].values\n",
    "tau_noContext = computedLikelihoodsVeridical['tau'].values\n",
    "\n",
    "fig7, ax7 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig8, ax8 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig9, ax9 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig10, ax10 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig11, ax11 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig12, ax12 = plt.subplots(1,1,figsize=(8,6)) \n",
    "for i in range(56):\n",
    "    ax7.errorbar(weightPreceedingTone_noContext[i*2], OverallAccuracy[i], \n",
    "                 xerr=[[weightPreceedingTone_noContext[i*2+1]],[weightPreceedingTone_noContext[i*2+1]]],\n",
    "                 color='k',marker='o')\n",
    "    ax8.errorbar(W1_noContext[i*2], OverallAccuracy[i], \n",
    "                 xerr=[[W1_noContext[i*2+1]],[W1_noContext[i*2+1]]],\n",
    "                 color='k',marker='o')\n",
    "    ax9.errorbar(WC_noContext[i*2], OverallAccuracy[i], \n",
    "                 xerr=[[WC_noContext[i*2+1]],[WC_noContext[i*2+1]]],\n",
    "                 color='k',marker='o')\n",
    "    ax10.errorbar(tau_noContext[i*2], OverallAccuracy[i], \n",
    "                 xerr=[[tau_noContext[i*2+1]],[tau_noContext[i*2+1]]],\n",
    "                 color='k',marker='o')\n",
    "    ax11.errorbar(weightPreceedingTone_noContext[i*2], pback_median_Veridical[i], \n",
    "                  xerr=[[weightPreceedingTone_noContext[i*2+1]],[weightPreceedingTone_noContext[i*2+1]]],\n",
    "                  yerr = [[-pback_lowerErrorbars_Veridical[i]+pback_median_Veridical[i]],\n",
    "                          [pback_upperErrorbars_Veridical[i]-pback_median_Veridical[i]]],\n",
    "                  color='k',marker='o')\n",
    "    ax12.errorbar(W1_noContext[i*2], tau_noContext[i*2], \n",
    "                  xerr=[[W1_noContext[i*2+1]],[W1_noContext[i*2+1]]],\n",
    "                  yerr = [[tau_noContext[i*2+1]],[tau_noContext[i*2+1]]],\n",
    "                  color='k',marker='o')\n",
    "ax7.set_xlabel('Weight of \\n preceding trial ($W_{1}e^{-1/\\\\tau}$)',fontsize=34)\n",
    "ax8.set_xlabel('Local effect ($W_{1}$)',fontsize=34)\n",
    "ax9.set_xlabel('Global effect ($W_{constant}$)',fontsize=34)\n",
    "ax10.set_xlabel('Integration time constant ($\\\\tau$)',fontsize=34)\n",
    "ax11.set_xlabel('Weight of preceding \\n trial ($W_{1}e^{-1/\\\\tau}$)',fontsize=34)\n",
    "ax11.set_ylabel('p$_{distractor}$',fontsize=34)\n",
    "ax12.set_xlabel('Short-term \\n learning ($W_{1}$)',fontsize=34)\n",
    "ax12.set_ylabel('Integration over \\n number of trials ($\\\\tau$)',fontsize=34)\n",
    "ax12.set_yticks([0,1,2])\n",
    "ax12.set_yticklabels([0,1,2])\n",
    "for ax in [ax7,ax8,ax9,ax10]:\n",
    "    ax.set_ylabel('Accuracy',fontsize=34)\n",
    "for ax in [ax7,ax8,ax9,ax10,ax11,ax12]:\n",
    "    ax.tick_params(axis='both',labelsize=32,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "    \n",
    "#fig7.savefig('figures/FromProlific/illustrations/effectOfPreviousTrialOnAccuracy.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "#fig8.savefig('figures/FromProlific/illustrations/effectOfW1OnAccuracy.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "#fig9.savefig('figures/FromProlific/illustrations/effectOfWCOnAccuracy.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "#fig10.savefig('figures/FromProlific/illustrations/effectOfTauOnAccuracy.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "#fig11.savefig('figures/FromProlific/illustrations/variationOfPreviousTrialWithPDistractor.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "#fig12.savefig('figures/FromProlific/illustrations/variationOfW1WithTau.pdf',\n",
    "#           bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "print(\"Wilcoxon of WC\", pg.wilcoxon(WC_noContext[::2]-0.5))\n",
    "print(\"Median and IQR tau\", np.median(tau_noContext[::2]), scipy.stats.iqr(tau_noContext[::2]))\n",
    "\n",
    "print(\"Correlation with accuracy\")\n",
    "print(\"Effect of previous trial\",\n",
    "      pg.corr(weightPreceedingTone_noContext[::2], OverallAccuracy,method='spearman'))\n",
    "print(\"Local effect\",\n",
    "      pg.corr(W1_noContext[::2], OverallAccuracy,method='spearman'))\n",
    "print(\"Global effect\",\n",
    "      pg.corr(WC_noContext[::2], OverallAccuracy,method='spearman'))\n",
    "print(\"Temporal effect\",\n",
    "      pg.corr(tau_noContext[::2], OverallAccuracy,method='spearman'))\n",
    "\n",
    "print(\"Correlation with pdistractor\")\n",
    "print(\"Effect of previous trial\",\n",
    "      pg.corr(weightPreceedingTone_noContext[::2], pback_median_Veridical,method='spearman'))\n",
    "print(\"Local effect\",\n",
    "      pg.corr(W1_noContext[::2], pback_median_Veridical,method='spearman'))\n",
    "print(\"Global effect\",\n",
    "      pg.corr(WC_noContext[::2], pback_median_Veridical,method='spearman'))\n",
    "print(\"Temporal effect\",\n",
    "      pg.corr(tau_noContext[::2], pback_median_Veridical,method='spearman'))\n",
    "\n",
    "print(\"Correlation with ss\")\n",
    "print(\"Effect of previous trial\",\n",
    "      pg.corr(weightPreceedingTone_noContext[::2], ss_median_Veridical,method='spearman'))\n",
    "print(\"Local effect\",\n",
    "      pg.corr(W1_noContext[::2], ss_median_Veridical,method='spearman'))\n",
    "print(\"Global effect\",\n",
    "      pg.corr(WC_noContext[::2], ss_median_Veridical,method='spearman'))\n",
    "print(\"Temporal effect\",\n",
    "      pg.corr(tau_noContext[::2], ss_median_Veridical,method='spearman'))\n",
    "print(\"Pdistractor\",\n",
    "      pg.corr(pback_median_Veridical[ss_median_Veridical<np.percentile(ss_median_Veridical,90)],\n",
    "              ss_median_Veridical[ss_median_Veridical<np.percentile(ss_median_Veridical,90)],method='spearman'))\n",
    "\n",
    "independentVariables = np.concatenate((np.expand_dims(ss_median_Veridical,axis=1),\n",
    "                                      np.expand_dims(tau_noContext[::2],axis=1)),axis=1)\n",
    "\n",
    "pg.linear_regression(X=independentVariables,y=OverallAccuracy/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-radical",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Old figure, no longer in the paper. The concept is however relevant to inter-participant variability in the biased sessions. \n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Qs: are subjects equally biased in both the long context cases? \n",
    "This is the internalized bias which is obtained from the matched raw data for subjects that do both the long \n",
    "context expts.\n",
    "\"\"\"\n",
    "internalizedBias = pd.read_excel(xls,'internalizedBias',nrows=53)\n",
    "\n",
    "biasLowForSubjectsWithBothLongContexts = internalizedBias['biasLowContext'].values\n",
    "biasLowForSubjectsWithBothLongContexts = biasLowForSubjectsWithBothLongContexts[~np.isnan(internalizedBias['biasHighContext'])]\n",
    "biasHighForSubjectsWithBothLongContexts = internalizedBias['biasHighContext'].values\n",
    "biasHighForSubjectsWithBothLongContexts = biasHighForSubjectsWithBothLongContexts[~np.isnan(biasHighForSubjectsWithBothLongContexts)]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot((0.5-biasLowForSubjectsWithBothLongContexts)*2,(biasHighForSubjectsWithBothLongContexts-0.5)*2,'k.')\n",
    "ax.plot(np.arange(-0.5,0.5,0.1),np.arange(-0.5,0.5,0.1),'k--')\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Internalized Bias in Low Context',fontsize=26)\n",
    "plt.ylabel('Internalized Bias in High Context',fontsize=26)\n",
    "#plt.savefig('figures/FromProlific/illustrations/biasLowContextVsbiasHighContext',bbox_inches=\"tight\")\n",
    "\n",
    "\"\"\"\n",
    "Qs: are subjects also biased in the no context case? This is benchmarking internalized bias against \n",
    "the no context.  \n",
    "\"\"\"\n",
    "\n",
    "biasLowForSubjectsWithNoAndLowContexts = internalizedBias['biasLowContext'].values\n",
    "biasNoForSubjectsWithNoAndLowContexts = internalizedBias['biasNoContext'].values\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,(biasNoForSubjectsWithNoAndLowContexts-0.5)*2,'o',color='orange')\n",
    "ax.hlines(0,xmin=-0.5,xmax=0.5,color='k',linestyles='--',linewidth=2)\n",
    "ax.vlines(0,ymin=-0.5,ymax=0.5,color='k',linestyles='--',linewidth=2)\n",
    "\n",
    "biasHighForSubjectsWithNoAndHighContexts = internalizedBias['biasHighContext'].values\n",
    "biasHighForSubjectsWithNoAndHighContexts = biasHighForSubjectsWithNoAndHighContexts[~np.isnan(biasHighForSubjectsWithNoAndHighContexts)]\n",
    "biasNoForSubjectsWithNoAndHighContexts = internalizedBias['biasNoContext'].values\n",
    "biasNoForSubjectsWithNoAndHighContexts = biasNoForSubjectsWithNoAndHighContexts[~np.isnan(internalizedBias['biasHighContext'])]\n",
    "\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,(biasNoForSubjectsWithNoAndHighContexts-0.5)*2,'o',color='brown')\n",
    "ax.hlines(0,xmin=-0.4,xmax=0.8,color='k',linestyles='--')\n",
    "ax.vlines(0,ymin=-0.4,ymax=0.4,color='k',linestyles='--')\n",
    "makeAxesPretty(ax)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xticks(fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "plt.xlabel('Internalized Bias in the $\\it{biased}$ sessions',fontsize=27)\n",
    "plt.ylabel('Internalized Bias in \\n the $\\it{unbiased}$ session',fontsize=27)\n",
    "#plt.savefig('figures/FromProlific/illustrations/biasNoContextVsbiasLongContext.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "\"\"\"\n",
    "Qs: how does internalized bias compare to performance accuracy?\n",
    "\"\"\"\n",
    "majorityVsMinorityCategoryAccuracyHighContext = internalizedBias['DifferenceInAccuracyOfCategoriesInHighContext'].values\n",
    "majorityVsMinorityCategoryAccuracyHighContext = majorityVsMinorityCategoryAccuracyHighContext[~np.isnan(majorityVsMinorityCategoryAccuracyHighContext)]\n",
    "majorityVsMinorityCategoryAccuracyLowContext = internalizedBias['DifferenceInAccuracyOfCategoriesInLowContext'].values\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,majorityVsMinorityCategoryAccuracyLowContext*100,\n",
    "        'o',color='orange')\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,majorityVsMinorityCategoryAccuracyHighContext*100,\n",
    "        'o',color='brown')\n",
    "makeAxesPretty(ax)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=27)\n",
    "plt.ylabel('Accuracy of overrepresented \\n category - accuracy of \\n underrepresented category',fontsize=27)\n",
    "#plt.savefig('figures/FromProlific/illustrations/accuracyInMajorityMinorityCategoriesExplainsInternalisedBias.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "AccuracyHighContext = np.ma.array(internalizedBias['accuracyHighContext'].values,mask=False)\n",
    "AccuracyHighContext = AccuracyHighContext[~np.isnan(AccuracyHighContext)]\n",
    "AccuracyLowContext = np.ma.array(internalizedBias['accuracyLowContext'].values,mask=False)\n",
    "AccuracyNoContext = internalizedBias['accuracyNoContext'].values\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,AccuracyLowContext,\n",
    "        'o',color='orange')\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,AccuracyHighContext,\n",
    "        'o',color='brown')\n",
    "makeAxesPretty(ax)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=27)\n",
    "plt.ylabel('Accuracy',fontsize=27)\n",
    "#plt.savefig('figures/FromProlific/illustrations/totalAccuracyVsInternalisedBias.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "ssMedian = internalizedBias['ss'].values\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,ssMedian,\n",
    "        'o',color='orange')\n",
    "ax.plot((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,ssMedian[:-5],\n",
    "        'o',color='brown')\n",
    "makeAxesPretty(ax)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=25)\n",
    "plt.yticks(fontsize=25)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=27)\n",
    "plt.ylabel('Sensory Uncertainty',fontsize=27)\n",
    "#plt.savefig('figures/FromProlific/illustrations/sensoryUncertaintyVsInternalisedBias.pdf',\n",
    "#            bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "DiffInAccuracyNoDistractorsLowContext = internalizedBias['DifferenceInAccuracyNoDistractorTrialsLowContext'].values\n",
    "DiffInAccuracyOneDistractorLowContext = internalizedBias['DifferenceInAccuracyOneDistractorTrialsLowContext'].values\n",
    "DiffInAccuracyTwoDistractorLowContext = internalizedBias['DifferenceInAccuracyTwoDistractorTrialsLowContext'].values\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,DiffInAccuracyNoDistractorsLowContext,\n",
    "        'o',color='skyblue')\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,DiffInAccuracyOneDistractorLowContext,\n",
    "        'o',color='blue')\n",
    "ax.plot((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,DiffInAccuracyTwoDistractorLowContext,\n",
    "        'o',color='black')\n",
    "makeAxesPretty(ax)\n",
    "ax.set_xticks(np.arange(0,0.8,0.1))\n",
    "ax.set_xticklabels(np.around(np.arange(0,0.8,0.1),1),fontsize=23)\n",
    "plt.yticks(fontsize=23)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "plt.xlabel('Internalized Bias',fontsize=25)\n",
    "plt.ylabel('Accuracy',fontsize=25)\n",
    "\n",
    "print(\"Median of internalized bias in the unbiased session for bias low subjects\", \n",
    "      np.median((0.5-biasNoForSubjectsWithNoAndLowContexts)*2), \n",
    "      scipy.stats.iqr((0.5-biasNoForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Median of internalized bias in the unbiased session for bias high subjects\", \n",
    "      np.median((0.5-biasNoForSubjectsWithNoAndHighContexts)*2), \n",
    "      scipy.stats.iqr((0.5-biasNoForSubjectsWithNoAndHighContexts)*2))\n",
    "print(\"Median of internalized bias in the biased low session\", \n",
    "      np.median((0.5-biasLowForSubjectsWithNoAndLowContexts)*2),\n",
    "      scipy.stats.iqr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Median of internalized bias in the biased high session\", \n",
    "     np.median((biasHighForSubjectsWithNoAndHighContexts-0.5)*2), \n",
    "      scipy.stats.iqr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2))\n",
    "print(\"Comparing bias in no and low sessions\",pg.wilcoxon((0.5-biasNoForSubjectsWithNoAndLowContexts)*2,\n",
    "                                                          (0.5-biasLowForSubjectsWithNoAndLowContexts)*2))\n",
    "print(\"Comparing bias in no and high sessions\",pg.wilcoxon((0.5-biasNoForSubjectsWithNoAndHighContexts)*2,\n",
    "                                                           (biasHighForSubjectsWithNoAndHighContexts-0.5)*2))\n",
    "print(\"Correlation of bias with difference in accuracy in biased low session\",\n",
    "      pg.corr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,\n",
    "              majorityVsMinorityCategoryAccuracyLowContext,method='spearman'))\n",
    "print(\"Correlation of bias with difference in accuracy in biased high session\",\n",
    "      pg.corr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,\n",
    "              majorityVsMinorityCategoryAccuracyHighContext,method='spearman'))\n",
    "print(\"Correlation of bias and accuracy in biased low session\", pg.corr((0.5-biasLowForSubjectsWithNoAndLowContexts)*2,\n",
    "                                                                        AccuracyLowContext,method='spearman'))\n",
    "print(\"Correlation of bias and accuracy in biased high session\", pg.corr((biasHighForSubjectsWithNoAndHighContexts-0.5)*2,\n",
    "                                                                         AccuracyHighContext,method='spearman'))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "ax.plot(AccuracyNoContext,AccuracyLowContext,'o',color='orange')\n",
    "ax.plot(AccuracyNoContext[:-5],AccuracyHighContext,'o',color='brown')\n",
    "ax.plot([60,90],[60,90],color='k',linestyle='--')\n",
    "makeAxesPretty(ax)\n",
    "plt.xticks(fontsize=23)\n",
    "plt.yticks(fontsize=23)\n",
    "ax.tick_params(axis='both',length=6,width=2)\n",
    "ax.set_xlabel('Accuracy in unbiased session',fontsize=25)\n",
    "ax.set_ylabel('Accuracy in biased sessions',fontsize=25)\n",
    "AccuracyLongContext = (AccuracyHighContext+AccuracyLowContext[:-5])/2\n",
    "print(\"Comparing accuracy in no and long sessions\",pg.ttest(AccuracyNoContext[:-5]-AccuracyLongContext,0))\n",
    "\n",
    "print(sum(AccuracyNoContext>AccuracyLowContext), \n",
    "      sum(AccuracyNoContext[:-5]>AccuracyHighContext))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-arizona",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure 6.\n",
    "Qs: What are the trends in the long context? \n",
    "\"\"\"\n",
    "\n",
    "bic_lowContext_probModel = np.ma.array(computedLikelihoodsLowContextVeridical['medianProb'].values,mask=False)\n",
    "bic_lowContext_probModel = bic_lowContext_probModel[~np.isnan(bic_lowContext_probModel)]\n",
    "\n",
    "bic_lowContext_randomModel = np.ma.array(computedLikelihoodsLowContextVeridical['medianRandom'].values,mask=False)\n",
    "bic_lowContext_randomModel = bic_lowContext_randomModel[~np.isnan(bic_lowContext_randomModel)]\n",
    "\n",
    "bic_lowContext_signalModel = np.ma.array(computedLikelihoodsLowContextVeridical['medianSignal'].values,mask=False)\n",
    "bic_lowContext_signalModel = bic_lowContext_signalModel[~np.isnan(bic_lowContext_signalModel)]\n",
    "\n",
    "bic_lowContext_lowerError_probModel = np.ma.array(computedLikelihoodsLowContextVeridical['5thPercentileProb'].values,mask=False)\n",
    "bic_lowContext_lowerError_probModel = bic_lowContext_lowerError_probModel[~np.isnan(bic_lowContext_lowerError_probModel)]\n",
    "\n",
    "bic_lowContext_upperError_probModel = np.ma.array(computedLikelihoodsLowContextVeridical['95thPercentileProb'].values,mask=False)\n",
    "bic_lowContext_upperError_probModel = bic_lowContext_upperError_probModel[~np.isnan(bic_lowContext_upperError_probModel)]\n",
    "\n",
    "bic_lowContext_lowerError_randomModel = np.ma.array(computedLikelihoodsLowContextVeridical['5thPercentileRandom'].values,mask=False)\n",
    "bic_lowContext_lowerError_randomModel = bic_lowContext_lowerError_randomModel[~np.isnan(bic_lowContext_lowerError_randomModel)]\n",
    "\n",
    "bic_lowContext_upperError_randomModel = np.ma.array(computedLikelihoodsLowContextVeridical['95thPercentileRandom'].values,mask=False)\n",
    "bic_lowContext_upperError_randomModel = bic_lowContext_upperError_randomModel[~np.isnan(bic_lowContext_upperError_randomModel)]\n",
    "\n",
    "bic_lowContext_lowerError_signalModel = np.ma.array(computedLikelihoodsLowContextVeridical['5thPercentileSignal'].values,mask=False)\n",
    "bic_lowContext_lowerError_signalModel = bic_lowContext_lowerError_signalModel[~np.isnan(bic_lowContext_lowerError_signalModel)]\n",
    "\n",
    "bic_lowContext_upperError_signalModel = np.ma.array(computedLikelihoodsLowContextVeridical['95thPercentileSignal'].values,mask=False)\n",
    "bic_lowContext_upperError_signalModel = bic_lowContext_upperError_signalModel[~np.isnan(bic_lowContext_upperError_signalModel)]\n",
    "\n",
    "size_lowContext = np.ma.array(computedLikelihoodsLowContextVeridical['sizeSubsampledDataset'].values,mask=False)\n",
    "size_lowContext = size_lowContext[~np.isnan(size_lowContext)]\n",
    "\n",
    "bic_highContext_probModel = np.ma.array(computedLikelihoodsHighContextVeridical['medianProb'].values,mask=False)\n",
    "bic_highContext_probModel = bic_highContext_probModel[~np.isnan(bic_highContext_probModel)]\n",
    "\n",
    "bic_highContext_randomModel = np.ma.array(computedLikelihoodsHighContextVeridical['medianRandom'].values,mask=False)\n",
    "bic_highContext_randomModel = bic_highContext_randomModel[~np.isnan(bic_highContext_randomModel)]\n",
    "\n",
    "bic_highContext_signalModel = np.ma.array(computedLikelihoodsHighContextVeridical['medianSignal'].values,mask=False)\n",
    "bic_highContext_signalModel = bic_highContext_signalModel[~np.isnan(bic_highContext_signalModel)]\n",
    "\n",
    "bic_highContext_lowerError_probModel = np.ma.array(computedLikelihoodsHighContextVeridical['5thPercentileProb'].values,mask=False)\n",
    "bic_highContext_lowerError_probModel = bic_highContext_lowerError_probModel[~np.isnan(bic_highContext_lowerError_probModel)]\n",
    "\n",
    "bic_highContext_upperError_probModel = np.ma.array(computedLikelihoodsHighContextVeridical['95thPercentileProb'].values,mask=False)\n",
    "bic_highContext_upperError_probModel = bic_highContext_upperError_probModel[~np.isnan(bic_highContext_upperError_probModel)]\n",
    "\n",
    "bic_highContext_lowerError_randomModel = np.ma.array(computedLikelihoodsHighContextVeridical['5thPercentileRandom'].values,mask=False)\n",
    "bic_highContext_lowerError_randomModel = bic_highContext_lowerError_randomModel[~np.isnan(bic_highContext_lowerError_randomModel)]\n",
    "\n",
    "bic_highContext_upperError_randomModel = np.ma.array(computedLikelihoodsHighContextVeridical['95thPercentileRandom'].values,mask=False)\n",
    "bic_highContext_upperError_randomModel = bic_highContext_upperError_randomModel[~np.isnan(bic_highContext_upperError_randomModel)]\n",
    "\n",
    "bic_highContext_lowerError_signalModel = np.ma.array(computedLikelihoodsHighContextVeridical['5thPercentileSignal'].values,mask=False)\n",
    "bic_highContext_lowerError_signalModel = bic_highContext_lowerError_signalModel[~np.isnan(bic_highContext_lowerError_signalModel)]\n",
    "\n",
    "bic_highContext_upperError_signalModel = np.ma.array(computedLikelihoodsHighContextVeridical['95thPercentileSignal'].values,mask=False)\n",
    "bic_highContext_upperError_signalModel = bic_highContext_upperError_signalModel[~np.isnan(bic_highContext_upperError_signalModel)]\n",
    "\n",
    "size_highContext = np.ma.array(computedLikelihoodsHighContextVeridical['sizeSubsampledDataset'].values,mask=False)\n",
    "size_highContext = size_highContext[~np.isnan(size_highContext)]\n",
    "\n",
    "pcategory_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['medianPLow'].values,mask=False)\n",
    "pcategory_lowContext = pcategory_lowContextWithNan[~np.isnan(pcategory_lowContextWithNan)]\n",
    "\n",
    "pcategory_lowContextWithNan_lowerError = np.ma.array(computedLikelihoodsLowContextVeridical['5thPercentilePLow'].values,mask=False)\n",
    "pcategory_lowContext_lowerError = pcategory_lowContextWithNan_lowerError[~np.isnan(pcategory_lowContextWithNan_lowerError)]\n",
    "\n",
    "pcategory_lowContextWithNan_upperError = np.ma.array(computedLikelihoodsLowContextVeridical['95thPercentilePLow'].values,mask=False)\n",
    "pcategory_lowContext_upperError = pcategory_lowContextWithNan_upperError[~np.isnan(pcategory_lowContextWithNan_upperError)]\n",
    "\n",
    "pback_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['medianPBack'].values,mask=False)\n",
    "pback_lowContext = pback_lowContextWithNan[~np.isnan(pback_lowContextWithNan)]\n",
    "\n",
    "pback_lowContextWithNan_lowerError = np.ma.array(computedLikelihoodsLowContextVeridical['5thPercentilePBack'].values,mask=False)\n",
    "pback_lowContext_lowerError = pback_lowContextWithNan_lowerError[~np.isnan(pback_lowContextWithNan_lowerError)]\n",
    "\n",
    "pback_lowContextWithNan_upperError = np.ma.array(computedLikelihoodsLowContextVeridical['95thPercentilePBack'].values,mask=False)\n",
    "pback_lowContext_upperError = pback_lowContextWithNan_upperError[~np.isnan(pback_lowContextWithNan_upperError)]\n",
    "\n",
    "WC_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['WC'].values,mask=False)\n",
    "WC_lowContext = WC_lowContextWithNan[~np.isnan(WC_lowContextWithNan)]\n",
    "\n",
    "W1_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['W1'].values,mask=False)\n",
    "W1_lowContext = W1_lowContextWithNan[~np.isnan(W1_lowContextWithNan)]\n",
    "\n",
    "tau_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['tau'].values,mask=False)\n",
    "tau_lowContext = tau_lowContextWithNan[~np.isnan(tau_lowContextWithNan)]\n",
    "\n",
    "weightPreceedingTone_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['W1*e-(1/tau)'].values,mask=False)\n",
    "weightPreceedingTone_lowContext = weightPreceedingTone_lowContextWithNan[~np.isnan(weightPreceedingTone_lowContextWithNan)]\n",
    "\n",
    "pcategory_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['medianPLow'].values,mask=False)\n",
    "pcategory_highContext = pcategory_highContextWithNan[~np.isnan(pcategory_highContextWithNan)]\n",
    "\n",
    "pcategory_highContextWithNan_lowerError = np.ma.array(computedLikelihoodsHighContextVeridical['5thPercentilePLow'].values,mask=False)\n",
    "pcategory_highContext_lowerError = pcategory_highContextWithNan_lowerError[~np.isnan(pcategory_highContextWithNan_lowerError)]\n",
    "\n",
    "pcategory_highContextWithNan_upperError = np.ma.array(computedLikelihoodsHighContextVeridical['95thPercentilePLow'].values,mask=False)\n",
    "pcategory_highContext_upperError = pcategory_highContextWithNan_upperError[~np.isnan(pcategory_highContextWithNan_upperError)]\n",
    "\n",
    "pback_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['medianPBack'].values,mask=False)\n",
    "pback_highContext = pback_highContextWithNan[~np.isnan(pback_highContextWithNan)]\n",
    "\n",
    "pback_highContextWithNan_lowerError = np.ma.array(computedLikelihoodsHighContextVeridical['5thPercentilePBack'].values,mask=False)\n",
    "pback_highContext_lowerError = pback_highContextWithNan_lowerError[~np.isnan(pback_highContextWithNan_lowerError)]\n",
    "\n",
    "pback_highContextWithNan_upperError = np.ma.array(computedLikelihoodsHighContextVeridical['95thPercentilePBack'].values,mask=False)\n",
    "pback_highContext_upperError = pback_highContextWithNan_upperError[~np.isnan(pback_highContextWithNan_upperError)]\n",
    "\n",
    "WC_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['WC'].values,mask=False)\n",
    "WC_highContext = WC_highContextWithNan[~np.isnan(WC_highContextWithNan)]\n",
    "\n",
    "W1_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['W1'].values,mask=False)\n",
    "W1_highContext = W1_highContextWithNan[~np.isnan(W1_highContextWithNan)]\n",
    "\n",
    "tau_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['tau'].values,mask=False)\n",
    "tau_highContext = tau_highContextWithNan[~np.isnan(tau_highContextWithNan)]\n",
    "\n",
    "weightPreceedingTone_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['W1*e-(1/tau)'].values,mask=False)\n",
    "weightPreceedingTone_highContext = weightPreceedingTone_highContextWithNan[~np.isnan(weightPreceedingTone_highContextWithNan)]\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig4, ax4 = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.5)\n",
    "for i in np.arange(53):     \n",
    "    ax1.errorbar(2*bic_lowContext_randomModel[i]+2*np.log(size_lowContext[i]),\n",
    "                 2*bic_lowContext_probModel[i]+3*np.log(size_lowContext[i]), \n",
    "                 xerr=[[-2*bic_lowContext_lowerError_randomModel[i]+2*bic_lowContext_randomModel[i]],\n",
    "                      [2*bic_lowContext_upperError_randomModel[i]-2*bic_lowContext_randomModel[i]]],\n",
    "                 yerr=[[-2*bic_lowContext_lowerError_probModel[i]+2*bic_lowContext_probModel[i]],\n",
    "                      [2*bic_lowContext_upperError_probModel[i]-2*bic_lowContext_probModel[i]]],\n",
    "                 color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    ax2.errorbar(2*bic_highContext_randomModel[i]+2*np.log(size_highContext[i]),\n",
    "                 2*bic_highContext_probModel[i]+3*np.log(size_highContext[i]), \n",
    "                 xerr=[[-2*bic_highContext_lowerError_randomModel[i]+2*bic_highContext_randomModel[i]],\n",
    "                      [2*bic_highContext_upperError_randomModel[i]-2*bic_highContext_randomModel[i]]],\n",
    "                 yerr=[[-2*bic_highContext_lowerError_probModel[i]+2*bic_highContext_probModel[i]],\n",
    "                      [2*bic_highContext_upperError_probModel[i]-2*bic_highContext_probModel[i]]],\n",
    "                 color='brown',marker='o')\n",
    "    \n",
    "ax1.plot(np.arange(200,800),np.arange(200,800),'k--',linewidth=2)  \n",
    "ax1.set_xticks([200,400,600,800])\n",
    "ax1.set_xticklabels([200,400,600,800])\n",
    "ax2.plot(np.arange(200,700),np.arange(200,700),'k--',linewidth=2) \n",
    "ax2.set_xticks([200,400,600])\n",
    "ax2.set_xticklabels([200,400,600])\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "    ax.tick_params(axis='both',labelsize=30,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "for ax in [ax1,ax2]:\n",
    "    ax.set_xlabel('BIC of random-guess model', fontsize=32)\n",
    "    ax.set_ylabel('BIC of full Bayesian model',fontsize=32)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategyRandomvsProb_lowContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategyRandomvsProb_highContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax3.errorbar(2*bic_lowContext_signalModel[i]+2*np.log(size_lowContext[i]),\n",
    "                     2*bic_lowContext_probModel[i]+3*np.log(size_lowContext[i]), \n",
    "                     xerr=[[-2*bic_lowContext_lowerError_signalModel[i]+2*bic_lowContext_signalModel[i]],\n",
    "                          [2*bic_lowContext_upperError_signalModel[i]-2*bic_lowContext_signalModel[i]]],\n",
    "                     yerr=[[-2*bic_lowContext_lowerError_probModel[i]+2*bic_lowContext_probModel[i]],\n",
    "                          [2*bic_lowContext_upperError_probModel[i]-2*bic_lowContext_probModel[i]]],\n",
    "                     color='orange',marker='o')\n",
    "\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax4.errorbar(2*bic_highContext_signalModel[i]+2*np.log(size_highContext[i]),\n",
    "                     2*bic_highContext_probModel[i]+3*np.log(size_highContext[i]), \n",
    "                     xerr=[[-2*bic_highContext_lowerError_signalModel[i]+2*bic_highContext_signalModel[i]],\n",
    "                          [2*bic_highContext_upperError_signalModel[i]-2*bic_highContext_signalModel[i]]],\n",
    "                     yerr=[[-2*bic_highContext_lowerError_probModel[i]+2*bic_highContext_probModel[i]],\n",
    "                          [2*bic_highContext_upperError_probModel[i]-2*bic_highContext_probModel[i]]],\n",
    "                     color='brown',marker='o')\n",
    "        \n",
    "ax3.plot(np.arange(200,1400),np.arange(200,1400),'k--',linewidth=2) \n",
    "ax3.set_xticks([300,600,900,1200])\n",
    "ax3.set_xticklabels([300,600,900,1200])\n",
    "ax3.set_yticks([300,600,900,1200])\n",
    "ax3.set_yticklabels([300,600,900,1200])\n",
    "ax4.plot(np.arange(200,1201),np.arange(200,1201),'k--',linewidth=2)\n",
    "ax4.set_xticks([300,600,900,1200])\n",
    "ax4.set_xticklabels([300,600,900,1200])\n",
    "ax4.set_yticks([300,600,900,1200])\n",
    "ax4.set_yticklabels([300,600,900,1200])\n",
    "for ax in [ax3,ax4]:\n",
    "    ax.set_xlabel('BIC of no-distractor model', fontsize=32)\n",
    "    ax.set_ylabel('BIC of full Bayesian model',fontsize=32)\n",
    "#fig3.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategySignalvsProb_lowContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig4.savefig('figures/FromProlific/illustrations/comparingBICAcrossStrategySignalvsProb_highContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "ss_lowContextWithNan = np.ma.array(computedLikelihoodsLowContextVeridical['ss'].values,mask=False)\n",
    "ss_lowContext = ss_lowContextWithNan[~np.isnan(ss_lowContextWithNan)]\n",
    "ss_highContextWithNan = np.ma.array(computedLikelihoodsHighContextVeridical['ss'].values,mask=False)\n",
    "ss_highContext = ss_highContextWithNan[~np.isnan(ss_highContextWithNan)]\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig4, ax4 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    ax2.plot(ss_lowContext[i], AccuracyLowContext[i], color='orange',marker='o')\n",
    "    if i not in [20,40]:\n",
    "        ax1.errorbar(pback_lowContext[i], AccuracyLowContext[i],\n",
    "                     xerr=[[pback_lowContext[i]-pback_lowContext_lowerError[i]],\n",
    "                           [pback_lowContext_upperError[i]-pback_lowContext[i]]],\n",
    "                     color='orange',marker='o')\n",
    "        ax3.errorbar(2*(pcategory_lowContext[i]-0.5), AccuracyLowContext[i],\n",
    "                     xerr=[[2*pcategory_lowContext[i]-2*pcategory_lowContext_lowerError[i]],\n",
    "                           [2*pcategory_lowContext_upperError[i]-2*pcategory_lowContext[i]]],\n",
    "                     color='orange',marker='o')\n",
    "        ax4.errorbar(2*(pcategory_lowContext[i]-0.5),pback_lowContext[i],\n",
    "                     xerr=[[2*pcategory_lowContext[i]-2*pcategory_lowContext_lowerError[i]],\n",
    "                           [2*pcategory_lowContext_upperError[i]-2*pcategory_lowContext[i]]],\n",
    "                     yerr=[[pback_lowContext[i]-pback_lowContext_lowerError[i]],\n",
    "                           [pback_lowContext_upperError[i]-pback_lowContext[i]]],\n",
    "                     color='orange',marker='o')\n",
    "ax1.set_xlabel('$p_{distractor}$',fontsize=34)\n",
    "ax2.set_xlabel('Sensory Uncertainty ($\\\\sigma_{sensory}$)',fontsize=34)\n",
    "ax3.set_xlabel('$2*(p_{category}-0.5)$',fontsize=34)\n",
    "ax4.set_xlabel('$2*(p_{category}-0.5)$',fontsize=34)\n",
    "ax4.set_ylabel('$p_{distractor}$',fontsize=34)\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "    ax.tick_params(axis='both',labelsize=32,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set_ylabel('Accuracy',fontsize=34)\n",
    "\n",
    "for i in range(48):  \n",
    "    ax2.plot(ss_highContext[i], AccuracyHighContext[i], color='brown',marker='o')\n",
    "    if i not in [20,40]:\n",
    "        ax1.errorbar(pback_highContext[i], AccuracyHighContext[i],\n",
    "                     xerr=[[pback_highContext[i]-pback_highContext_lowerError[i]],\n",
    "                           [pback_highContext_upperError[i]-pback_highContext[i]]],\n",
    "                     color='brown',marker='o')        \n",
    "        ax3.errorbar(2*(0.5-pcategory_highContext[i]), AccuracyHighContext[i],\n",
    "                     xerr=[[2*pcategory_highContext[i]-2*pcategory_highContext_lowerError[i]],\n",
    "                           [2*pcategory_highContext_upperError[i]-2*pcategory_highContext[i]]],\n",
    "                     color='brown',marker='o')\n",
    "        ax4.errorbar(2*(0.5-pcategory_highContext[i]),pback_highContext[i],\n",
    "                     xerr=[[2*pcategory_highContext[i]-2*pcategory_highContext_lowerError[i]],\n",
    "                           [2*pcategory_highContext_upperError[i]-2*pcategory_highContext[i]]],\n",
    "                     yerr=[[pback_highContext[i]-pback_highContext_lowerError[i]],\n",
    "                           [pback_highContext_upperError[i]-pback_highContext[i]]],\n",
    "                     color='brown',marker='o')\n",
    "ax1.set_xlabel('$p_{distractor}$',fontsize=34)\n",
    "ax2.set_xlabel('Sensory Uncertainty ($\\\\sigma_{sensory}$)',fontsize=34)\n",
    "ax3.set_xlabel('$2*(p_{category}-0.5)$',fontsize=34)\n",
    "ax4.set_xlabel('$2*(p_{category}-0.5)$',fontsize=34)\n",
    "ax4.set_ylabel('$p_{distractor}$',fontsize=34)\n",
    "ax4.set_yticks([0,0.5,1])\n",
    "for ax in [ax1,ax2,ax3,ax4]:\n",
    "    ax.tick_params(axis='both',labelsize=32,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "for ax in [ax1,ax2,ax3]:\n",
    "    ax.set_ylabel('Accuracy',fontsize=34)\n",
    "#fig1.savefig('figures/FromProlific/illustrations/effectOfPdistractorOnAccuracy_LongContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/effectOfSSOnAccuracy_LongContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax1.errorbar(WC_lowContext[i*2]-0.5, AccuracyLowContext[i],\n",
    "                     xerr=[[WC_lowContext[i*2+1]],[WC_lowContext[i*2+1]]],\n",
    "                     color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax1.errorbar(0.5-WC_highContext[i*2], AccuracyHighContext[i],\n",
    "                     xerr=[[WC_highContext[i*2+1]],[WC_highContext[i*2+1]]],\n",
    "                     color='brown',marker='o')\n",
    "        \n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax2.errorbar(weightPreceedingTone_lowContext[i*2], AccuracyLowContext[i],\n",
    "                     xerr=[[weightPreceedingTone_lowContext[i*2+1]],[weightPreceedingTone_lowContext[i*2+1]]],\n",
    "                     color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax2.errorbar(weightPreceedingTone_highContext[i*2], AccuracyHighContext[i],\n",
    "                     xerr=[[weightPreceedingTone_highContext[i*2+1]],[weightPreceedingTone_highContext[i*2+1]]],\n",
    "                     color='brown',marker='o')  \n",
    "        \n",
    "fig3, ax3 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax3.errorbar(tau_lowContext[i*2], AccuracyLowContext[i],\n",
    "                     xerr=[[tau_lowContext[i*2+1]],[tau_lowContext[i*2+1]]],\n",
    "                     color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax3.errorbar(tau_highContext[i*2], AccuracyHighContext[i],\n",
    "                     xerr=[[tau_highContext[i*2+1]],[tau_highContext[i*2+1]]],\n",
    "                     color='brown',marker='o')  \n",
    "        \n",
    "fig4, ax4 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax4.errorbar(W1_lowContext[i*2], AccuracyLowContext[i],\n",
    "                 xerr=[[W1_lowContext[i*2+1]],[W1_lowContext[i*2+1]]],\n",
    "                 color='orange',marker='o')  \n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax4.errorbar(W1_highContext[i*2], AccuracyHighContext[i],\n",
    "                 xerr=[[W1_highContext[i*2+1]],[W1_highContext[i*2+1]]],\n",
    "                 color='orange',marker='o') \n",
    "        \n",
    "fig5, ax5 = plt.subplots(1,1,figsize=(8,6)) \n",
    "# including all participants for this plot, since everyone can have a varying prior\n",
    "for i in range(53):  \n",
    "    ax5.errorbar(weightPreceedingTone_lowContext[i*2], WC_lowContext[i*2]-0.5,\n",
    "                 xerr=[[weightPreceedingTone_lowContext[i*2+1]],[weightPreceedingTone_lowContext[i*2+1]]],\n",
    "                 yerr=[[WC_lowContext[i*2+1]],[WC_lowContext[i*2+1]]],   \n",
    "                 color='orange',marker='o')  \n",
    "for i in range(48): \n",
    "    ax5.errorbar(weightPreceedingTone_highContext[i*2], 0.5-WC_highContext[i*2],\n",
    "                 xerr=[[weightPreceedingTone_highContext[i*2+1]],[weightPreceedingTone_highContext[i*2+1]]],\n",
    "                 yerr=[[WC_highContext[i*2+1]],[WC_highContext[i*2+1]]],  \n",
    "                 color='brown',marker='o')\n",
    "        \n",
    "fig6, ax6 = plt.subplots(1,1,figsize=(8,6))        \n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax6.errorbar(weightPreceedingTone_lowContext[i*2], pback_lowContext[i],\n",
    "                     xerr=[[weightPreceedingTone_lowContext[i*2+1]],[weightPreceedingTone_lowContext[i*2+1]]],\n",
    "                     yerr=[[pback_lowContext[i]-pback_lowContext_lowerError[i]],\n",
    "                           [pback_lowContext_upperError[i]-pback_lowContext[i]]],  \n",
    "                     color='orange',marker='o')  \n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax6.errorbar(weightPreceedingTone_highContext[i*2], pback_highContext[i],\n",
    "                     xerr=[[weightPreceedingTone_highContext[i*2+1]],[weightPreceedingTone_highContext[i*2+1]]],\n",
    "                     yerr=[[pback_highContext[i]-pback_highContext_lowerError[i]],\n",
    "                           [pback_highContext_upperError[i]-pback_highContext[i]]],  \n",
    "                     color='brown',marker='o')\n",
    "        \n",
    "fig7, ax7 = plt.subplots(1,1,figsize=(8,6))        \n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax7.errorbar(WC_lowContext[i*2]-0.5, pback_lowContext[i],\n",
    "                     xerr=[[WC_lowContext[i*2+1]],[WC_lowContext[i*2+1]]],\n",
    "                     yerr=[[pback_lowContext[i]-pback_lowContext_lowerError[i]],\n",
    "                           [pback_lowContext_upperError[i]-pback_lowContext[i]]],  \n",
    "                     color='orange',marker='o')  \n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax7.errorbar(0.5-WC_highContext[i*2], pback_highContext[i],\n",
    "                     xerr=[[WC_highContext[i*2+1]],[WC_highContext[i*2+1]]],\n",
    "                     yerr=[[pback_highContext[i]-pback_highContext_lowerError[i]],\n",
    "                           [pback_highContext_upperError[i]-pback_highContext[i]]],  \n",
    "                     color='brown',marker='o')\n",
    "\n",
    "fig8, ax8 = plt.subplots(1,1,figsize=(8,6))\n",
    "for i in range(53):  \n",
    "    if i not in [20,40]:\n",
    "        ax8.errorbar(tau_lowContext[i*2], W1_lowContext[i],\n",
    "                     xerr=[[tau_lowContext[i*2+1]],[tau_lowContext[i*2+1]]],\n",
    "                     yerr=[[W1_lowContext[i*2+1]],[W1_lowContext[i*2+1]]],\n",
    "                     color='orange',marker='o')\n",
    "for i in range(48): \n",
    "    if i not in [20,40]:\n",
    "        ax8.errorbar(tau_highContext[i*2], W1_highContext[i],\n",
    "                     xerr=[[tau_highContext[i*2+1]],[tau_highContext[i*2+1]]],\n",
    "                     yerr=[[W1_highContext[i*2+1]],[W1_highContext[i*2+1]]],\n",
    "                     color='brown',marker='o')  \n",
    "        \n",
    "for ax in [ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8]:\n",
    "    ax.tick_params(axis='both',labelsize=32,length=6,width=2)\n",
    "    makeAxesPretty(ax)\n",
    "for ax in [ax1,ax2,ax3,ax4]:    \n",
    "    ax.set_ylabel('Accuracy',fontsize=34)\n",
    "ax1.set_xlabel('Long-term \\n learning ($W_{Constant}$)',fontsize=34)\n",
    "ax2.set_xlabel('Weight of preceding \\n trial $(W_{1}e^{-1/\\\\tau})$',fontsize=34)\n",
    "ax3.set_xlabel('Integration time constant ($\\\\tau$)',fontsize=34)\n",
    "ax4.set_xlabel('Short-term \\n learning ($W_{1}$)',fontsize=34)\n",
    "ax5.set_xlabel('Weight of preceding \\n trial $(W_{1}e^{-1/\\\\tau})$',fontsize=34)\n",
    "ax5.set_ylabel('Long-term \\n learning ($W_{Constant}$)',fontsize=34)\n",
    "ax6.set_xlabel('Weight of preceding \\n trial $(W_{1}e^{-1/\\\\tau})$',fontsize=34)\n",
    "ax6.set_ylabel('$p_{distractor}$',fontsize=34)\n",
    "ax7.set_xlabel('Long-term \\n learning ($W_{Constant}$)',fontsize=34)\n",
    "ax7.set_ylabel('$p_{distractor}$',fontsize=34)\n",
    "ax8.set_xlabel('Integration time constant ($\\\\tau$)',fontsize=34)\n",
    "ax8.set_ylabel('Short-term \\n learning ($W_{1}$)',fontsize=34)\n",
    "#fig2.savefig('figures/FromProlific/illustrations/effectOfPreviousTrialOnAccuracy_longContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig3.savefig('figures/FromProlific/illustrations/effectOfTauOnAccuracy_longContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig5.savefig('figures/FromProlific/illustrations/variationOfWConstantAndEffectOfPreviousTrial_LongContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig6.savefig('figures/FromProlific/illustrations/variationOfPdistractorWithEffectOfPreviousTrial_LongContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "#fig7.savefig('figures/FromProlific/illustrations/variationOfPdistractorWithWConstant_LongContext.pdf',\n",
    "#             bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "    \n",
    "print(\"Median and IQR of prob model BICs low context for all subjects\", \n",
    "      np.median(2*bic_lowContext_probModel+3*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_probModel+3*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of prob model BICs high context for all subjects\", \n",
    "      np.median(2*bic_highContext_probModel+3*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_probModel+3*np.log(size_highContext)))\n",
    "print(\"Median and IQR of random model BICs low context all subjects\", \n",
    "      np.median(2*bic_lowContext_randomModel+2*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_randomModel+2*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of random model BICs high context all subjects\", \n",
    "      np.median(2*bic_highContext_randomModel+2*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_randomModel+2*np.log(size_highContext)))\n",
    "print(\"Comparing random and prob model BICs low context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_lowContext_randomModel+2*np.log(size_lowContext), \n",
    "                  2*bic_lowContext_probModel+3*np.log(size_lowContext)))\n",
    "print(\"Comparing random and prob model BICs high context using wilcoxon\", \n",
    "      pg.wilcoxon(2*bic_highContext_randomModel+2*np.log(size_highContext),\n",
    "                  2*bic_highContext_probModel+3*np.log(size_highContext)))\n",
    "print(\"Correlation between ss and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "\n",
    "bic_lowContext_randomModel.mask[[20,40]] = True\n",
    "bic_lowContext_signalModel.mask[[20,40]] = True\n",
    "bic_lowContext_probModel.mask[[20,40]] = True\n",
    "bic_highContext_randomModel.mask[[20,40]] = True\n",
    "bic_highContext_signalModel.mask[[20,40]] = True\n",
    "bic_highContext_probModel.mask[[20,40]] = True\n",
    "size_lowContext.mask[[20,40]] = True\n",
    "size_highContext.mask[[20,40]] = True\n",
    "\n",
    "pcategory_lowContext.data[[20,40]] = np.nan\n",
    "pback_lowContext.data[[20,40]] = np.nan\n",
    "pcategory_highContext.data[[20,40]] = np.nan\n",
    "pback_highContext.data[[20,40]] = np.nan\n",
    "AccuracyLowContext.data[[20,40]] = np.nan\n",
    "AccuracyHighContext.data[[20,40]] = np.nan\n",
    "ss_lowContext[[20,40]] = np.nan\n",
    "ss_highContext[[20,40]] = np.nan\n",
    "WC_lowContext[[40,41,80,81]] = np.nan\n",
    "W1_lowContext[[40,41,80,81]] = np.nan\n",
    "tau_lowContext[[40,41,80,81]] = np.nan\n",
    "weightPreceedingTone_lowContext[[40,41,80,81]] = np.nan\n",
    "WC_highContext[[40,41,80,81]] = np.nan\n",
    "W1_highContext[[40,41,80,81]] = np.nan\n",
    "tau_highContext[[40,41,80,81]] = np.nan\n",
    "weightPreceedingTone_highContext[[40,41,80,81]] = np.nan\n",
    "\n",
    "print(\"Median and IQR of prob model BICs low context for n-2 subjects\", \n",
    "      np.median(2*bic_lowContext_probModel+3*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_probModel+3*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of prob model BICs high context for n-2 subjects\", \n",
    "      np.median(2*bic_highContext_probModel+3*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_probModel+3*np.log(size_highContext)))\n",
    "print(\"Median and IQR of signal model BICs low context for n-2 subjects\", \n",
    "      np.median(2*bic_lowContext_signalModel+2*np.log(size_lowContext)),\n",
    "      scipy.stats.iqr(2*bic_lowContext_signalModel+2*np.log(size_lowContext)))\n",
    "print(\"Median and IQR of signal model BICs high context for n-2 subjects\", \n",
    "      np.median(2*bic_highContext_signalModel+2*np.log(size_highContext)),\n",
    "      scipy.stats.iqr(2*bic_highContext_signalModel+2*np.log(size_highContext)))\n",
    "print(\"Comparing signal and prob model BICs low context using wilcoxon for n-2 subjects\", \n",
    "      pg.wilcoxon(2*bic_lowContext_signalModel+2*np.log(size_lowContext),\n",
    "                  2*bic_lowContext_probModel+3*np.log(size_lowContext)))\n",
    "print(\"Comparing signal and prob model BICs high context using wilcoxon for n-2 subjects\", \n",
    "      pg.wilcoxon(2*bic_highContext_signalModel+2*np.log(size_highContext),\n",
    "                  2*bic_highContext_probModel+3*np.log(size_highContext)))\n",
    "\n",
    "print(\"Participants with non-zero W1\", sum(np.concatenate((W1_lowContext[~np.isnan(W1_lowContext)][::2],\n",
    "                                                           W1_highContext[~np.isnan(W1_highContext)][::2]))>0))\n",
    "print(\"Median and IQR tau\", np.median(np.concatenate((tau_lowContext[~np.isnan(tau_lowContext)][::2],\n",
    "                                                      tau_highContext[~np.isnan(tau_highContext)][::2]))), \n",
    "      scipy.stats.iqr(np.concatenate((tau_lowContext[~np.isnan(tau_lowContext)][::2],\n",
    "                                      tau_highContext[~np.isnan(tau_highContext)][::2]))))\n",
    "print(\"Mean pdistractor\", np.mean(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                                                 pback_highContext[~np.isnan(pback_highContext)]))))\n",
    "\n",
    "print(\"Correlation between pdistractor and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                              pback_highContext[~np.isnan(pback_highContext)])), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between pcategory and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((2*(pcategory_lowContext[~np.isnan(pcategory_lowContext)]-0.5),\n",
    "                              2*(0.5-pcategory_highContext[~np.isnan(pcategory_highContext)]))), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between WC and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((WC_lowContext[~np.isnan(WC_lowContext)][::2]-0.5,\n",
    "               0.5-WC_highContext[~np.isnan(WC_highContext)][::2])), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between effect of previous trial and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((weightPreceedingTone_lowContext[~np.isnan(weightPreceedingTone_lowContext)][::2],\n",
    "                              weightPreceedingTone_highContext[~np.isnan(weightPreceedingTone_highContext)][::2])), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between Tau and accuracy in biased context\",\n",
    "      pg.corr(np.concatenate((tau_lowContext[~np.isnan(tau_lowContext)][::2],\n",
    "                              tau_highContext[~np.isnan(tau_highContext)][::2])), \n",
    "              np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                              AccuracyHighContext[~np.isnan(AccuracyHighContext)])),\n",
    "              method='spearman'))\n",
    "\n",
    "print(\"Correlation between pcategory and pdistractor in biased context\",\n",
    "      pg.corr(np.concatenate((2*(pcategory_lowContext[~np.isnan(pcategory_lowContext)]-0.5),\n",
    "                              2*(0.5-pcategory_highContext[~np.isnan(pcategory_highContext)]))), \n",
    "              np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                              pback_highContext[~np.isnan(pback_highContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between WC and pdistractor in biased context\",\n",
    "      pg.corr(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                              pback_highContext[~np.isnan(pback_highContext)])), \n",
    "              np.concatenate((WC_lowContext[~np.isnan(WC_lowContext)][::2]-0.5,\n",
    "                              0.5-WC_highContext[~np.isnan(WC_highContext)][::2])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between Weight of previous tone and pdistractor in biased context\",\n",
    "      pg.corr(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                              pback_highContext[~np.isnan(pback_highContext)])), \n",
    "              np.concatenate((weightPreceedingTone_lowContext[~np.isnan(weightPreceedingTone_lowContext)][::2],\n",
    "                              weightPreceedingTone_highContext[~np.isnan(weightPreceedingTone_highContext)][::2])),\n",
    "              method='spearman'))\n",
    "\n",
    "print(\"Correlation between pdistractor and ss in biased context\",\n",
    "      pg.corr(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                              pback_highContext[~np.isnan(pback_highContext)])), \n",
    "              np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between pcategory and ss in biased context\",\n",
    "      pg.corr(np.concatenate((2*(pcategory_lowContext[~np.isnan(pcategory_lowContext)]-0.5),\n",
    "                              2*(0.5-pcategory_highContext[~np.isnan(pcategory_highContext)]))), \n",
    "              np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between WC and ss in biased context\",\n",
    "      pg.corr(np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])), \n",
    "              np.concatenate((WC_lowContext[~np.isnan(WC_lowContext)][::2]-0.5,\n",
    "                              0.5-WC_highContext[~np.isnan(WC_highContext)][::2])),\n",
    "              method='spearman'))\n",
    "print(\"Correlation between Weight of previous tone and ss in biased context\",\n",
    "      pg.corr(np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])), \n",
    "              np.concatenate((weightPreceedingTone_lowContext[~np.isnan(weightPreceedingTone_lowContext)][::2],\n",
    "                              weightPreceedingTone_highContext[~np.isnan(weightPreceedingTone_highContext)][::2])),\n",
    "              method='spearman'))\n",
    "combiningLocalGlobalEffects_lowContext = WC_lowContext[::2]+0.4*weightPreceedingTone_lowContext[::2]\n",
    "combiningLocalGlobalEffects_highContext = WC_highContext[::2]-0.4*weightPreceedingTone_highContext[::2]\n",
    "print(\"Correlation between Global+Local and ss in biased context\",\n",
    "      pg.corr(np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                              ss_highContext[~np.isnan(ss_highContext)])), \n",
    "              np.concatenate((combiningLocalGlobalEffects_lowContext[~np.isnan(combiningLocalGlobalEffects_lowContext)],\n",
    "                              combiningLocalGlobalEffects_highContext[~np.isnan(combiningLocalGlobalEffects_highContext)])),\n",
    "              method='spearman'))\n",
    "\n",
    "print(\"Correlation between WC and Effect of previous trial in biased context\",\n",
    "      pg.corr(np.concatenate((weightPreceedingTone_lowContext[~np.isnan(weightPreceedingTone_lowContext)][::2],\n",
    "                              weightPreceedingTone_highContext[~np.isnan(weightPreceedingTone_highContext)][::2])), \n",
    "              np.concatenate((WC_lowContext[~np.isnan(WC_lowContext)][::2]-0.5,\n",
    "                              0.5-WC_highContext[~np.isnan(WC_highContext)][::2])),\n",
    "              method='spearman'))\n",
    "\n",
    "independentVariables = np.concatenate((np.expand_dims(np.concatenate((ss_lowContext[~np.isnan(ss_lowContext)],\n",
    "                                                                      ss_highContext[~np.isnan(ss_highContext)])),axis=1),\n",
    "                                      np.expand_dims(np.concatenate((weightPreceedingTone_lowContext[~np.isnan(weightPreceedingTone_lowContext)][::2],\n",
    "                                                                     weightPreceedingTone_highContext[~np.isnan(weightPreceedingTone_highContext)][::2])),\n",
    "                                                     axis=1),\n",
    "                                      np.expand_dims(np.concatenate((pback_lowContext[~np.isnan(pback_lowContext)],\n",
    "                                                                     pback_highContext[~np.isnan(pback_highContext)])),axis=1)),\n",
    "                                      axis=1)\n",
    "\n",
    "lm = pg.linear_regression(X=independentVariables,\n",
    "                         y=np.concatenate((AccuracyLowContext[~np.isnan(AccuracyLowContext)],\n",
    "                                           AccuracyHighContext[~np.isnan(AccuracyHighContext)]))/100)\n",
    "\n",
    "fig8, ax8 = plt.subplots(1,1,figsize=(8,6))\n",
    "ax8.errorbar([2,4,6],lm['coef'][1:],yerr=lm['se'][1:],color='k')\n",
    "ax8.tick_params(axis='both',labelsize=43,length=6,width=2)\n",
    "makeAxesPretty(ax8)\n",
    "ax8.set_xlabel('Regressors',fontsize=45)\n",
    "ax8.set_ylabel('Weights',fontsize=45)\n",
    "ax8.set_xticks([2,4,6])\n",
    "ax8.set_xticklabels(['Sensory \\n uncertainty','$W_{1}e^{-1/\\\\tau}$','$p_{distractor}$'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-bargain",
   "metadata": {},
   "source": [
    "\n",
    "## From here on Discarded code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-removal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure: how does the interplay of long-term context and relevance uncertainty indirectly affect accuracy\n",
    "\"\"\"\n",
    "\n",
    "effectLongTermLearningAndRelevanceOnAccuracy = pd.read_excel(xls,'interplayLearningRelevanceUncertaintyBiased')\n",
    "\n",
    "measureOfLongTermContextLC = effectLongTermLearningAndRelevanceOnAccuracy['biasLowContext'].values\n",
    "measureOfLongTermContextLC = measureOfLongTermContextLC[~np.isnan(measureOfLongTermContextLC)]\n",
    "measureOfLongTermContextHC = effectLongTermLearningAndRelevanceOnAccuracy['biasHighContext'].values\n",
    "measureOfLongTermContextHC = measureOfLongTermContextHC[~np.isnan(measureOfLongTermContextHC)]\n",
    "\n",
    "accuracyAllSignalTonesLC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithAllSignalTonesLC'].values\n",
    "accuracyAllSignalTonesLC = accuracyAllSignalTonesLC[~np.isnan(accuracyAllSignalTonesLC)]\n",
    "accuracyAllSignalTonesHC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithAllSignalTonesHC'].values\n",
    "accuracyAllSignalTonesHC = accuracyAllSignalTonesHC[~np.isnan(accuracyAllSignalTonesHC)]\n",
    "accuracyAllSignalTonesUnbiased = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithAllSignalTonesUnbiased'].values\n",
    "accuracyAllSignalTonesUnbiased = accuracyAllSignalTonesUnbiased[~np.isnan(accuracyAllSignalTonesUnbiased)]\n",
    "\n",
    "accuracyTwoSignalTonesLC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithTwoSignalTonesLC'].values\n",
    "accuracyTwoSignalTonesLC = accuracyTwoSignalTonesLC[~np.isnan(accuracyTwoSignalTonesLC)]\n",
    "accuracyTwoSignalTonesHC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithTwoSignalTonesHC'].values\n",
    "accuracyTwoSignalTonesHC = accuracyTwoSignalTonesHC[~np.isnan(accuracyTwoSignalTonesHC)]\n",
    "accuracyTwoSignalTonesUnbiased = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithTwoSignalTonesUnbiased'].values\n",
    "accuracyTwoSignalTonesUnbiased = accuracyTwoSignalTonesUnbiased[~np.isnan(accuracyTwoSignalTonesUnbiased)]\n",
    "\n",
    "accuracyOneSignalToneLC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithOneSignalToneLC'].values\n",
    "accuracyOneSignalToneLC = accuracyOneSignalToneLC[~np.isnan(accuracyOneSignalToneLC)]\n",
    "accuracyOneSignalToneHC = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithOneSignalToneHC'].values\n",
    "accuracyOneSignalToneHC = accuracyOneSignalToneHC[~np.isnan(accuracyOneSignalToneHC)]\n",
    "accuracyOneSignalToneUnbiased = effectLongTermLearningAndRelevanceOnAccuracy['accuracyForTrialsWithOneSignalToneUnbiased'].values\n",
    "accuracyOneSignalToneUnbiased = accuracyOneSignalToneUnbiased[~np.isnan(accuracyOneSignalToneUnbiased)]\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "ax1.plot(1-measureOfLongTermContextLC, (-accuracyAllSignalTonesLC+accuracyAllSignalTonesUnbiased)*100,\n",
    "         '.',color='lightcoral')\n",
    "ax1.plot(1-measureOfLongTermContextLC, (-accuracyTwoSignalTonesLC+accuracyTwoSignalTonesUnbiased)*100,\n",
    "         '.',color='red')\n",
    "ax1.plot(1-measureOfLongTermContextLC, (-accuracyOneSignalToneLC+accuracyOneSignalToneUnbiased)*100,'k.')\n",
    "ax1.set_xlim([0.45,0.8])\n",
    "ax1.set_xticks(np.arange(0.5,0.9,0.1))\n",
    "ax1.set_xticklabels(np.around(np.arange(0.5,0.9,0.1),1),fontsize=25)\n",
    "ax1.set_yticks(np.arange(-30,41,10))\n",
    "ax1.set_yticklabels(np.around(np.arange(-30,41,10),1),fontsize=25)\n",
    "ax1.set_xlabel('Bias in the low context expt',fontsize=27)\n",
    "ax1.set_ylabel('Difference in accuracies',fontsize=27)\n",
    "\n",
    "ax2.plot(measureOfLongTermContextHC, (-accuracyAllSignalTonesHC+accuracyAllSignalTonesUnbiased)*100,\n",
    "         '.',color='lightcoral')\n",
    "ax2.plot(measureOfLongTermContextHC, (-accuracyTwoSignalTonesHC+accuracyTwoSignalTonesUnbiased)*100,\n",
    "         '.',color='red')\n",
    "ax2.plot(measureOfLongTermContextHC, (-accuracyOneSignalToneHC+accuracyOneSignalToneUnbiased)*100,'k.')\n",
    "ax2.set_xlim([0.45,0.9])\n",
    "ax2.set_xticks(np.arange(0.5,1,0.1))\n",
    "ax2.set_xticklabels(np.around(np.arange(0.5,1,0.1),1),fontsize=25)\n",
    "ax2.set_yticks(np.arange(-15,35,10))\n",
    "ax2.set_yticklabels(np.around(np.arange(-10,31,10),1),fontsize=25)\n",
    "ax2.set_xlabel('Bias in the high context expt',fontsize=27)\n",
    "ax2.set_ylabel('Difference in accuracies',fontsize=27)\n",
    "\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=1,interaction_only=False,include_bias = False)\n",
    "X_forPoly = measureOfLongTermContextHC.reshape(-1, 1).copy() \n",
    "X_withInteraction = poly.fit_transform(X_forPoly)\n",
    "\n",
    "X_withInteraction = sm.add_constant(X_withInteraction)\n",
    "model_withInteraction = sm.OLS(accuracyAllSignalTonesUnbiased-accuracyAllSignalTonesHC, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "model_withInteraction = sm.OLS(accuracyTwoSignalTonesUnbiased-accuracyTwoSignalTonesHC, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "model_withInteraction = sm.OLS(accuracyOneSignalToneUnbiased-accuracyOneSignalToneHC, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-withdrawal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Figure: how does the interplay of short-term context and relevance uncertainty indirectly affect accuracy\n",
    "\"\"\"\n",
    "\n",
    "effectShortTermLearningAndRelevanceOnAccuracy = pd.read_excel(xls,'interplayLearningRelevanceUncertaintyUnbiased')\n",
    "\n",
    "measureOfShortTermContext = effectShortTermLearningAndRelevanceOnAccuracy['measureOfShortTermContext'].values\n",
    "measureOfShortTermContext = measureOfShortTermContext[~np.isnan(measureOfShortTermContext)]\n",
    "\n",
    "accuracySameCategoryAllSignalTones = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromSameCategoryAllSignalTones'].values\n",
    "accuracySameCategoryAllSignalTones = accuracySameCategoryAllSignalTones[~np.isnan(accuracySameCategoryAllSignalTones)]\n",
    "accuracyOppCategoryAllSignalTones = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromOppCategoryAllSignalTones'].values\n",
    "accuracyOppCategoryAllSignalTones = accuracyOppCategoryAllSignalTones[~np.isnan(accuracyOppCategoryAllSignalTones)]\n",
    "accuracySameCategoryTwoSignalTones = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromSameCategoryTwoSignalTones'].values\n",
    "accuracySameCategoryTwoSignalTones = accuracySameCategoryTwoSignalTones[~np.isnan(accuracySameCategoryTwoSignalTones)]\n",
    "accuracyOppCategoryTwoSignalTones = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromOppCategoryTwoSignalTones'].values\n",
    "accuracyOppCategoryTwoSignalTones = accuracyOppCategoryTwoSignalTones[~np.isnan(accuracyOppCategoryTwoSignalTones)]\n",
    "accuracySameCategoryOneSignalTone = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromSameCategoryOneSignalTone'].values\n",
    "accuracySameCategoryOneSignalTone = accuracySameCategoryOneSignalTone[~np.isnan(accuracySameCategoryOneSignalTone)]\n",
    "accuracyOppCategoryOneSignalTone = effectShortTermLearningAndRelevanceOnAccuracy['accuracyForTrialsFromOppCategoryOneSignalTone'].values\n",
    "accuracyOppCategoryOneSignalTone = accuracyOppCategoryOneSignalTone[~np.isnan(accuracyOppCategoryOneSignalTone)]\n",
    "\n",
    "print(pg.corr(y = accuracySameCategoryAllSignalTones-accuracyOppCategoryAllSignalTones,\n",
    "              x = measureOfShortTermContext, method='pearson'))\n",
    "print(pg.corr(y = accuracySameCategoryTwoSignalTones-accuracyOppCategoryTwoSignalTones,\n",
    "              x = measureOfShortTermContext, method='pearson'))\n",
    "print(pg.corr(y = accuracySameCategoryOneSignalTone-accuracyOppCategoryOneSignalTone,\n",
    "              x = measureOfShortTermContext, method='pearson'))\n",
    "\n",
    "\"\"\"\n",
    "Anova analysis for effect of p_distractor and sigma sensory on accuracy and on category choice\n",
    "\"\"\" \n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=1,interaction_only=False,include_bias = False)\n",
    "X_forPoly = measureOfShortTermContext.reshape(-1, 1).copy() \n",
    "X_withInteraction = poly.fit_transform(X_forPoly)\n",
    "\n",
    "X_withInteraction = sm.add_constant(X_withInteraction)\n",
    "model_withInteraction = sm.OLS(accuracySameCategoryAllSignalTones-accuracyOppCategoryAllSignalTones, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "model_withInteraction = sm.OLS(accuracySameCategoryTwoSignalTones-accuracyOppCategoryTwoSignalTones, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "model_withInteraction = sm.OLS(accuracySameCategoryOneSignalTone-accuracyOppCategoryOneSignalTone, \n",
    "                               X_withInteraction)\n",
    "results_withInteraction = model_withInteraction.fit()\n",
    "print(results_withInteraction.summary())\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6)) \n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "ax1.plot(measureOfShortTermContext, (accuracySameCategoryAllSignalTones-accuracyOppCategoryAllSignalTones)*100,\n",
    "         '.',color='lightcoral')\n",
    "ax1.plot(measureOfShortTermContext, (accuracySameCategoryTwoSignalTones-accuracyOppCategoryTwoSignalTones)*100,\n",
    "         '.',color='red')\n",
    "ax1.plot(measureOfShortTermContext, (accuracySameCategoryOneSignalTone-accuracyOppCategoryOneSignalTone)*100,'k.')\n",
    "ax1.set_xlim([0.4,0.8])\n",
    "ax1.set_xticks(np.arange(0.4,0.9,0.1))\n",
    "ax1.set_xticklabels(np.around(np.arange(0.4,0.9,0.1),1),fontsize=25)\n",
    "ax1.set_yticks(np.arange(-10,41,10))\n",
    "ax1.set_yticklabels(np.around(np.arange(-10,41,10),1),fontsize=25)\n",
    "ax1.set_xlabel('Short-term context bias',fontsize=27)\n",
    "ax1.set_ylabel('Difference in accuracies',fontsize=27)\n",
    "\n",
    "ax2.errorbar([1,2,3],[1.54,1.68,2.13],yerr=[0.13,0.15,0.23],linestyle='',color='k',marker='o')\n",
    "ax2.set_xticks([1,2,3])\n",
    "ax2.set_xticklabels([0,1,2],fontsize=25)\n",
    "ax2.set_yticks(np.arange(0,3))\n",
    "ax2.set_yticklabels(np.around(np.arange(0,3),1),fontsize=25)\n",
    "ax2.set_xlabel('Number of distractors in trial',fontsize=27)\n",
    "ax2.set_ylabel('Slope of the bias vs \\n difference in accuracy curve',fontsize=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a829214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Model free analysis of short term effects of bias.\n",
    "\"\"\"\n",
    "\n",
    "fig1, ax1 = plt.subplots(1,1,figsize=(8,6))\n",
    "fig2, ax2 = plt.subplots(1,1,figsize=(8,6))\n",
    "\n",
    "bias = np.append(biasLowForAllSubjectsWithLowContext[:-5], biasHighForAllSubjectsWithHighContext)\n",
    "accuracyLCOverrepOverrep = ((internalizedBias['AccuracyOfSimpleTrialsLowContextLL'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsLowContextLL'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsLowContextLL'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5]))\n",
    "accuracyHCOverrepOverrep = ((internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsHighContextHH'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsHighContextHH'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'][:-5]))\n",
    "accuracyLCOverrepUnderrep = ((internalizedBias['AccuracyOfSimpleTrialsLowContextLH'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsLowContextLH'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLH'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLH'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsLowContextLH'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLH'][:-5]))\n",
    "accuracyHCOverrepUnderrep = ((internalizedBias['AccuracyOfSimpleTrialsHighContextHL'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsHighContextHL'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHL'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHL'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsHighContextHL'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHL'][:-5]))\n",
    "accuracyLCUnderrepOverrep = ((internalizedBias['AccuracyOfSimpleTrialsLowContextHL'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsLowContextHL'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHL'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHL'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsLowContextHL'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHL'][:-5]))\n",
    "accuracyHCUnderrepOverrep = ((internalizedBias['AccuracyOfSimpleTrialsHighContextLH'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsHighContextLH'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLH'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLH'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsHighContextLH'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLH'][:-5]))\n",
    "accuracyLCUnderrepUnderrep = ((internalizedBias['AccuracyOfSimpleTrialsLowContextHH'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsLowContextHH'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsLowContextHH'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5]))\n",
    "accuracyHCUnderrepUnderrep = ((internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][:-5]*\n",
    "                            internalizedBias['NumberOfSimpleTrialsHighContextLL'][:-5] + \n",
    "                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'][:-5]*\n",
    "                            internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'][:-5])/\n",
    "                            (internalizedBias['NumberOfSimpleTrialsHighContextLL'][:-5] + \n",
    "                             internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'][:-5]))\n",
    "accuracyOverrepOverrep = np.append(accuracyLCOverrepOverrep, accuracyHCOverrepOverrep)\n",
    "accuracyOverrepUnderrep = np.append(accuracyLCOverrepUnderrep, accuracyHCOverrepUnderrep)\n",
    "accuracyUnderrepOverrep = np.append(accuracyLCUnderrepOverrep, accuracyHCUnderrepOverrep)\n",
    "accuracyUnderrepUnderrep = np.append(accuracyLCUnderrepUnderrep, accuracyHCUnderrepUnderrep)\n",
    "numberTrialsOverrepOverrep = np.append(internalizedBias['NumberOfSimpleTrialsLowContextLL'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5],\n",
    "                                       internalizedBias['NumberOfSimpleTrialsHighContextHH'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'][:-5])\n",
    "numberTrialsOverrepUnderrep = np.append(internalizedBias['NumberOfSimpleTrialsLowContextLH'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLH'][:-5],\n",
    "                                       internalizedBias['NumberOfSimpleTrialsHighContextHL'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHL'][:-5])\n",
    "numberTrialsUnderrepOverrep = np.append(internalizedBias['NumberOfSimpleTrialsLowContextHL'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHL'][:-5],\n",
    "                                       internalizedBias['NumberOfSimpleTrialsHighContextLH'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLH'][:-5])\n",
    "numberTrialsUnderrepUnderrep = np.append(internalizedBias['NumberOfSimpleTrialsLowContextHH'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5],\n",
    "                                       internalizedBias['NumberOfSimpleTrialsHighContextLL'][:-5] + \n",
    "                                       internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'][:-5])\n",
    "\n",
    "\"\"\"\n",
    "accuracyOverrepOverrep = np.append(internalizedBias['AccuracyOfSimpleTrialsLowContextLL'][:-5],\n",
    "                                   internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][:-5])\n",
    "accuracyOverrepOverrepWithDist = np.append(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLL'][:-5],\n",
    "                                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'][:-5])\n",
    "numberTrialsOverrepOverrepWithDist = np.append(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5],\n",
    "                                               internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'][:-5])\n",
    "accuracyOverrepUnderrep = np.append(internalizedBias['AccuracyOfSimpleTrialsLowContextLH'][:-5],\n",
    "                                    internalizedBias['AccuracyOfSimpleTrialsHighContextHL'][:-5])\n",
    "accuracyOverrepUnderrepWithDist = np.append(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLH'][:-5],\n",
    "                                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHL'][:-5])\n",
    "accuracyUnderrepUnderrep = np.append(internalizedBias['AccuracyOfSimpleTrialsLowContextHH'][:-5],\n",
    "                                   internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][:-5])\n",
    "accuracyUnderrepUnderrepWithDist = np.append(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHH'][:-5],\n",
    "                                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'][:-5])\n",
    "numberTrialsUnderrepUnderrepWithDist = np.append(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5],\n",
    "                                               internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'][:-5])\n",
    "accuracyUnderrepOverrep = np.append(internalizedBias['AccuracyOfSimpleTrialsLowContextHL'][:-5],\n",
    "                                    internalizedBias['AccuracyOfSimpleTrialsHighContextLH'][:-5])\n",
    "accuracyUnderrepOverrepWithDist = np.append(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHL'][:-5],\n",
    "                                            internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLH'][:-5])\n",
    "\"\"\"\n",
    "\n",
    "ax1.plot(np.abs(bias-0.5)*2, accuracyOverrepOverrep*100, \n",
    "         'P',color='pink',markersize=10)\n",
    "ax1.plot(np.abs(bias-0.5)*2, accuracyOverrepUnderrep*100, \n",
    "         'mediumorchid',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "ax2.plot(np.abs(bias-0.5)*2, accuracyUnderrepOverrep*100, \n",
    "         'P',color='pink',markersize=10)\n",
    "ax2.plot(np.abs(bias-0.5)*2, accuracyUnderrepUnderrep*100, \n",
    "         'mediumorchid',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "print(\"Mean and Sem of accuracy when both trials are from the overrepresented category\",\n",
    "     np.mean(accuracyOverrepOverrep*100), np.std(accuracyOverrepOverrep*100)/np.sqrt(len(accuracyOverrepOverrep)))\n",
    "print(\"Mean and Sem of number of trials for OverrepOverrep\",\n",
    "      np.mean(numberTrialsOverrepOverrep), np.std(numberTrialsOverrepOverrep)/np.sqrt(len(numberTrialsOverrepOverrep)))\n",
    "print(\"Mean and Sem of number of trials for OverrepUnderrep\",\n",
    "      np.mean(numberTrialsOverrepUnderrep), np.std(numberTrialsOverrepUnderrep)/np.sqrt(len(numberTrialsOverrepUnderrep)))\n",
    "print(\"Mean and Sem of accuracy when both trials are from the underrepresented category\",\n",
    "     np.mean(accuracyUnderrepUnderrep*100), np.std(accuracyUnderrepUnderrep*100)/np.sqrt(len(accuracyUnderrepUnderrep)))\n",
    "print(\"Mean and Sem of number of trials for the UnderrepUnderrep\",\n",
    "      np.mean(numberTrialsUnderrepUnderrep), np.std(numberTrialsUnderrepUnderrep)/np.sqrt(len(numberTrialsUnderrepUnderrep)))\n",
    "print(\"Mean and Sem of number of trials for the UnderrepOverrep\",\n",
    "      np.mean(numberTrialsUnderrepOverrep), np.std(numberTrialsUnderrepOverrep)/np.sqrt(len(numberTrialsUnderrepOverrep)))\n",
    "\n",
    "print(pg.corr(np.abs(bias-0.5)*2, accuracyOverrepOverrep*100, method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, accuracyOverrepUnderrep*100, method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, accuracyUnderrepUnderrep*100,  method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, accuracyUnderrepOverrep*100, method='spearman'))\n",
    "\n",
    "ax1.set_xlabel('Internalized Bias', fontsize=20)\n",
    "ax1.set_ylabel('Accuracy',fontsize=20)\n",
    "ax1.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax1.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax1.set_xticklabels(np.around(np.arange(0,0.7,0.2),1))\n",
    "ax1.set_ylim([0,110])\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.set_xlabel('Internalized Bias', fontsize=20)\n",
    "ax2.set_ylabel('Accuracy',fontsize=20)\n",
    "ax2.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax2.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax2.set_xticklabels(np.around(np.arange(0,0.7,0.2),1))\n",
    "ax2.set_ylim([0,110])\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "fig, [ax1,ax2] = plt.subplots(1,2,figsize=(15,6))\n",
    "ax1.plot(np.abs(0.5-biasLowForSubjectsWithNoAndLowContexts[:-5])*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsLowContextLL'][:-5]*100,'o',color='pink')\n",
    "ax1.plot(np.abs(0.5-biasLowForSubjectsWithNoAndLowContexts[:-5])*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsLowContextLH'][:-5]*100,'o',color='mediumorchid')\n",
    "ax1.plot(np.abs(0.5-biasHighForSubjectsWithNoAndHighContexts)*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextHH'][:-5]*100,'o',color='pink')\n",
    "ax1.plot(np.abs(0.5-biasHighForSubjectsWithNoAndHighContexts)*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextHL'][:-5]*100,'o',color='mediumorchid')\n",
    "ax2.plot(np.abs(0.5-biasLowForSubjectsWithNoAndLowContexts[:-5])*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsLowContextHL'][:-5]*100,'o',color='pink')\n",
    "ax2.plot(np.abs(0.5-biasLowForSubjectsWithNoAndLowContexts[:-5])*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsLowContextHH'][:-5]*100,'o',color='mediumorchid')\n",
    "ax2.plot(np.abs(0.5-biasHighForSubjectsWithNoAndHighContexts)*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextLH'][:-5]*100,'o',color='pink')\n",
    "ax2.plot(np.abs(0.5-biasHighForSubjectsWithNoAndHighContexts)*2,\n",
    "        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextLL'][:-5]*100,'o',color='mediumorchid')\n",
    "ax1.set_xlabel('Internalized Bias', fontsize=20)\n",
    "ax1.set_ylabel('Accuracy',fontsize=20)\n",
    "ax1.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax1.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax1.set_xticklabels(np.around(np.arange(0,0.7,0.2),1))\n",
    "ax1.set_ylim([0,110])\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "fig1.savefig('figures/FromProlific/illustrations/biasEffectsOnTrialPairs_previousTrialOverrepresented.pdf',\n",
    "            bbox_inches=\"tight\",transparent=True)\n",
    "ax2.set_xlabel('Internalized Bias', fontsize=20)\n",
    "ax2.set_ylabel('Accuracy',fontsize=20)\n",
    "ax2.tick_params(axis='both',labelsize=18,length=6,width=2)\n",
    "ax2.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax2.set_xticklabels(np.around(np.arange(0,0.7,0.2),1))\n",
    "ax2.set_ylim([0,110])\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "fig2.savefig('figures/FromProlific/illustrations/biasEffectsOnTrialPairs_previousTrialUnderrepresented.pdf',\n",
    "             bbox_inches=\"tight\",transparent=True)\n",
    "\n",
    "print(pg.corr(np.abs(bias-0.5)*2, \n",
    "              np.append(internalizedBias['AccuracyOfMinOneSignalTrialsLowContextLL'][:-5]*100,\n",
    "                        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextHH'][:-5]*100),\n",
    "                        method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, \n",
    "              np.append(internalizedBias['AccuracyOfMinOneSignalTrialsLowContextLH'][:-5]*100,\n",
    "                        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextHL'][:-5]*100),\n",
    "                        method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, \n",
    "              np.append(internalizedBias['AccuracyOfMinOneSignalTrialsLowContextHH'][:-5]*100,\n",
    "                        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextLL'][:-5]*100),\n",
    "                        method='spearman'))\n",
    "print(pg.corr(np.abs(bias-0.5)*2, \n",
    "              np.append(internalizedBias['AccuracyOfMinOneSignalTrialsLowContextHL'][:-5]*100,\n",
    "                        internalizedBias['AccuracyOfMinOneSignalTrialsHighContextLH'][:-5]*100),\n",
    "                        method='spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qs: Model free analysis of short term effects of bias.\n",
    "\"\"\"\n",
    "\n",
    "fig,[[ax1,ax2],[ax3,ax4]] = plt.subplots(2,2,figsize=(15,15))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.3, hspace=None)\n",
    "\n",
    "bias = np.append(biasLowForAllSubjectsWithLowContext[:-5], biasHighForAllSubjectsWithHighContext[:-5])\n",
    "accuracyOverrepOverrep = np.append(internalizedBias['AccuracyOfSimpleTrialsLowContextLL'][:-5],\n",
    "                                   internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][:-5])\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextLL'][:-5]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2,\n",
    "         internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*100,\n",
    "         'o',color='teal',markersize=10)\n",
    "ax1.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextLH'][:-5]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2,\n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextHH'][:-5]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2,\n",
    "         internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*100,\n",
    "         'o',color='teal',markersize=10)\n",
    "ax2.plot((0.5-biasLowForAllSubjectsWithLowContext[:-5])*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsLowContextHL'][:-5]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "                 \n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'][~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'])]*100,\n",
    "         'o',color='teal',markersize=10)\n",
    "ax3.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextHL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHL'])]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])]*100,\n",
    "         'P',color='pink',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2,\n",
    "         internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'][~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'])]*100,\n",
    "         'o',color='teal',markersize=10)\n",
    "ax4.plot((biasHighForAllSubjectsWithHighContext-0.5)*2, \n",
    "         internalizedBias['AccuracyOfSimpleTrialsHighContextLH'][~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLH'])]*100,\n",
    "         'purple',marker='d',linestyle='none',markersize=10)\n",
    "\n",
    "print(\"Avg accuracy in low context when both trials are low but current stimulus has 1/2 distractor\",\n",
    "     np.mean(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*100))\n",
    "print(\"Std accuracy in low context when both trials are low but current stimulus has 1/2 distractor\",\n",
    "     np.std(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*100))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*100),\n",
    "      np.std(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextLL'][:-5]*100))\n",
    "print(\"Avg accuracy in low context when both trials are high but current stimulus has one distractor\",\n",
    "     np.mean(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*100))\n",
    "print(\"Std accuracy in low context when both trials are high but current stimulus has one distractor\",\n",
    "     np.std(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*100))\n",
    "print(\"mean and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*100),\n",
    "      np.std(internalizedBias['NumberOfOneAndTwoDistractorTrialsLowContextHH'][:-5]*100))\n",
    "\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextLL'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryLowContextLowGaussianTrials']*internalizedBias['NumberOfLowContextLowGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextLL']*internalizedBias['NumberOfSimpleTrialsLowContextLL'])/\n",
    "         (internalizedBias['NumberOfLowContextLowGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextLL']),\n",
    "             method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextLH'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextHH'], method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              (internalizedBias['ExpectationOfPriorCategoryLowContextHighGaussianTrials']*internalizedBias['NumberOfLowContextHighGaussianTrials']\n",
    "         -internalizedBias['AccuracyOfSimpleTrialsLowContextHH']*internalizedBias['NumberOfSimpleTrialsLowContextHH'])/\n",
    "         (internalizedBias['NumberOfLowContextHighGaussianTrials']-internalizedBias['NumberOfSimpleTrialsLowContextHH']),\n",
    "             method='spearman'))\n",
    "print(pg.corr(0.5-biasLowForAllSubjectsWithLowContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsLowContextHL'], method='spearman'))\n",
    "\n",
    "print(\"Avg accuracy in high context when both trials are high but current stimulus has 1/2 distractor\",\n",
    "     np.mean(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "             [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'])])*100)\n",
    "print(\"Std accuracy in high context when both trials are high but current stimulus has one distractor\",\n",
    "     np.std(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "            [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'])])*100)\n",
    "print(\"Avg and Std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "              [~np.isnan(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'])]),\n",
    "      np.std(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "             [~np.isnan(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextHH'])]))\n",
    "\n",
    "print(\"Avg accuracy in high context when both trials are low but current stimulus has 1/2 distractor\",\n",
    "     np.mean(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL']\n",
    "             [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'])])*100)\n",
    "print(\"Std accuracy in high context when both trials are low but current stimulus has 1/2 distractor\",\n",
    "     np.std(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL']\n",
    "            [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextLL'])])*100)\n",
    "print(\"Avg and std of number of trials for the above\",\n",
    "      np.mean(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL']\n",
    "              [~np.isnan(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'])]),\n",
    "      np.std(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL']\n",
    "             [~np.isnan(internalizedBias['NumberOfOneAndTwoDistractorTrialsHighContextLL'])]))\n",
    "\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextHH']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHH'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "             internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'])],method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextHL']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextHL'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextLL']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLL'])], method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfOneAndTwoDistractorTrialsHighContextHH'])],method='spearman'))\n",
    "print(pg.corr(-0.5+biasHighForAllSubjectsWithHighContext,\n",
    "              internalizedBias['AccuracyOfSimpleTrialsHighContextLH']\n",
    "              [~np.isnan(internalizedBias['AccuracyOfSimpleTrialsHighContextLH'])], method='spearman'))\n",
    "\n",
    "\n",
    "ax1.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax1.set_ylabel('Accuracy',fontsize=25)\n",
    "ax1.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax1.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax1.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "ax2.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax2.set_ylabel('Accuracy',fontsize=25)\n",
    "ax2.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax2.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax2.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "ax3.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax3.set_ylabel('Accuracy',fontsize=25)\n",
    "ax3.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax3.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax3.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax3.spines['top'].set_visible(False)\n",
    "ax3.spines['right'].set_visible(False)\n",
    "ax4.set_xlabel('Internalized Bias', fontsize=25)\n",
    "ax4.set_ylabel('Accuracy',fontsize=25)\n",
    "ax4.tick_params(axis='both',labelsize=23,length=6,width=2)\n",
    "ax4.set_xticks(np.arange(0,0.7,0.2))\n",
    "ax4.set_xticklabels(np.around(np.arange(0,0.7,0.2),1),fontsize=23)\n",
    "ax4.spines['top'].set_visible(False)\n",
    "ax4.spines['right'].set_visible(False)\n",
    "\n",
    "#plt.savefig('figures/FromProlific/illustrations/biasEffectsOnTrialPairs_longContext.eps',bbox_inches=\"tight\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

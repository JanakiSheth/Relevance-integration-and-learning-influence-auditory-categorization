{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.io import savemat\n",
    "import scipy\n",
    "import matplotlib.cm as cm\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental task\n",
    "def task(freq_seq, lm, hm, s, n_trials = 10, n_tones = 3, p_low = 0.5, p_back = 0.3):\n",
    "    log_freq_seq_mid = np.median(exptFreqSeqArray)\n",
    "    log_freq_low = [lm,s]#[log_freq_seq_mid - 0.15,0.1]  #low freq condition is gaussian \n",
    "    log_freq_high = [hm,s]#[log_freq_seq_mid + 0.15,0.1] #high freq condition is gaussian\n",
    "    trial_tones = []\n",
    "    dist_chosen = np.array([])\n",
    "    kind_of_tones = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        signal_rand = np.random.random()\n",
    "        low_dist = signal_rand < p_low #choosing true tone from either low or high condition\n",
    "        tones = []\n",
    "        tone_kind = []\n",
    "        for n_tone in range(n_tones):\n",
    "            signal_back = np.random.random()\n",
    "            background = signal_back < p_back #choosing background or true tone\n",
    "            if background:\n",
    "                nearest_log_tone = np.random.choice(exptFreqSeqArray)\n",
    "                #background freq is chosen from a uniform distribution\n",
    "                tone_kind.append(0)\n",
    "            else: \n",
    "                if low_dist:\n",
    "                    tone = min(max(np.random.randn()*log_freq_low[1] + log_freq_low[0],\\\n",
    "                                   exptFreqSeqArray[0]),exptFreqSeqArray[-1])                    \n",
    "                    tone_kind.append(1)\n",
    "                else:\n",
    "                    tone = min(max(np.random.randn()*log_freq_high[1] + log_freq_high[0],\\\n",
    "                                   exptFreqSeqArray[0]),exptFreqSeqArray[-1])\n",
    "                    tone_kind.append(2)\n",
    "                nearest_log_tone = exptFreqSeqArray[np.argmin(np.abs(exptFreqSeqArray - tone))]\n",
    "            nearest_tone = freq_seq[np.argmin(np.abs(freq_seq - 10**nearest_log_tone))]        \n",
    "            tones.append(nearest_tone)\n",
    "        trial_tones.append(tones)\n",
    "        dist_chosen = np.append(dist_chosen, 1-low_dist)\n",
    "        kind_of_tones.append(tone_kind)\n",
    "    return trial_tones, dist_chosen, kind_of_tones, log_freq_low, log_freq_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqSeq = np.arange(90,3000,1) #array of possible true tones\n",
    "exptFreqSeqArray = np.arange(np.log10(freqSeq[0]), np.log10(freqSeq[-1]), np.log10(1003/1000)*40)\n",
    "logFreqBins = exptFreqSeqArray[np.arange(1,len(exptFreqSeqArray),2)]#(exptFreqSeqArray[:-1]+exptFreqSeqArray[1:])/2\n",
    "expt_freq_seq_mid = np.median(exptFreqSeqArray)\n",
    "low_dist = [expt_freq_seq_mid - 0.15,0.1]\n",
    "high_dist = [expt_freq_seq_mid + 0.15,0.1]\n",
    "    \n",
    "def trialCategories(trial_tones, trial_behaviour, corrans):\n",
    "    \"\"\"\n",
    "    dividing trials into different categories : \n",
    "    1. all trials with tones from gaussian distributions\n",
    "    2. all trials with one tone as distractor from extremes\n",
    "    3. all trials with one tone as distractor from gaussian distribution\n",
    "    4. all trials with two tones as distractor from opp extremes\n",
    "    5. all trials with two tones as distractors (one from opp and other from same side extreme)\n",
    "    \n",
    "    These will be stored in arrays where the first 3 columns are the tones. 4th is the subject's behaviour.\n",
    "    5th is the correct answer.\n",
    "    6th and 7th is optimal subject's reponses given the strategy of taking an average of all the tones.\n",
    "    8th and 9th is optimal subject's responses given the strategy of ignoring the distractor tones and taking \n",
    "    an average of the remaining tones.\n",
    "    10th is optimal subject's responses given the strategy of taking majority category (i.e. 2 out of 3).\n",
    "    11th and 12th is optimal subject's responses given the strategy of using only the \n",
    "    highest tone for classification.\n",
    "    \n",
    "    As of 04-27 I think that pback=1 in the model corresponds to taking an average over all the tones \n",
    "    (because in that case we think that each of the tones has signal in it). However taking pback=0 in the model \n",
    "    corresponds to the other two cases, because when pback = 0 for a particular tone it implies that the tone \n",
    "    does not give information about the category\n",
    "    \"\"\"\n",
    "    \n",
    "    allGaussLow = np.zeros((len(trial_behaviour),13))\n",
    "    allGaussHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distGaussHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distGaussLow = np.zeros((len(trial_behaviour),13))\n",
    "    distLowOtherLow = np.zeros((len(trial_behaviour),13))\n",
    "    distLowOtherHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distHighOtherLow = np.zeros((len(trial_behaviour),13))\n",
    "    distHighOtherHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distLowOtherLowHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distHighOtherLowHigh = np.zeros((len(trial_behaviour),13))\n",
    "    distTwoExtrOtherGauss = np.zeros((len(trial_behaviour),13))\n",
    "    distTwoExtrSameGauss = np.zeros((len(trial_behaviour),13))\n",
    "    distThreeExtr = np.zeros((len(trial_behaviour),13))\n",
    "    distThreeExtrTwoSame = np.zeros((len(trial_behaviour),13))\n",
    "    oneDistractor = np.zeros((len(trial_behaviour),5))\n",
    "    twoDistractors = np.zeros((len(trial_behaviour),5))\n",
    "    \n",
    "    n_tones = len(trial_tones[0])\n",
    "    print(high_dist[0]+2*high_dist[1], \n",
    "          low_dist[0]-2*low_dist[1], \n",
    "          expt_freq_seq_mid)\n",
    "    \n",
    "    iAllGaussLow = 0; iAllGaussHigh = 0; iDistGaussHigh = 0; iDistGaussLow = 0; iDistLowOtherLow = 0; \n",
    "    iDistLowOtherHigh = 0; iDistHighOtherLow = 0; iDistHighOtherHigh = 0; iDistLowOtherLowHigh = 0;\n",
    "    iDistHighOtherLowHigh = 0; iDistTwoExtrOtherGauss = 0; iDistTwoExtrSameGauss = 0; iDistThreeExtr = 0; \n",
    "    iDistThreeExtrTwoSame = 0; iOneDistractor = 0; iTwoDistractors = 0;\n",
    "    \n",
    "    for i_trial in range(len(trial_tones)):\n",
    "        if (sum(trial_tones[i_trial]<expt_freq_seq_mid)==3 \n",
    "            & sum(trial_tones[i_trial]>(low_dist[0]-2*low_dist[1]))==3):\n",
    "            allGaussLow[iAllGaussLow,:3] = trial_tones[i_trial,:]\n",
    "            allGaussLow[iAllGaussLow,3] = trial_behaviour[i_trial]\n",
    "            allGaussLow[iAllGaussLow,4] = corrans[i_trial]\n",
    "            allGaussLow[iAllGaussLow,5] = np.mean(allGaussLow[iAllGaussLow,:3])\n",
    "            allGaussLow[iAllGaussLow,6] = allGaussLow[iAllGaussLow,5] > expt_freq_seq_mid \n",
    "            allGaussLow[iAllGaussLow,7] = allGaussLow[iAllGaussLow,5]\n",
    "            allGaussLow[iAllGaussLow,8] = allGaussLow[iAllGaussLow,6]\n",
    "            allGaussLow[iAllGaussLow,9] = allGaussLow[iAllGaussLow,6]\n",
    "            allGaussLow[iAllGaussLow,10] = np.max(trial_tones[i_trial,:]) \n",
    "            allGaussLow[iAllGaussLow,11] = allGaussLow[iAllGaussLow,10] > expt_freq_seq_mid\n",
    "            allGaussLow[iAllGaussLow,12] = int(i_trial)\n",
    "            iAllGaussLow += 1\n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>expt_freq_seq_mid)==3 \n",
    "              & sum(trial_tones[i_trial]<(high_dist[0]+2*high_dist[1]))==3): \n",
    "            allGaussHigh[iAllGaussHigh,:3] = trial_tones[i_trial,:]\n",
    "            allGaussHigh[iAllGaussHigh,3] = trial_behaviour[i_trial]\n",
    "            allGaussHigh[iAllGaussHigh,4] = corrans[i_trial]\n",
    "            allGaussHigh[iAllGaussHigh,5] = np.mean(allGaussHigh[iAllGaussHigh,:3])\n",
    "            allGaussHigh[iAllGaussHigh,6] = allGaussHigh[iAllGaussHigh,5] > expt_freq_seq_mid \n",
    "            allGaussHigh[iAllGaussHigh,7] = allGaussHigh[iAllGaussHigh,5]\n",
    "            allGaussHigh[iAllGaussHigh,8] = allGaussHigh[iAllGaussHigh,6]\n",
    "            allGaussHigh[iAllGaussHigh,9] = allGaussHigh[iAllGaussHigh,6]  \n",
    "            allGaussHigh[iAllGaussHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            allGaussHigh[iAllGaussHigh,11] = allGaussHigh[iAllGaussHigh,10] > expt_freq_seq_mid\n",
    "            allGaussHigh[iAllGaussHigh,12] = int(i_trial)\n",
    "            iAllGaussHigh += 1\n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]<expt_freq_seq_mid)==1\n",
    "              and sum(trial_tones[i_trial]>expt_freq_seq_mid)==2\n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):\n",
    "            distGaussLow[iDistGaussLow,:3] = trial_tones[i_trial,:]\n",
    "            distGaussLow[iDistGaussLow,3] = trial_behaviour[i_trial]\n",
    "            distGaussLow[iDistGaussLow,4] = corrans[i_trial]\n",
    "            distGaussLow[iDistGaussLow,5] = np.mean(distGaussLow[iDistGaussLow,:3])\n",
    "            distGaussLow[iDistGaussLow,6] = distGaussLow[iDistGaussLow,5] > expt_freq_seq_mid\n",
    "            distGaussLow[iDistGaussLow,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]>expt_freq_seq_mid])\n",
    "            distGaussLow[iDistGaussLow,8] = distGaussLow[iDistGaussLow,7] > expt_freq_seq_mid\n",
    "            distGaussLow[iDistGaussLow,9] = 1\n",
    "            distGaussLow[iDistGaussLow,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distGaussLow[iDistGaussLow,11] = distGaussLow[iDistGaussLow,10] > expt_freq_seq_mid \n",
    "            distGaussLow[iDistGaussLow,12] = int(i_trial)\n",
    "            #print(distGaussLow[iDistGaussLow,:3])\n",
    "            iDistGaussLow += 1\n",
    "            #pdb.set_trace()    \n",
    "        \n",
    "        elif (sum(trial_tones[i_trial]<expt_freq_seq_mid)==2\n",
    "              and sum(trial_tones[i_trial]>expt_freq_seq_mid)==1\n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):\n",
    "            distGaussHigh[iDistGaussHigh,:3] = trial_tones[i_trial,:]\n",
    "            distGaussHigh[iDistGaussHigh,3] = trial_behaviour[i_trial]\n",
    "            distGaussHigh[iDistGaussHigh,4] = corrans[i_trial]\n",
    "            distGaussHigh[iDistGaussHigh,5] = np.mean(distGaussHigh[iDistGaussHigh,:3])\n",
    "            distGaussHigh[iDistGaussHigh,6] = distGaussHigh[iDistGaussHigh,5] > expt_freq_seq_mid\n",
    "            distGaussHigh[iDistGaussHigh,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]<expt_freq_seq_mid])\n",
    "            distGaussHigh[iDistGaussHigh,8] = distGaussHigh[iDistGaussHigh,7] > expt_freq_seq_mid\n",
    "            distGaussHigh[iDistGaussHigh,9] = 0\n",
    "            distGaussHigh[iDistGaussHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distGaussHigh[iDistGaussHigh,11] = distGaussHigh[iDistGaussHigh,10] > expt_freq_seq_mid \n",
    "            distGaussHigh[iDistGaussHigh,12] = int(i_trial)\n",
    "            #print(distGaussHigh[iDistGaussHigh,:3])\n",
    "            iDistGaussHigh += 1\n",
    "            #pdb.set_trace()    \n",
    "        \n",
    "        elif (sum(trial_tones[i_trial]<expt_freq_seq_mid)==3 \n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==1):\n",
    "            distLowOtherLow[iDistLowOtherLow,:3] = trial_tones[i_trial,:]\n",
    "            distLowOtherLow[iDistLowOtherLow,3] = trial_behaviour[i_trial]\n",
    "            distLowOtherLow[iDistLowOtherLow,4] = corrans[i_trial]\n",
    "            distLowOtherLow[iDistLowOtherLow,5] = np.mean(distLowOtherLow[iDistLowOtherLow,:3])\n",
    "            distLowOtherLow[iDistLowOtherLow,6] = distLowOtherLow[iDistLowOtherLow,5] > expt_freq_seq_mid\n",
    "            distLowOtherLow[iDistLowOtherLow,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]>(low_dist[0]-2*low_dist[1])])\n",
    "            distLowOtherLow[iDistLowOtherLow,8] = distLowOtherLow[iDistLowOtherLow,7] >expt_freq_seq_mid\n",
    "            distLowOtherLow[iDistLowOtherLow,9] = 0\n",
    "            distLowOtherLow[iDistLowOtherLow,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distLowOtherLow[iDistLowOtherLow,11] = distLowOtherLow[iDistLowOtherLow,10] > expt_freq_seq_mid \n",
    "            distLowOtherLow[iDistLowOtherLow,12] = int(i_trial)\n",
    "            iDistLowOtherLow += 1\n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>expt_freq_seq_mid)==3 \n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==1):\n",
    "            distHighOtherHigh[iDistHighOtherHigh,:3] = trial_tones[i_trial,:]\n",
    "            distHighOtherHigh[iDistHighOtherHigh,3] = trial_behaviour[i_trial]\n",
    "            distHighOtherHigh[iDistHighOtherHigh,4] = corrans[i_trial]\n",
    "            distHighOtherHigh[iDistHighOtherHigh,5] = np.mean(distHighOtherHigh[iDistHighOtherHigh,:3])\n",
    "            distHighOtherHigh[iDistHighOtherHigh,6] = distHighOtherHigh[iDistHighOtherHigh,5] > expt_freq_seq_mid\n",
    "            distHighOtherHigh[iDistHighOtherHigh,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]<(high_dist[0]+2*high_dist[1])])\n",
    "            distHighOtherHigh[iDistHighOtherHigh,8] = distHighOtherHigh[iDistHighOtherHigh,7] > expt_freq_seq_mid\n",
    "            distHighOtherHigh[iDistHighOtherHigh,9] = 1\n",
    "            distHighOtherHigh[iDistHighOtherHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distHighOtherHigh[iDistHighOtherHigh,11] = distHighOtherHigh[iDistHighOtherHigh,10] > expt_freq_seq_mid \n",
    "            distHighOtherHigh[iDistHighOtherHigh,12] = int(i_trial)\n",
    "            iDistHighOtherHigh += 1\n",
    "                \n",
    "        elif (sum(trial_tones[i_trial]>expt_freq_seq_mid)==2 \n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==1):    \n",
    "            distLowOtherHigh[iDistLowOtherHigh,:3] = trial_tones[i_trial,:]\n",
    "            distLowOtherHigh[iDistLowOtherHigh,3] = trial_behaviour[i_trial]\n",
    "            distLowOtherHigh[iDistLowOtherHigh,4] = corrans[i_trial]\n",
    "            distLowOtherHigh[iDistLowOtherHigh,5] = np.mean(distLowOtherHigh[iDistLowOtherHigh,:3])\n",
    "            distLowOtherHigh[iDistLowOtherHigh,6] = distLowOtherHigh[iDistLowOtherHigh,5] > expt_freq_seq_mid\n",
    "            distLowOtherHigh[iDistLowOtherHigh,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]>expt_freq_seq_mid])\n",
    "            distLowOtherHigh[iDistLowOtherHigh,8] = distLowOtherHigh[iDistLowOtherHigh,7] > expt_freq_seq_mid\n",
    "            distLowOtherHigh[iDistLowOtherHigh,9] = 1\n",
    "            distLowOtherHigh[iDistLowOtherHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distLowOtherHigh[iDistLowOtherHigh,11] = distLowOtherHigh[iDistLowOtherHigh,10] > expt_freq_seq_mid \n",
    "            distLowOtherHigh[iDistLowOtherHigh,12] = int(i_trial)\n",
    "            iDistLowOtherHigh += 1\n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]<expt_freq_seq_mid)==2 \n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==1\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):\n",
    "            distHighOtherLow[iDistHighOtherLow,:3] = trial_tones[i_trial,:]\n",
    "            distHighOtherLow[iDistHighOtherLow,3] = trial_behaviour[i_trial]\n",
    "            distHighOtherLow[iDistHighOtherLow,4] = corrans[i_trial]\n",
    "            distHighOtherLow[iDistHighOtherLow,5] = np.mean(distHighOtherLow[iDistHighOtherLow,:3])\n",
    "            distHighOtherLow[iDistHighOtherLow,6] = distHighOtherLow[iDistHighOtherLow,5] > expt_freq_seq_mid\n",
    "            distHighOtherLow[iDistHighOtherLow,7] = np.mean(trial_tones[i_trial]\n",
    "                                                         [trial_tones[i_trial]<expt_freq_seq_mid])\n",
    "            distHighOtherLow[iDistHighOtherLow,8] = distHighOtherLow[iDistHighOtherLow,7] > expt_freq_seq_mid\n",
    "            distHighOtherLow[iDistHighOtherLow,9] = 0\n",
    "            distHighOtherLow[iDistHighOtherLow,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distHighOtherLow[iDistHighOtherLow,11] = distHighOtherLow[iDistHighOtherLow,10] > expt_freq_seq_mid \n",
    "            distHighOtherLow[iDistHighOtherLow,12] = int(i_trial)\n",
    "            iDistHighOtherLow += 1\n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>expt_freq_seq_mid)==1 \n",
    "              and sum(trial_tones[i_trial]<expt_freq_seq_mid)==2 \n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==1):    \n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,:3] = trial_tones[i_trial,:]\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,3] = trial_behaviour[i_trial]\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,4] = corrans[i_trial]\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,5] = np.mean(distLowOtherLowHigh[iDistLowOtherLowHigh,:3])\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,6] = distLowOtherLowHigh[iDistLowOtherLowHigh,5] > expt_freq_seq_mid\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,7] = None\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,8] = None\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,9] = 0\n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,11] = distLowOtherLowHigh[iDistLowOtherLowHigh,10] > expt_freq_seq_mid \n",
    "            distLowOtherLowHigh[iDistLowOtherLowHigh,12] = int(i_trial)\n",
    "            #print('a',distLowOtherLowHigh[iDistLowOtherLowHigh,:3])\n",
    "            iDistLowOtherLowHigh += 1    \n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>expt_freq_seq_mid)==2 \n",
    "              and sum(trial_tones[i_trial]<expt_freq_seq_mid)==1 \n",
    "              and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==1\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):    \n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,:3] = trial_tones[i_trial,:]\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,3] = trial_behaviour[i_trial]\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,4] = corrans[i_trial]\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,5] = np.mean(distHighOtherLowHigh[iDistHighOtherLowHigh,:3])\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,6] = distHighOtherLowHigh[iDistHighOtherLowHigh,5] > expt_freq_seq_mid\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,7] = None\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,8] = None\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,9] = 1\n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,11] = distHighOtherLowHigh[iDistHighOtherLowHigh,10] > expt_freq_seq_mid \n",
    "            distHighOtherLowHigh[iDistHighOtherLowHigh,12] = int(i_trial)\n",
    "            #print('a',distHighOtherLowHigh[iDistHighOtherLowHigh,:3])\n",
    "            iDistHighOtherLowHigh += 1 \n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==2 \n",
    "              and sum(trial_tones[i_trial]<expt_freq_seq_mid)==1\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3] = trial_tones[i_trial,:]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,3] = trial_behaviour[i_trial]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,4] = corrans[i_trial]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,5] = np.mean(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3])\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,6] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,5] > expt_freq_seq_mid\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,7] = trial_tones[i_trial][np.argmin(abs(trial_tones[i_trial]-expt_freq_seq_mid))]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,8] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,7] > expt_freq_seq_mid\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,11] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,10] > expt_freq_seq_mid \n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,12] = int(i_trial)\n",
    "            #print(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3])\n",
    "            iDistTwoExtrOtherGauss += 1\n",
    "              \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0 \n",
    "              and sum(trial_tones[i_trial]>expt_freq_seq_mid)==1\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==2):            \n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3] = trial_tones[i_trial,:]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,3] = trial_behaviour[i_trial]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,4] = corrans[i_trial]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,5] = np.mean(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3])\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,6] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,5] > expt_freq_seq_mid\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,7] = trial_tones[i_trial][np.argmin(abs(trial_tones[i_trial]-expt_freq_seq_mid))]\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,8] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,7] > expt_freq_seq_mid\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,11] = distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,10] > expt_freq_seq_mid \n",
    "            distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,12] = int(i_trial)\n",
    "            #print(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,:3])\n",
    "            iDistTwoExtrOtherGauss += 1  \n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==0 \n",
    "              and sum(trial_tones[i_trial]<expt_freq_seq_mid)==3\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==2):\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,:3] = trial_tones[i_trial,:]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,3] = trial_behaviour[i_trial]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,4] = corrans[i_trial]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,5] = np.mean(distTwoExtrSameGauss[iDistTwoExtrSameGauss,:3])\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,6] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,5] > expt_freq_seq_mid\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,7] = trial_tones[i_trial][np.argmin(abs(trial_tones[i_trial]-expt_freq_seq_mid))]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,8] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,7] > expt_freq_seq_mid\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,11] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,10] > expt_freq_seq_mid \n",
    "            #print(distTwoExtrSameGauss[iDistTwoExtrSameGauss,[0,1,2,3,4,6,8,9]])\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,12] = int(i_trial)\n",
    "            iDistTwoExtrSameGauss += 1\n",
    "              \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==2\n",
    "              and sum(trial_tones[i_trial]>expt_freq_seq_mid)==3\n",
    "              and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==0):            \n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,:3] = trial_tones[i_trial,:]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,3] = trial_behaviour[i_trial]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,4] = corrans[i_trial]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,5] = np.mean(distTwoExtrSameGauss[iDistTwoExtrSameGauss,:3])\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,6] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,5] > expt_freq_seq_mid\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,7] = trial_tones[i_trial][np.argmin(abs(trial_tones[i_trial]-expt_freq_seq_mid))]\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,8] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,7] > expt_freq_seq_mid\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,11] = distTwoExtrSameGauss[iDistTwoExtrSameGauss,10] > expt_freq_seq_mid \n",
    "            #print(distTwoExtrSameGauss[iDistTwoExtrSameGauss,[0,1,2,3,4,6,8,9]])\n",
    "            distTwoExtrSameGauss[iDistTwoExtrSameGauss,12] = int(i_trial)\n",
    "            iDistTwoExtrSameGauss += 1    \n",
    "            \n",
    "        elif (sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==3):\n",
    "            distThreeExtr[iDistThreeExtr,:3] = trial_tones[i_trial,:]\n",
    "            distThreeExtr[iDistThreeExtr,3] = trial_behaviour[i_trial]\n",
    "            distThreeExtr[iDistThreeExtr,4] = corrans[i_trial]\n",
    "            distThreeExtr[iDistThreeExtr,5] = np.mean(distThreeExtr[iDistThreeExtr,:3])\n",
    "            distThreeExtr[iDistThreeExtr,6] = distThreeExtr[iDistThreeExtr,5] > expt_freq_seq_mid\n",
    "            distThreeExtr[iDistThreeExtr,7] = None\n",
    "            distThreeExtr[iDistThreeExtr,8] = None\n",
    "            distThreeExtr[iDistThreeExtr,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distThreeExtr[iDistThreeExtr,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distThreeExtr[iDistThreeExtr,11] = distThreeExtr[iDistThreeExtr,10] > expt_freq_seq_mid \n",
    "            distThreeExtr[iDistThreeExtr,12] = int(i_trial)\n",
    "            #print(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,[0,1,2,3,5,6,7,8,9]])\n",
    "            iDistThreeExtr += 1\n",
    "              \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==3):\n",
    "            distThreeExtr[iDistThreeExtr,:3] = trial_tones[i_trial,:]\n",
    "            distThreeExtr[iDistThreeExtr,3] = trial_behaviour[i_trial]\n",
    "            distThreeExtr[iDistThreeExtr,4] = corrans[i_trial]\n",
    "            distThreeExtr[iDistThreeExtr,5] = np.mean(distThreeExtr[iDistThreeExtr,:3])\n",
    "            distThreeExtr[iDistThreeExtr,6] = distThreeExtr[iDistThreeExtr,5] > expt_freq_seq_mid\n",
    "            distThreeExtr[iDistThreeExtr,7] = None\n",
    "            distThreeExtr[iDistThreeExtr,8] = None\n",
    "            distThreeExtr[iDistThreeExtr,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distThreeExtr[iDistThreeExtr,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distThreeExtr[iDistThreeExtr,11] = distThreeExtr[iDistThreeExtr,10] > expt_freq_seq_mid \n",
    "            distThreeExtr[iDistThreeExtr,12] = int(i_trial)\n",
    "            #print(distTwoExtrOtherGauss[iDistTwoExtrOtherGauss,[0,1,2,3,5,6,7,8,9]])\n",
    "            iDistThreeExtr += 1    \n",
    "       \n",
    "        elif (sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==2\n",
    "             and sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==1):\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,:3] = trial_tones[i_trial,:]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,3] = trial_behaviour[i_trial]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,4] = corrans[i_trial]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,5] = np.mean(distThreeExtrTwoSame[iDistThreeExtrTwoSame,:3])\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,6] = distThreeExtrTwoSame[iDistThreeExtrTwoSame,5] > expt_freq_seq_mid\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,7] = None\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,8] = None\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,11] = distThreeExtrTwoSame[iDistThreeExtrTwoSame,10] > expt_freq_seq_mid \n",
    "            #print(distThreeExtrTwoSame[iDistThreeExtrTwoSame,[0,1,2,3,4,5,6,7,8,9]])\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,12] = int(i_trial)\n",
    "            iDistThreeExtrTwoSame += 1\n",
    "              \n",
    "        elif (sum(trial_tones[i_trial]>(high_dist[0]+2*high_dist[1]))==2\n",
    "             and sum(trial_tones[i_trial]<(low_dist[0]-2*low_dist[1]))==1):\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,:3] = trial_tones[i_trial,:]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,3] = trial_behaviour[i_trial]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,4] = corrans[i_trial]\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,5] = np.mean(distThreeExtrTwoSame[iDistThreeExtrTwoSame,:3])\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,6] = distThreeExtrTwoSame[iDistThreeExtrTwoSame,5] > expt_freq_seq_mid\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,7] = None\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,8] = None\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,9] = sum(trial_tones[i_trial]>expt_freq_seq_mid)>1\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,10] = np.max(trial_tones[i_trial,:]) \n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,11] = distThreeExtrTwoSame[iDistThreeExtrTwoSame,10] > expt_freq_seq_mid \n",
    "            #print(distThreeExtrTwoSame[iDistThreeExtrTwoSame,[0,1,2,3,4,5,6,7,8,9]])\n",
    "            distThreeExtrTwoSame[iDistThreeExtrTwoSame,12] = int(i_trial)\n",
    "            iDistThreeExtrTwoSame += 1   \n",
    "\n",
    "    for i_trial in range(len(trial_tones)):         \n",
    "        if ((int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][0]<expt_freq_seq_mid) +\n",
    "             int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][1]<expt_freq_seq_mid) +\n",
    "             int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][2]<expt_freq_seq_mid))==2):\n",
    "            oneDistractor[iOneDistractor,:3] = trial_tones[i_trial,:]\n",
    "            oneDistractor[iOneDistractor,3] = trial_behaviour[i_trial]\n",
    "            oneDistractor[iOneDistractor,4] = corrans[i_trial]\n",
    "            iOneDistractor += 1   \n",
    "            \n",
    "        elif ((int(expt_freq_seq_mid<trial_tones[i_trial][0]<high_dist[0]+2*high_dist[1]) +\n",
    "             int(expt_freq_seq_mid<trial_tones[i_trial][1]<high_dist[0]+2*high_dist[1]) +\n",
    "             int(expt_freq_seq_mid<trial_tones[i_trial][2]<high_dist[0]+2*high_dist[1]))==2):\n",
    "            oneDistractor[iOneDistractor,:3] = trial_tones[i_trial,:]\n",
    "            oneDistractor[iOneDistractor,3] = trial_behaviour[i_trial]\n",
    "            oneDistractor[iOneDistractor,4] = corrans[i_trial]\n",
    "            iOneDistractor += 1\n",
    "            \n",
    "        elif ((int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][0]<high_dist[0]+2*high_dist[1]) +\n",
    "             int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][1]<high_dist[0]+2*high_dist[1]) +\n",
    "             int(low_dist[0]-2*low_dist[1]<trial_tones[i_trial][2]<high_dist[0]+2*high_dist[1]))==1):\n",
    "            twoDistractors[iTwoDistractors,:3] = trial_tones[i_trial,:]\n",
    "            twoDistractors[iTwoDistractors,3] = trial_behaviour[i_trial]\n",
    "            twoDistractors[iTwoDistractors,4] = corrans[i_trial]\n",
    "            iTwoDistractors += 1\n",
    "    \n",
    "    return(allGaussLow[:iAllGaussLow,:], \n",
    "           allGaussHigh[:iAllGaussHigh,:], \n",
    "           distGaussLow[:iDistGaussLow,:],\n",
    "           distGaussHigh[:iDistGaussHigh,:],\n",
    "           distLowOtherLow[:iDistLowOtherLow,:], \n",
    "           distHighOtherHigh[:iDistHighOtherHigh,:],\n",
    "           distLowOtherHigh[:iDistLowOtherHigh,:], \n",
    "           distHighOtherLow[:iDistHighOtherLow,:],\n",
    "           distLowOtherLowHigh[:iDistLowOtherLowHigh,:], \n",
    "           distHighOtherLowHigh[:iDistHighOtherLowHigh,:],\n",
    "           distTwoExtrOtherGauss[:iDistTwoExtrOtherGauss,:], \n",
    "           distTwoExtrSameGauss[:iDistTwoExtrSameGauss,:],\n",
    "           distThreeExtr[:iDistThreeExtr,:], \n",
    "           distThreeExtrTwoSame[:iDistThreeExtrTwoSame,:],\n",
    "           oneDistractor[:iOneDistractor,:],\n",
    "           twoDistractors[:iTwoDistractors,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracies(allGaussLow, allGaussHigh, distGaussLow, distGaussHigh, distLowOtherLow, \n",
    "               distHighOtherHigh, distLowOtherHigh, distHighOtherLow, distLowOtherLowHigh,\n",
    "               distHighOtherLowHigh, distTwoExtrOtherGauss, distTwoExtrSameGauss, \n",
    "               distThreeExtrTwoSame, corrans_expt, trial_behaviour_expt):\n",
    "    \"\"\"\n",
    "    Calculating accuracy table\n",
    "\n",
    "    rows: different categories\n",
    "    columns: different strategies - |ideal subject accuracy|% of trials that can be explained using a given strategy| *\n",
    "    number of strategies\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = np.zeros((10, 9))\n",
    "    distSameOtherSame = np.concatenate((distLowOtherLow,distHighOtherHigh),axis=0)\n",
    "    distractorGaussian = np.concatenate((distGaussLow,distGaussHigh),axis=0)\n",
    "    allGaussian = np.concatenate((allGaussLow,allGaussHigh),axis=0)\n",
    "    biasWithUnclearSignal = np.zeros((1,4))\n",
    "    \n",
    "    \"\"\"\n",
    "    accuracy[0,0] = np.mean(allGaussLow[:,3]==allGaussLow[:,4])\n",
    "    accuracy[0,1] = np.mean(allGaussLow[:,4]==allGaussLow[:,6])\n",
    "    accuracy[0,2] = np.mean(allGaussLow[:,3]==allGaussLow[:,6])\n",
    "    accuracy[0,3] = np.mean(allGaussLow[:,4]==allGaussLow[:,8])\n",
    "    accuracy[0,4] = np.mean(allGaussLow[:,3]==allGaussLow[:,8])\n",
    "    accuracy[0,5] = np.mean(allGaussLow[:,4]==allGaussLow[:,9])\n",
    "    accuracy[0,6] = np.mean(allGaussLow[:,3]==allGaussLow[:,9])\n",
    "    accuracy[0,7] = np.mean(allGaussLow[:,4]==allGaussLow[:,11])\n",
    "    accuracy[0,8] = np.mean(allGaussLow[:,3]==allGaussLow[:,11])\n",
    "    \n",
    "    accuracy[1,0] = np.mean(allGaussHigh[:,3]==allGaussHigh[:,4])\n",
    "    accuracy[1,1] = np.mean(allGaussHigh[:,4]==allGaussHigh[:,6])\n",
    "    accuracy[1,2] = np.mean(allGaussHigh[:,3]==allGaussHigh[:,6])\n",
    "    accuracy[1,3] = np.mean(allGaussHigh[:,4]==allGaussHigh[:,8])\n",
    "    accuracy[1,4] = np.mean(allGaussHigh[:,3]==allGaussHigh[:,8])\n",
    "    accuracy[1,5] = np.mean(allGaussHigh[:,4]==allGaussHigh[:,9])\n",
    "    accuracy[1,6] = np.mean(allGaussHigh[:,3]==allGaussHigh[:,9])\n",
    "    accuracy[1,7] = np.mean(allGaussHigh[:,4]==allGaussHigh[:,11])\n",
    "    accuracy[1,8] = np.mean(allGaussHigh[:,3]==allGaussHigh[:,11])\n",
    "    \"\"\"\n",
    "    accuracy[0,0] = np.mean(allGaussian[:,3]==allGaussian[:,4])\n",
    "    accuracy[0,1] = np.mean(allGaussian[:,4]==allGaussian[:,6])\n",
    "    accuracy[0,2] = np.mean(allGaussian[:,3]==allGaussian[:,6])\n",
    "    accuracy[0,3] = np.mean(allGaussian[:,4]==allGaussian[:,8])\n",
    "    accuracy[0,4] = np.mean(allGaussian[:,3]==allGaussian[:,8])\n",
    "    accuracy[0,5] = np.mean(allGaussian[:,4]==allGaussian[:,9])\n",
    "    accuracy[0,6] = np.mean(allGaussian[:,3]==allGaussian[:,9])\n",
    "    accuracy[0,7] = np.mean(allGaussian[:,4]==allGaussian[:,11])\n",
    "    accuracy[0,8] = np.mean(allGaussian[:,3]==allGaussian[:,11])\n",
    "    \n",
    "    accuracy[2,0] = np.mean(distractorGaussian[:,3]==distractorGaussian[:,4])\n",
    "    accuracy[2,1] = np.mean(distractorGaussian[:,4]==distractorGaussian[:,6])\n",
    "    accuracy[2,2] = np.mean(distractorGaussian[:,3]==distractorGaussian[:,6])\n",
    "    accuracy[2,3] = np.mean(distractorGaussian[:,4]==distractorGaussian[:,8])\n",
    "    accuracy[2,4] = np.mean(distractorGaussian[:,3]==distractorGaussian[:,8])\n",
    "    accuracy[2,5] = np.mean(distractorGaussian[:,4]==distractorGaussian[:,9])\n",
    "    accuracy[2,6] = np.mean(distractorGaussian[:,3]==distractorGaussian[:,9])\n",
    "    accuracy[2,7] = np.mean(distractorGaussian[:,4]==distractorGaussian[:,11])\n",
    "    accuracy[2,8] = np.mean(distractorGaussian[:,3]==distractorGaussian[:,11])\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy[2,0] = np.mean(distGaussLow[:,3]==distGaussLow[:,4])\n",
    "    accuracy[2,1] = np.mean(distGaussLow[:,4]==distGaussLow[:,6])\n",
    "    accuracy[2,2] = np.mean(distGaussLow[:,3]==distGaussLow[:,6])\n",
    "    accuracy[2,3] = np.mean(distGaussLow[:,4]==distGaussLow[:,8])\n",
    "    accuracy[2,4] = np.mean(distGaussLow[:,3]==distGaussLow[:,8])\n",
    "    accuracy[2,5] = np.mean(distGaussLow[:,4]==distGaussLow[:,9])\n",
    "    accuracy[2,6] = np.mean(distGaussLow[:,3]==distGaussLow[:,9])\n",
    "    accuracy[2,7] = np.mean(distGaussLow[:,4]==distGaussLow[:,11])\n",
    "    accuracy[2,8] = np.mean(distGaussLow[:,3]==distGaussLow[:,11])\n",
    "    \n",
    "    accuracy[3,0] = np.mean(distGaussHigh[:,3]==distGaussHigh[:,4])\n",
    "    accuracy[3,1] = np.mean(distGaussHigh[:,4]==distGaussHigh[:,6])\n",
    "    accuracy[3,2] = np.mean(distGaussHigh[:,3]==distGaussHigh[:,6])\n",
    "    accuracy[3,3] = np.mean(distGaussHigh[:,4]==distGaussHigh[:,8])\n",
    "    accuracy[3,4] = np.mean(distGaussHigh[:,3]==distGaussHigh[:,8])\n",
    "    accuracy[3,5] = np.mean(distGaussHigh[:,4]==distGaussHigh[:,9])\n",
    "    accuracy[3,6] = np.mean(distGaussHigh[:,3]==distGaussHigh[:,9])\n",
    "    accuracy[3,7] = np.mean(distGaussHigh[:,4]==distGaussHigh[:,11])\n",
    "    accuracy[3,8] = np.mean(distGaussHigh[:,3]==distGaussHigh[:,11])\n",
    "\n",
    "    accuracy[4,0] = np.mean(distLowOtherLow[:,3]==distLowOtherLow[:,4])\n",
    "    accuracy[4,1] = np.mean(distLowOtherLow[:,4]==distLowOtherLow[:,6])\n",
    "    accuracy[4,2] = np.mean(distLowOtherLow[:,3]==distLowOtherLow[:,6])\n",
    "    accuracy[4,3] = np.mean(distLowOtherLow[:,4]==distLowOtherLow[:,8])\n",
    "    accuracy[4,4] = np.mean(distLowOtherLow[:,3]==distLowOtherLow[:,8])\n",
    "    accuracy[4,5] = np.mean(distLowOtherLow[:,4]==distLowOtherLow[:,9])\n",
    "    accuracy[4,6] = np.mean(distLowOtherLow[:,3]==distLowOtherLow[:,9])\n",
    "    accuracy[4,7] = np.mean(distLowOtherLow[:,4]==distLowOtherLow[:,11])\n",
    "    accuracy[4,8] = np.mean(distLowOtherLow[:,3]==distLowOtherLow[:,11])\n",
    "\n",
    "    accuracy[5,0] = np.mean(distHighOtherHigh[:,3]==distHighOtherHigh[:,4])\n",
    "    accuracy[5,1] = np.mean(distHighOtherHigh[:,4]==distHighOtherHigh[:,6])\n",
    "    accuracy[5,2] = np.mean(distHighOtherHigh[:,3]==distHighOtherHigh[:,6])\n",
    "    accuracy[5,3] = np.mean(distHighOtherHigh[:,4]==distHighOtherHigh[:,8])\n",
    "    accuracy[5,4] = np.mean(distHighOtherHigh[:,3]==distHighOtherHigh[:,8])\n",
    "    accuracy[5,5] = np.mean(distHighOtherHigh[:,4]==distHighOtherHigh[:,9])\n",
    "    accuracy[5,6] = np.mean(distHighOtherHigh[:,3]==distHighOtherHigh[:,9])\n",
    "    accuracy[5,7] = np.mean(distHighOtherHigh[:,4]==distHighOtherHigh[:,11])\n",
    "    accuracy[5,8] = np.mean(distHighOtherHigh[:,3]==distHighOtherHigh[:,11])\n",
    "    \"\"\"\n",
    "    accuracy[3,0] = np.mean(distSameOtherSame[:,3]==distSameOtherSame[:,4])\n",
    "    accuracy[3,1] = np.mean(distSameOtherSame[:,4]==distSameOtherSame[:,6])\n",
    "    accuracy[3,2] = np.mean(distSameOtherSame[:,3]==distSameOtherSame[:,6])\n",
    "    accuracy[3,3] = np.mean(distSameOtherSame[:,4]==distSameOtherSame[:,8])\n",
    "    accuracy[3,4] = np.mean(distSameOtherSame[:,3]==distSameOtherSame[:,8])\n",
    "    accuracy[3,5] = np.mean(distSameOtherSame[:,4]==distSameOtherSame[:,9])\n",
    "    accuracy[3,6] = np.mean(distSameOtherSame[:,3]==distSameOtherSame[:,9])\n",
    "    accuracy[3,7] = np.mean(distSameOtherSame[:,4]==distSameOtherSame[:,11])\n",
    "    accuracy[3,8] = np.mean(distSameOtherSame[:,3]==distSameOtherSame[:,11])\n",
    "    \n",
    "    accuracy[4,0] = np.mean(distLowOtherHigh[:,3]==distLowOtherHigh[:,4])\n",
    "    accuracy[4,1] = np.mean(distLowOtherHigh[:,4]==distLowOtherHigh[:,6])\n",
    "    accuracy[4,2] = np.mean(distLowOtherHigh[:,3]==distLowOtherHigh[:,6])\n",
    "    accuracy[4,3] = np.mean(distLowOtherHigh[:,4]==distLowOtherHigh[:,8])\n",
    "    accuracy[4,4] = np.mean(distLowOtherHigh[:,3]==distLowOtherHigh[:,8])\n",
    "    accuracy[4,5] = np.mean(distLowOtherHigh[:,4]==distLowOtherHigh[:,9])\n",
    "    accuracy[4,6] = np.mean(distLowOtherHigh[:,3]==distLowOtherHigh[:,9])\n",
    "    accuracy[4,7] = np.mean(distLowOtherHigh[:,4]==distLowOtherHigh[:,11])\n",
    "    accuracy[4,8] = np.mean(distLowOtherHigh[:,3]==distLowOtherHigh[:,11])\n",
    "\n",
    "    accuracy[5,0] = np.mean(distHighOtherLow[:,3]==distHighOtherLow[:,4])\n",
    "    accuracy[5,1] = np.mean(distHighOtherLow[:,4]==distHighOtherLow[:,6])\n",
    "    accuracy[5,2] = np.mean(distHighOtherLow[:,3]==distHighOtherLow[:,6])\n",
    "    accuracy[5,3] = np.mean(distHighOtherLow[:,4]==distHighOtherLow[:,8])\n",
    "    accuracy[5,4] = np.mean(distHighOtherLow[:,3]==distHighOtherLow[:,8])\n",
    "    accuracy[5,5] = np.mean(distHighOtherLow[:,4]==distHighOtherLow[:,9])\n",
    "    accuracy[5,6] = np.mean(distHighOtherLow[:,3]==distHighOtherLow[:,9])\n",
    "    accuracy[5,7] = np.mean(distHighOtherLow[:,4]==distHighOtherLow[:,11])\n",
    "    accuracy[5,8] = np.mean(distHighOtherLow[:,3]==distHighOtherLow[:,11])\n",
    "\n",
    "    accuracy[6,0] = np.mean(distTwoExtrOtherGauss[:,3]==distTwoExtrOtherGauss[:,4])\n",
    "    accuracy[6,1] = np.mean(distTwoExtrOtherGauss[:,4]==distTwoExtrOtherGauss[:,6])\n",
    "    accuracy[6,2] = np.mean(distTwoExtrOtherGauss[:,3]==distTwoExtrOtherGauss[:,6])\n",
    "    accuracy[6,3] = np.mean(distTwoExtrOtherGauss[:,4]==distTwoExtrOtherGauss[:,8])\n",
    "    accuracy[6,4] = np.mean(distTwoExtrOtherGauss[:,3]==distTwoExtrOtherGauss[:,8])\n",
    "    accuracy[6,5] = np.mean(distTwoExtrOtherGauss[:,4]==distTwoExtrOtherGauss[:,9])\n",
    "    accuracy[6,6] = np.mean(distTwoExtrOtherGauss[:,3]==distTwoExtrOtherGauss[:,9])\n",
    "    accuracy[6,7] = np.mean(distTwoExtrOtherGauss[:,4]==distTwoExtrOtherGauss[:,11])\n",
    "    accuracy[6,8] = np.mean(distTwoExtrOtherGauss[:,3]==distTwoExtrOtherGauss[:,11])\n",
    "\n",
    "    accuracy[7,0] = np.mean(distTwoExtrSameGauss[:,3]==distTwoExtrSameGauss[:,4])\n",
    "    accuracy[7,1] = np.mean(distTwoExtrSameGauss[:,4]==distTwoExtrSameGauss[:,6])\n",
    "    accuracy[7,2] = np.mean(distTwoExtrSameGauss[:,3]==distTwoExtrSameGauss[:,6])\n",
    "    accuracy[7,3] = np.mean(distTwoExtrSameGauss[:,4]==distTwoExtrSameGauss[:,8])\n",
    "    accuracy[7,4] = np.mean(distTwoExtrSameGauss[:,3]==distTwoExtrSameGauss[:,8])\n",
    "    accuracy[7,5] = np.mean(distTwoExtrSameGauss[:,4]==distTwoExtrSameGauss[:,9])\n",
    "    accuracy[7,6] = np.mean(distTwoExtrSameGauss[:,3]==distTwoExtrSameGauss[:,9])\n",
    "    accuracy[7,7] = np.mean(distTwoExtrSameGauss[:,4]==distTwoExtrSameGauss[:,11])\n",
    "    accuracy[7,8] = np.mean(distTwoExtrSameGauss[:,3]==distTwoExtrSameGauss[:,11])\n",
    "    \"\"\"\n",
    "    accuracy[8,0] = np.mean(distThreeExtr[:,3]==distThreeExtr[:,4])\n",
    "    accuracy[8,1] = np.mean(distThreeExtr[:,4]==distThreeExtr[:,6])\n",
    "    accuracy[8,2] = np.mean(distThreeExtr[:,3]==distThreeExtr[:,6])\n",
    "    accuracy[8,3] = np.mean(distThreeExtr[:,4]==distThreeExtr[:,8])\n",
    "    accuracy[8,4] = np.mean(distThreeExtr[:,3]==distThreeExtr[:,8])\n",
    "    accuracy[8,5] = np.mean(distThreeExtr[:,4]==distThreeExtr[:,9])\n",
    "    accuracy[8,6] = np.mean(distThreeExtr[:,3]==distThreeExtr[:,9])\n",
    "    accuracy[8,7] = np.mean(distThreeExtr[:,4]==distThreeExtr[:,11])\n",
    "    accuracy[8,8] = np.mean(distThreeExtr[:,3]==distThreeExtr[:,11])\n",
    "    \"\"\"\n",
    "    accuracy[9,0] = np.mean(distThreeExtrTwoSame[:,3]==distThreeExtrTwoSame[:,4])\n",
    "    accuracy[9,1] = np.mean(distThreeExtrTwoSame[:,4]==distThreeExtrTwoSame[:,6])\n",
    "    accuracy[9,2] = np.mean(distThreeExtrTwoSame[:,3]==distThreeExtrTwoSame[:,6])\n",
    "    accuracy[9,3] = np.mean(distThreeExtrTwoSame[:,4]==distThreeExtrTwoSame[:,8])\n",
    "    accuracy[9,4] = np.mean(distThreeExtrTwoSame[:,3]==distThreeExtrTwoSame[:,8])\n",
    "    accuracy[9,5] = np.mean(distThreeExtrTwoSame[:,4]==distThreeExtrTwoSame[:,9])\n",
    "    accuracy[9,6] = np.mean(distThreeExtrTwoSame[:,3]==distThreeExtrTwoSame[:,9])\n",
    "    accuracy[9,7] = np.mean(distThreeExtrTwoSame[:,4]==distThreeExtrTwoSame[:,11])\n",
    "    accuracy[9,8] = np.mean(distThreeExtrTwoSame[:,3]==distThreeExtrTwoSame[:,11])\n",
    "    \n",
    "    biasWithUnclearSignal[:,0] = np.mean(distLowOtherLowHigh[:,3]==0)\n",
    "    biasWithUnclearSignal[:,1] = np.mean(distLowOtherLowHigh[:,3]==1)\n",
    "    biasWithUnclearSignal[:,2] = np.mean(distHighOtherLowHigh[:,3]==0)\n",
    "    biasWithUnclearSignal[:,3] = np.mean(distHighOtherLowHigh[:,3]==1)\n",
    "    \n",
    "    print(\"Total accuracy of the subject: \")\n",
    "    print(np.sum(corrans_expt==trial_behaviour_expt)/len(trial_behaviour_expt))\n",
    "\n",
    "    print(\"Overall accuracy of categories: \")\n",
    "    print(accuracy[:,0])\n",
    "\n",
    "    print(\"optimal subject accuracies: \")\n",
    "    print(accuracy[:,[1,3,5,7]])\n",
    "\n",
    "    print(\"what percentage of the data can be explained by the optimal subject: \")\n",
    "    print(accuracy[:,[2,4,6,8]])   \n",
    "    \n",
    "    return accuracy*100, biasWithUnclearSignal*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5ea1f4fa7a70090fd0715b34_categorization_task_2021-03-01_18h06.17.396.csv');\n",
    "\n",
    "trial_tones_5b34, trial_behaviour_5b34, trial_tone_cat_5b34, corrans_5b34 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5fad0bb914cb0035f917d619_categorization_task_2021-03-01_17h13.10.896.csv');\n",
    "\n",
    "trial_tones_d619, trial_behaviour_d619, trial_tone_cat_d619, corrans_d619 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5cc86a4e264a60000124c2e0_categorization_task_2021-03-01_18h07.47.350.csv');\n",
    "\n",
    "trial_tones_c2e0, trial_behaviour_c2e0, trial_tone_cat_c2e0, corrans_c2e0 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5f3990fea9ef865370ac735a_categorization_task_2021-03-01_17h41.13.928.csv');\n",
    "\n",
    "trial_tones_735a, trial_behaviour_735a, trial_tone_cat_735a, corrans_735a = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5f3801b18c88962be7831304_categorization_task_2021-03-01_19h52.16.006.csv');\n",
    "\n",
    "trial_tones_1304, trial_behaviour_1304, trial_tone_cat_1304, corrans_1304 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/60143ceed282ae3ceda28de3_categorization_task_2021-03-01_18h53.15.963.csv');\n",
    "\n",
    "trial_tones_8de3, trial_behaviour_8de3, trial_tone_cat_8de3, corrans_8de3 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5ec48aac33b93f000ab96b7f_categorization_task_2021-03-01_20h56.07.985.csv');\n",
    "\n",
    "trial_tones_6b7f, trial_behaviour_6b7f, trial_tone_cat_6b7f, corrans_6b7f = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5ec3a28f46bca2016fc91c3f_categorization_task_2021-03-01_21h03.59.423.csv');\n",
    "\n",
    "trial_tones_1c3f, trial_behaviour_1c3f, trial_tone_cat_1c3f, corrans_1c3f = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5ec3a1fe1bfa1720b3731604_categorization_task_2021-03-01_21h58.03.550.csv');\n",
    "\n",
    "trial_tones_1604, trial_behaviour_1604, trial_tone_cat_1604, corrans_1604 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5f14e794a89ac35a3f89801d_categorization_task_2021-02-19_23h35.01.131.csv');\n",
    "\n",
    "trial_tones_801d, trial_behaviour_801d, trial_tone_cat_801d, corrans_801d = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5f4b1ba295b8186ec4901396_categorization_task_2021-02-19_18h32.43.841.csv');\n",
    "\n",
    "trial_tones_1396, trial_behaviour_1396, trial_tone_cat_1396, corrans_1396 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/5f567ed454a3b50b805354db_categorization_task_2021-02-19_22h29.09.897.csv');\n",
    "\n",
    "trial_tones_54db, trial_behaviour_54db, trial_tone_cat_54db, corrans_54db = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-03-03_04h34.17_b73c4368-7bd9-11eb-a333-ac1f6b405aea/VIOLINIST_categorization_task_2021-03-02_18h00.06.158.csv');\n",
    "\n",
    "trial_tones_viol, trial_behaviour_viol, trial_tone_cat_viol, corrans_viol = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-04-18_21h32.52_a165fbe6-a08d-11eb-a749-ac1f6b405aea/603ecb8767d4bc041eedb4c7_categorization_task_2021-04-18_12h12.29.327.csv');\n",
    "\n",
    "trial_tones_b4c7, trial_behaviour_b4c7, trial_tone_cat_b4c7, corrans_b4c7 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-04-19_13h07.33_346fb076-a110-11eb-a757-ac1f6b405aea/a12e_categorization_task_2021-04-19_11h01.01.358.csv');\n",
    "\n",
    "trial_tones_a12e, trial_behaviour_a12e, trial_tone_cat_a12e, corrans_a12e = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-16_23h33.50_2b5881da-b69f-11eb-a977-ac1f6b405aea/6060687bbc223f086e4fc653_categorization_task_2021-05-16_21h19.38.241.csv');\n",
    "\n",
    "trial_tones_c653, trial_behaviour_c653, trial_tone_cat_c653, corrans_c653 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-16_23h33.50_2b5881da-b69f-11eb-a977-ac1f6b405aea/607950e0efb144952e6f4b7f_categorization_task_2021-05-16_11h00.32.119.csv');\n",
    "\n",
    "trial_tones_4b7f, trial_behaviour_4b7f, trial_tone_cat_4b7f, corrans_4b7f = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-16_23h33.50_2b5881da-b69f-11eb-a977-ac1f6b405aea/609feb16aafd2f71e730fc3a_categorization_task_2021-05-16_15h37.20.773.csv');\n",
    "\n",
    "trial_tones_fc3a, trial_behaviour_fc3a, trial_tone_cat_fc3a, corrans_fc3a = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/601306071a72651244570d04_categorization_task_2021-05-18_04h46.56.555.csv');\n",
    "\n",
    "trial_tones_0d04, trial_behaviour_0d04, trial_tone_cat_0d04, corrans_0d04 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/609dd2bc3360f6e164901570_categorization_task_2021-05-17_22h53.33.874.csv');\n",
    "\n",
    "trial_tones_1570, trial_behaviour_1570, trial_tone_cat_1570, corrans_1570 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5e830b929f2ed2018ec42099_categorization_task_2021-05-17_22h01.23.634.csv');\n",
    "\n",
    "trial_tones_2099, trial_behaviour_2099, trial_tone_cat_2099, corrans_2099 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f627674b6f93f19ff74592d_categorization_task_2021-05-17_22h05.20.424.csv');\n",
    "\n",
    "trial_tones_592d, trial_behaviour_592d, trial_tone_cat_592d, corrans_592d = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/606030fbea6f7e84bc7aa45d_categorization_task_2021-05-17_20h04.48.021.csv');\n",
    "\n",
    "trial_tones_a45d, trial_behaviour_a45d, trial_tone_cat_a45d, corrans_a45d = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f3cb7486048540009840cf1_categorization_task_2021-05-18_05h15.37.577.csv');\n",
    "\n",
    "trial_tones_0cf1, trial_behaviour_0cf1, trial_tone_cat_0cf1, corrans_0cf1 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/6062c088821c76a49374e453_categorization_task_2021-05-17_23h24.18.467.csv');\n",
    "\n",
    "trial_tones_e453, trial_behaviour_e453, trial_tone_cat_e453, corrans_e453 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/60660870280f2b522cc3214b_categorization_task_2021-05-18_09h04.16.970.csv');\n",
    "\n",
    "trial_tones_214b, trial_behaviour_214b, trial_tone_cat_214b, corrans_214b = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f3716c55fa34618ff41e045_categorization_task_2021-05-18_10h25.52.803.csv');\n",
    "\n",
    "trial_tones_e045, trial_behaviour_e045, trial_tone_cat_e045, corrans_e045 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f2a12f5837dd81e5881b7cc_categorization_task_2021-05-18_16h19.05.932.csv');\n",
    "\n",
    "trial_tones_b7cc, trial_behaviour_b7cc, trial_tone_cat_b7cc, corrans_b7cc = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f591c6d75be841230aa6ced_categorization_task_2021-05-19_00h36.07.372.csv');\n",
    "\n",
    "trial_tones_6ced, trial_behaviour_6ced, trial_tone_cat_6ced, corrans_6ced = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-19_20h09.38_23b63e9a-b8de-11eb-a9b5-ac1f6b405aea/5f4032479b2b92152488188a_categorization_task_2021-05-19_16h44.41.870.csv');\n",
    "\n",
    "trial_tones_188a, trial_behaviour_188a, trial_tone_cat_188a, corrans_188a = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-20_20h09.05_3a409060-b9a7-11eb-a9da-ac1f6b405aea/5e485d1d4edf63000d0b1620_categorization_task_2021-05-19_19h19.38.331.csv');\n",
    "\n",
    "trial_tones_1620, trial_behaviour_1620, trial_tone_cat_1620, corrans_1620 = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-20_20h09.05_3a409060-b9a7-11eb-a9da-ac1f6b405aea/60a3c37fb7414cf62b79e21e_categorization_task_2021-05-20_14h16.08.213.csv');\n",
    "\n",
    "trial_tones_e12e, trial_behaviour_e12e, trial_tone_cat_e12e, corrans_e12e = analysis2(csv_test,csv_data)\n",
    "\n",
    "csv_test = pd.read_csv('../auditory_categorization_prolific/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2021-05-20_20h09.05_3a409060-b9a7-11eb-a9da-ac1f6b405aea/5e7235e6d302e91679226b0e_categorization_task_2021-05-20_16h24.10.294.csv');\n",
    "\n",
    "trial_tones_6b0e, trial_behaviour_6b0e, trial_tone_cat_6b0e, corrans_6b0e = analysis2(csv_test,csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5fad0bb914cb0035f917d619_categorization_task_longLow_2021-03-29_23h07.31.656.csv');\n",
    "\n",
    "trial_tones_d619Lc, trial_behaviour_d619Lc, trial_tone_cat_d619Lc, corrans_d619Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5cc86a4e264a60000124c2e0_categorization_task_longLow_2021-04-01_10h42.12.417.csv');\n",
    "\n",
    "trial_tones_c2e0Lc, trial_behaviour_c2e0Lc, trial_tone_cat_c2e0Lc, corrans_c2e0Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-05-13_18h12.29_c75655f2-b416-11eb-a951-ac1f6b405aea/5f3990fea9ef865370ac735a_categorization_task_longLow_2021-04-19_17h09.03.462.csv');\n",
    "\n",
    "trial_tones_735aLc, trial_behaviour_735aLc, trial_tone_cat_735aLc, corrans_735aLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-05-15_18h46.10_d0fb81d4-b5ad-11eb-a96f-ac1f6b405aea/5f3801b18c88962be7831304_categorization_task_longLow_2021-05-15_19h10.15.555.csv');\n",
    "\n",
    "trial_tones_1304Lc, trial_behaviour_1304Lc, trial_tone_cat_1304Lc, corrans_1304Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5ec48aac33b93f000ab96b7f_categorization_task_longLow_2021-03-30_00h40.03.108.csv');\n",
    "\n",
    "trial_tones_6b7fLc, trial_behaviour_6b7fLc, trial_tone_cat_6b7fLc, corrans_6b7fLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5ec3a28f46bca2016fc91c3f_categorization_task_longLow_2021-03-30_01h15.32.575.csv');\n",
    "\n",
    "trial_tones_1c3fLc, trial_behaviour_1c3fLc, trial_tone_cat_1c3fLc, corrans_1c3fLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5ec3a1fe1bfa1720b3731604_categorization_task_longLow_2021-03-30_00h40.53.029.csv');\n",
    "\n",
    "trial_tones_1604Lc, trial_behaviour_1604Lc, trial_tone_cat_1604Lc, corrans_1604Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f14e794a89ac35a3f89801d_categorization_task_longLow_2021-05-17_15h46.25.182.csv');\n",
    "\n",
    "trial_tones_801dLc, trial_behaviour_801dLc, trial_tone_cat_801dLc, corrans_801dLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5f4b1ba295b8186ec4901396_categorization_task_longLow_2021-03-31_09h58.35.793.csv');\n",
    "\n",
    "trial_tones_1396Lc, trial_behaviour_1396Lc, trial_tone_cat_1396Lc, corrans_1396Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/603ecb8767d4bc041eedb4c7_categorization_task_longLow_2021-03-29_23h42.07.895.csv');\n",
    "\n",
    "trial_tones_b4c7Lc, trial_behaviour_b4c7Lc, trial_tone_cat_b4c7Lc, corrans_b4c7Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-04-01_21h21.21_349e7548-9330-11eb-a617-ac1f6b405aea/5f809cdbe0804f0ec6daa12e_categorization_task_longLow_2021-03-30_10h54.28.163.csv');\n",
    "\n",
    "trial_tones_a12eLc, trial_behaviour_a12eLc, trial_tone_cat_a12eLc, corrans_a12eLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-05-15_18h46.10_d0fb81d4-b5ad-11eb-a96f-ac1f6b405aea/6060687bbc223f086e4fc653_categorization_task_longLow_2021-05-15_20h38.22.974.csv');\n",
    "\n",
    "trial_tones_c653Lc, trial_behaviour_c653Lc, trial_tone_cat_c653Lc, corrans_c653Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-05-15_18h46.10_d0fb81d4-b5ad-11eb-a96f-ac1f6b405aea/607950e0efb144952e6f4b7f_categorization_task_longLow_2021-05-15_19h42.31.771.csv');\n",
    "\n",
    "trial_tones_4b7fLc, trial_behaviour_4b7fLc, trial_tone_cat_4b7fLc, corrans_4b7fLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-05-15_18h46.10_d0fb81d4-b5ad-11eb-a96f-ac1f6b405aea/609feb16aafd2f71e730fc3a_categorization_task_longLow_2021-05-15_20h37.38.994.csv');\n",
    "\n",
    "trial_tones_fc3aLc, trial_behaviour_fc3aLc, trial_tone_cat_fc3aLc, corrans_fc3aLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/601306071a72651244570d04_categorization_task_longLow_2021-06-09_03h41.33.989.csv');\n",
    "\n",
    "trial_tones_0d04Lc, trial_behaviour_0d04Lc, trial_tone_cat_0d04Lc, corrans_0d04Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-11_19h04.17_d1d63a32-cae7-11eb-abb1-ac1f6b405aea/609dd2bc3360f6e164901570_categorization_task_longLow_2021-06-11_13h42.28.848.csv');\n",
    "\n",
    "trial_tones_1570Lc, trial_behaviour_1570Lc, trial_tone_cat_1570Lc, corrans_1570Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5e830b929f2ed2018ec42099_categorization_task_longLow_2021-06-08_14h09.42.717.csv');\n",
    "\n",
    "trial_tones_2099Lc, trial_behaviour_2099Lc, trial_tone_cat_2099Lc, corrans_2099Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f627674b6f93f19ff74592d_categorization_task_longLow_2021-06-08_12h00.15.358.csv');\n",
    "\n",
    "trial_tones_592dLc, trial_behaviour_592dLc, trial_tone_cat_592dLc, corrans_592dLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/606030fbea6f7e84bc7aa45d_categorization_task_longLow_2021-06-08_13h36.49.749.csv');\n",
    "\n",
    "trial_tones_a45dLc, trial_behaviour_a45dLc, trial_tone_cat_a45dLc, corrans_a45dLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f3cb7486048540009840cf1_categorization_task_longLow_2021-06-08_21h40.18.046.csv');\n",
    "\n",
    "trial_tones_0cf1Lc, trial_behaviour_0cf1Lc, trial_tone_cat_0cf1Lc, corrans_0cf1Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/6062c088821c76a49374e453_categorization_task_longLow_2021-06-08_17h23.26.803.csv');\n",
    "\n",
    "trial_tones_e453Lc, trial_behaviour_e453Lc, trial_tone_cat_e453Lc, corrans_e453Lc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/60660870280f2b522cc3214b_categorization_task_longLow_2021-06-08_10h56.05.372.csv');\n",
    "\n",
    "trial_tones_214bLc, trial_behaviour_214bLc, trial_tone_cat_214bLc, corrans_214bLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f2a12f5837dd81e5881b7cc_categorization_task_longLow_2021-06-08_18h38.21.637.csv');\n",
    "\n",
    "trial_tones_b7ccLc, trial_behaviour_b7ccLc, trial_tone_cat_b7ccLc, corrans_b7ccLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f591c6d75be841230aa6ced_categorization_task_longLow_2021-06-08_17h43.28.458.csv');\n",
    "\n",
    "trial_tones_6cedLc, trial_behaviour_6cedLc, trial_tone_cat_6cedLc, corrans_6cedLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/5f4032479b2b92152488188a_categorization_task_longLow_2021-06-08_18h13.56.450.csv');\n",
    "\n",
    "trial_tones_188aLc, trial_behaviour_188aLc, trial_tone_cat_188aLc, corrans_188aLc = analysis2(csv_testLc,csv_dataLc)\n",
    "\n",
    "csv_testLc = pd.read_csv('../auditory_categorization_longLow/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataLc = pd.read_csv('auditory_categorization_lc_online_data/auditory_categorization_v2_119865_2021-06-10_00h51.07_f0ebe880-c985-11eb-ab99-ac1f6b405aea/60a3c37fb7414cf62b79e21e_categorization_task_longLow_2021-06-08_20h10.04.207.csv');\n",
    "\n",
    "trial_tones_e12eLc, trial_behaviour_e12eLc, trial_tone_cat_e12eLc, corrans_e12eLc = analysis2(csv_testLc,csv_dataLc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5f3990fea9ef865370ac735a_categorization_task_longHigh_2021-06-18_15h49.59.982.csv');\n",
    "\n",
    "trial_tones_735aHc, trial_behaviour_735aHc, trial_tone_cat_735aHc, corrans_735aHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5f3801b18c88962be7831304_categorization_task_longHigh_2021-06-17_14h28.57.096.csv');\n",
    "\n",
    "trial_tones_1304Hc, trial_behaviour_1304Hc, trial_tone_cat_1304Hc, corrans_1304Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_21h01.46_8ea3ab86-d141-11eb-abf1-ac1f6b405aea/5ec48aac33b93f000ab96b7f_categorization_task_longHigh_2021-06-19_21h42.04.614.csv');\n",
    "\n",
    "trial_tones_6b7fHc, trial_behaviour_6b7fHc, trial_tone_cat_6b7fHc, corrans_6b7fHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5ec3a28f46bca2016fc91c3f_categorization_task_longHigh_2021-06-16_10h08.17.055.csv');\n",
    "\n",
    "trial_tones_1c3fHc, trial_behaviour_1c3fHc, trial_tone_cat_1c3fHc, corrans_1c3fHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5ec3a1fe1bfa1720b3731604_categorization_task_longHigh_2021-06-17_12h28.40.530.csv');\n",
    "\n",
    "trial_tones_1604Hc, trial_behaviour_1604Hc, trial_tone_cat_1604Hc, corrans_1604Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-20_21h31.41_e74189a2-d20e-11eb-abf6-ac1f6b405aea/5f4b1ba295b8186ec4901396_categorization_task_longHigh_2021-06-20_14h09.48.106.csv');\n",
    "\n",
    "trial_tones_1396Hc, trial_behaviour_1396Hc, trial_tone_cat_1396Hc, corrans_1396Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-20_21h31.41_e74189a2-d20e-11eb-abf6-ac1f6b405aea/603ecb8767d4bc041eedb4c7_categorization_task_longHigh_2021-06-20_21h45.30.847.csv');\n",
    "\n",
    "trial_tones_b4c7Hc, trial_behaviour_b4c7Hc, trial_tone_cat_b4c7Hc, corrans_b4c7Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5f809cdbe0804f0ec6daa12e_categorization_task_longHigh_2021-06-16_09h31.02.317.csv');\n",
    "\n",
    "trial_tones_a12eHc, trial_behaviour_a12eHc, trial_tone_cat_a12eHc, corrans_a12eHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/6060687bbc223f086e4fc653_categorization_task_longHigh_2021-06-16_15h06.31.093.csv');\n",
    "\n",
    "trial_tones_c653Hc, trial_behaviour_c653Hc, trial_tone_cat_c653Hc, corrans_c653Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/607950e0efb144952e6f4b7f_categorization_task_longHigh_2021-06-19_11h32.49.411.csv')\n",
    "trial_tones_4b7fHc, trial_behaviour_4b7fHc, trial_tone_cat_4b7fHc, corrans_4b7fHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/609feb16aafd2f71e730fc3a_categorization_task_longHigh_2021-06-16_09h05.25.383.csv');\n",
    "\n",
    "trial_tones_fc3aHc, trial_behaviour_fc3aHc, trial_tone_cat_fc3aHc, corrans_fc3aHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/601306071a72651244570d04_categorization_task_longHigh_2021-06-16_23h31.15.482.csv');\n",
    "\n",
    "trial_tones_0d04Hc, trial_behaviour_0d04Hc, trial_tone_cat_0d04Hc, corrans_0d04Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5e830b929f2ed2018ec42099_categorization_task_longHigh_2021-06-17_22h42.21.250.csv');\n",
    "\n",
    "trial_tones_2099Hc, trial_behaviour_2099Hc, trial_tone_cat_2099Hc, corrans_2099Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/5f627674b6f93f19ff74592d_categorization_task_longHigh_2021-06-16_16h30.18.835.csv');\n",
    "\n",
    "trial_tones_592dHc, trial_behaviour_592dHc, trial_tone_cat_592dHc, corrans_592dHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/606030fbea6f7e84bc7aa45d_categorization_task_longHigh_2021-06-16_05h39.27.990.csv');\n",
    "\n",
    "trial_tones_a45dHc, trial_behaviour_a45dHc, trial_tone_cat_a45dHc, corrans_a45dHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/6062c088821c76a49374e453_categorization_task_longHigh_2021-06-16_00h53.08.975.csv');\n",
    "\n",
    "trial_tones_e453Hc, trial_behaviour_e453Hc, trial_tone_cat_e453Hc, corrans_e453Hc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-06-19_15h40.10_a17ef1de-d114-11eb-abf0-ac1f6b405aea/60660870280f2b522cc3214b_categorization_task_longHigh_2021-06-16_14h49.32.751.csv');\n",
    "\n",
    "trial_tones_214bHc, trial_behaviour_214bHc, trial_tone_cat_214bHc, corrans_214bHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-09-13_16h55.31_16f797ec-880d-4ffe-8c64-e679fe6e5c9b/5f2a12f5837dd81e5881b7cc_categorization_task_longHigh_2021-06-16_00h09.32.674.csv');\n",
    "\n",
    "trial_tones_b7ccHc, trial_behaviour_b7ccHc, trial_tone_cat_b7ccHc, corrans_b7ccHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "DataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-09-13_16h55.31_16f797ec-880d-4ffe-8c64-e679fe6e5c9b/5f591c6d75be841230aa6ced_categorization_task_longHigh_2021-06-22_04h11.02.407.csv');                       \n",
    "trial_tones_6cedHc, trial_behaviour_6cedHc, trial_tone_cat_6cedHc, corrans_6cedHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-09-13_16h55.31_16f797ec-880d-4ffe-8c64-e679fe6e5c9b/5f4032479b2b92152488188a_categorization_task_longHigh_2021-06-16_06h34.48.233.csv');\n",
    "\n",
    "trial_tones_188aHc, trial_behaviour_188aHc, trial_tone_cat_188aHc, corrans_188aHc = analysis2(csv_testHc,csv_dataHc)\n",
    "\n",
    "csv_testHc = pd.read_csv('../auditory_categorization_longHigh/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_dataHc = pd.read_csv('auditory_categorization_Hc_online_data/auditory_categorization_v3_143976_2021-09-13_16h55.31_16f797ec-880d-4ffe-8c64-e679fe6e5c9b/60a3c37fb7414cf62b79e21e_categorization_task_longHigh_2021-06-16_22h13.55.728.csv');\n",
    "\n",
    "trial_tones_e12eHc, trial_behaviour_e12eHc, trial_tone_cat_e12eHc, corrans_e12eHc = analysis2(csv_testHc,csv_dataHc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_freq_percept = np.arange(0.6,4.7,0.1) # array of possible perceptual tones\n",
    "    \n",
    "def plotting(allGaussian, distractorGaussian, distLowOtherHigh, distHighOtherLow):\n",
    "        \n",
    "    \"\"\"\n",
    "    1d plots\n",
    "    \"\"\"\n",
    "    \n",
    "    allMeanTone_behvSubjArray = np.zeros((4,len(logFreqBins)))\n",
    "    GaussMeanTone_behvSubjArray = np.zeros((4,len(logFreqBins)))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.xlabel('Mean of log10(freq) of all tones')\n",
    "    plt.ylabel('p(B_H|T)')\n",
    "    cnt=0\n",
    "    for iCategory in [allGaussian, distractorGaussian, distLowOtherHigh, distHighOtherLow]:     \n",
    "        allMeanLogBins = np.zeros((len(iCategory),))\n",
    "        allMeanTone_behaviourOptimal = np.zeros((len(logFreqBins),))\n",
    "        allMeanTone_behaviourSubj = np.zeros((len(logFreqBins),))    \n",
    "\n",
    "        for iTrial in range(len(iCategory)):\n",
    "            allMeanLogBins[iTrial] = logFreqBins[np.where(logFreqBins < iCategory[iTrial,5])[0][-1]]      \n",
    "        for i_tone in range(len(logFreqBins)):\n",
    "            allMeanTone_behaviourOptimal[i_tone] = np.mean(iCategory[allMeanLogBins==logFreqBins[i_tone],6])\n",
    "            allMeanTone_behaviourSubj[i_tone] = np.mean(iCategory[allMeanLogBins==logFreqBins[i_tone],3])     \n",
    "        #plt.plot(logFreqBins, allMeanTone_behaviourOptimal)\n",
    "        plt.plot(logFreqBins, allMeanTone_behaviourSubj)  \n",
    "        allMeanTone_behvSubjArray[cnt,:] = allMeanTone_behaviourSubj\n",
    "        cnt += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Mean of log10(freq) of Tones from Gaussians)')\n",
    "    plt.ylabel('p(B_H|T)')\n",
    "    cnt=0\n",
    "    for iCategory in [allGaussian, distractorGaussian, distLowOtherHigh, distHighOtherLow]:        \n",
    "        GaussMeanLogBins = np.zeros((len(iCategory),))\n",
    "        GaussMeanTone_behaviourOptimal = np.zeros((len(logFreqBins),))\n",
    "        GaussMeanTone_behaviourSubj = np.zeros((len(logFreqBins),))    \n",
    "\n",
    "        for iTrial in range(len(iCategory)):\n",
    "            GaussMeanLogBins[iTrial] = logFreqBins[np.where(logFreqBins < iCategory[iTrial,7])[0][-1]]   \n",
    "        for i_tone in range(len(logFreqBins)):\n",
    "            GaussMeanTone_behaviourOptimal[i_tone] = np.mean(iCategory[GaussMeanLogBins==logFreqBins[i_tone],8])\n",
    "            GaussMeanTone_behaviourSubj[i_tone] = np.mean(iCategory[GaussMeanLogBins==logFreqBins[i_tone],3])     \n",
    "        #plt.plot(logFreqBins, allMeanTone_behaviourOptimal)\n",
    "        plt.plot(logFreqBins, GaussMeanTone_behaviourSubj)   \n",
    "        GaussMeanTone_behvSubjArray[cnt,:] = GaussMeanTone_behaviourSubj\n",
    "        cnt += 1\n",
    "     \n",
    "    return allMeanTone_behvSubjArray, GaussMeanTone_behvSubjArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ideal subject behaviour under all three strategies\n",
    "\"\"\"\n",
    "def idealSubjectBehaviour(tonesPresented):\n",
    "    idealMeanBehaviour = (np.mean(tonesPresented,axis=1)>expt_freq_seq_mid).astype(int) \n",
    "    idealVotingBehaviour = (np.sum(tonesPresented>expt_freq_seq_mid,axis=1)>1).astype(int)\n",
    "    idealSignalMeanBehaviour = np.copy(idealVotingBehaviour)\n",
    "    for ibehv in range(len(idealSignalMeanBehaviour)):\n",
    "        if (sum(tonesPresented[ibehv]>high_dist[0]+2*high_dist[1])\n",
    "              + sum(tonesPresented[ibehv]<low_dist[0]-2*low_dist[1])==3):\n",
    "            idealSignalMeanBehaviour[ibehv] = np.random.choice([0,1])\n",
    "        elif (sum(tonesPresented[ibehv]>expt_freq_seq_mid)==2 \n",
    "              and sum(tonesPresented[ibehv]<expt_freq_seq_mid)==1 \n",
    "              and sum(tonesPresented[ibehv]>(high_dist[0]+2*high_dist[1]))==1\n",
    "              and sum(tonesPresented[ibehv]<(low_dist[0]-2*low_dist[1]))==0):\n",
    "            idealSignalMeanBehaviour[ibehv] = np.random.choice([0,1])\n",
    "        elif (sum(tonesPresented[ibehv]>expt_freq_seq_mid)==1\n",
    "              and sum(tonesPresented[ibehv]<expt_freq_seq_mid)==2 \n",
    "              and sum(tonesPresented[ibehv]>(high_dist[0]+2*high_dist[1]))==0\n",
    "              and sum(tonesPresented[ibehv]<(low_dist[0]-2*low_dist[1]))==1):\n",
    "            idealSignalMeanBehaviour[ibehv] = np.random.choice([0,1]) \n",
    "        elif (sum(tonesPresented[ibehv]>(high_dist[0]+2*high_dist[1]))==0 \n",
    "              and sum(tonesPresented[ibehv]>expt_freq_seq_mid)==1\n",
    "              and sum(tonesPresented[ibehv]<(low_dist[0]-2*low_dist[1]))==2):\n",
    "            idealSignalMeanBehaviour[ibehv] = 1 \n",
    "        elif (sum(tonesPresented[ibehv]>(high_dist[0]+2*high_dist[1]))==2 \n",
    "              and sum(tonesPresented[ibehv]<expt_freq_seq_mid)==1\n",
    "              and sum(tonesPresented[ibehv]<(low_dist[0]-2*low_dist[1]))==0):  \n",
    "            idealSignalMeanBehaviour[ibehv] = 0\n",
    "    return idealMeanBehaviour, idealSignalMeanBehaviour, idealVotingBehaviour\n",
    "\n",
    "\n",
    "def compareBehaviourWithDistractors(tonesPresented,subjectBehaviour,idealMeanBehaviour,\n",
    "                                    idealSignalMeanBehaviour, idealVotingBehaviour):\n",
    "    \n",
    "    lowTrialsWithDistractors = np.array([],dtype=int)\n",
    "    highTrialsWithDistractors = np.array([],dtype=int)\n",
    "    for i_trial in range(len(tonesPresented)):\n",
    "        if (sum(tonesPresented[i_trial]>(high_dist[0]+2*high_dist[1]))==0 \n",
    "          and sum(tonesPresented[i_trial]>expt_freq_seq_mid)==1\n",
    "          and sum(tonesPresented[i_trial]<(low_dist[0]-2*low_dist[1]))==1):\n",
    "            highTrialsWithDistractors = np.append(highTrialsWithDistractors,i_trial)\n",
    "        if (sum(tonesPresented[i_trial]>(high_dist[0]+2*high_dist[1]))==1 \n",
    "          and sum(tonesPresented[i_trial]<expt_freq_seq_mid)==1\n",
    "          and sum(tonesPresented[i_trial]<(low_dist[0]-2*low_dist[1]))==0):  \n",
    "            lowTrialsWithDistractors = np.append(lowTrialsWithDistractors,i_trial)\n",
    "    trialsWithDistractors = np.union1d(lowTrialsWithDistractors, highTrialsWithDistractors)\n",
    "    accuracyMeanBehaviour = np.mean(subjectBehaviour[trialsWithDistractors]==\n",
    "                                   idealMeanBehaviour[trialsWithDistractors])\n",
    "    accuracySignalMeanBehaviuor = np.mean(subjectBehaviour[trialsWithDistractors]==\n",
    "                                         idealSignalMeanBehaviour[trialsWithDistractors])\n",
    "    accuracyVotingBehaviour = np.mean(subjectBehaviour[trialsWithDistractors]==\n",
    "                                     idealVotingBehaviour[trialsWithDistractors])\n",
    "    return accuracyMeanBehaviour, accuracySignalMeanBehaviuor, accuracyVotingBehaviour\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyzing no context data\n",
    "\"\"\"\n",
    "\n",
    "trial_tones_expt = [trial_tones_5b34, trial_tones_d619, trial_tones_c2e0, trial_tones_735a, trial_tones_1304,\n",
    "                    trial_tones_8de3, trial_tones_6b7f, trial_tones_1c3f, trial_tones_1604, trial_tones_801d,\n",
    "                    trial_tones_1396, trial_tones_54db, trial_tones_viol, trial_tones_b4c7, trial_tones_a12e,\n",
    "                    trial_tones_c653, trial_tones_4b7f, trial_tones_fc3a, trial_tones_0d04, trial_tones_1570,\n",
    "                    trial_tones_2099, trial_tones_592d, trial_tones_a45d, trial_tones_0cf1, trial_tones_e453,\n",
    "                    trial_tones_214b, trial_tones_e045, trial_tones_b7cc, trial_tones_6ced, trial_tones_188a,\n",
    "                    trial_tones_1620, trial_tones_e12e, trial_tones_6b0e]\n",
    "trial_behaviour_expt = [trial_behaviour_5b34, trial_behaviour_d619, trial_behaviour_c2e0, trial_behaviour_735a, \n",
    "                        trial_behaviour_1304, trial_behaviour_8de3, trial_behaviour_6b7f, trial_behaviour_1c3f, \n",
    "                        trial_behaviour_1604, trial_behaviour_801d, trial_behaviour_1396, trial_behaviour_54db, \n",
    "                        trial_behaviour_viol, trial_behaviour_b4c7, trial_behaviour_a12e, trial_behaviour_c653,\n",
    "                        trial_behaviour_4b7f, trial_behaviour_fc3a, trial_behaviour_0d04, trial_behaviour_1570,\n",
    "                        trial_behaviour_2099, trial_behaviour_592d, trial_behaviour_a45d, trial_behaviour_0cf1, \n",
    "                        trial_behaviour_e453, trial_behaviour_214b, trial_behaviour_e045, trial_behaviour_b7cc, \n",
    "                        trial_behaviour_6ced, trial_behaviour_188a, trial_behaviour_1620, trial_behaviour_e12e, \n",
    "                        trial_behaviour_6b0e]\n",
    "corrans_expt = [corrans_5b34, corrans_d619, corrans_c2e0, corrans_735a, corrans_1304, corrans_8de3, corrans_6b7f,\n",
    "                corrans_1c3f, corrans_1604, corrans_801d, corrans_1396, corrans_54db, corrans_viol, corrans_b4c7,\n",
    "                corrans_a12e, corrans_c653, corrans_4b7f, corrans_fc3a, corrans_0d04, corrans_1570, corrans_2099, \n",
    "                corrans_592d, corrans_a45d, corrans_0cf1, corrans_e453, corrans_214b, corrans_e045, corrans_b7cc, \n",
    "                corrans_6ced, corrans_188a, corrans_1620, corrans_e12e, corrans_6b0e]\n",
    "mleParams_expt = np.zeros((len(trial_tones_expt),6))\n",
    "mleParams_expt[0,:] = [2.53,2.9,0.055,0.17,0.5,0.46]\n",
    "mleParams_expt[1,:] = [2.56,2.99,0.051,0.26,0.25,0.46]\n",
    "mleParams_expt[2,:] = [2.5,2.89,0.047,0.15,0.84,0.51]\n",
    "mleParams_expt[3,:] = [2.5,2.9,0.056,0.14,0.92,0.5]\n",
    "mleParams_expt[4,:] = [2.05,3.08,0.048,0.5,0.64,0.62]\n",
    "mleParams_expt[5,:] = [2.49,2.91,0.052,0.18,0.73,0.5]\n",
    "mleParams_expt[6,:] = [2.41,3.15,0.051,0.24,0.25,0.26]\n",
    "mleParams_expt[7,:] = [2.59,2.84,0.052,0.14,0.84,0.52]\n",
    "mleParams_expt[8,:] = [2.42,3.09,0.05,0.2,0.27,0.26]\n",
    "mleParams_expt[9,:] = [2.25,3.12,0.05,0.24,0.26,0.66]\n",
    "mleParams_expt[10,:] = [2.45,2.87,0.055,0.18,0.77,0.65]\n",
    "mleParams_expt[11,:] = [2.28,3.12,0.054,0.17,0.26,0.51]\n",
    "mleParams_expt[12,:] = [2.61,2.85,0.048,0.27,0.83,0.48]\n",
    "mleParams_expt[13,:] = [2.4,3.03,0.05,0.22,0.47,0.58]\n",
    "mleParams_expt[14,:] = [2.49,2.98,0.065,0.13,0.5,0.28]\n",
    "mleParams_expt[15,:] = [2.43,2.96,0.051,0.15,0.72,0.5]\n",
    "mleParams_expt[16,:] = [2.43,3.09,0.04,0.24,0.24,0.3]\n",
    "mleParams_expt[17,:] = [2.42,3,0.059,0.14,0.9,0.49]\n",
    "mleParams_expt[18,:] = [2.66,3.01,0.049,0.25,0.46,0.26]\n",
    "mleParams_expt[19,:] = [2.39,2.95,0.055,0.16,0.69,0.65]\n",
    "mleParams_expt[20,:] = [2.69,2.71,0.047,0.12,0.83,0.5]\n",
    "mleParams_expt[21,:] = [2.57,2.98,0.05,0.24,0.7,0.45]\n",
    "mleParams_expt[22,:] = [2.61,2.83,0.057,0.22,0.23,0.45]\n",
    "mleParams_expt[23,:] = [2.63,2.89,0.06,0.18,0.5,0.41]\n",
    "mleParams_expt[24,:] = [2.22,3.14,0.4,0.37,0.79,0.48]\n",
    "mleParams_expt[25,:] = [2.47,2.95,0.039,0.29,0.82,0.46]\n",
    "mleParams_expt[26,:] = [2.19,3.39,0.05,0.78,0.5,0.65]\n",
    "mleParams_expt[27,:] = [2.46,2.9,0.057,0.19,0.26,0.49]\n",
    "mleParams_expt[28,:] = [2.4,3.02,0.057,0.13,0.77,0.51]\n",
    "mleParams_expt[29,:] = [2.36,3.06,0.06,0.14,0.43,0.55]\n",
    "mleParams_expt[30,:] = [2.64,3.16,0.88,0.62,0.89,0.45]\n",
    "mleParams_expt[31,:] = [2.44,2.94,0.058,0.14,0.63,0.57]\n",
    "mleParams_expt[32,:] = [3.14,3.42,0.22,0.62,0.92,0.44]\n",
    "\n",
    "lesionParams_expt = np.zeros((len(trial_tones_expt),6))\n",
    "lesionParams_expt[0,:] = [2.23,3.3,0.05,0.26,0.0001,0.05]\n",
    "lesionParams_expt[1,:] = [3.08,3.31,0.048,0.28,0.0001,0.037]\n",
    "lesionParams_expt[2,:] = [2.1,3.3,0.05,0.25,0.0001,0.85]\n",
    "lesionParams_expt[3,:] = [2.05,3.27,0.046,0.26,0.0001,0.86]\n",
    "lesionParams_expt[4,:] = [2.26,3.29,0.66,0.64,0.0001,0.45]\n",
    "lesionParams_expt[5,:] = [2.04,3.37,0.053,0.21,0.0001,0.47]\n",
    "lesionParams_expt[6,:] = [2.15,3.36,0.037,0.26,0.0001,0.051]\n",
    "lesionParams_expt[7,:] = [2.4,3.24,0.051,0.44,0.0001,0.25]\n",
    "lesionParams_expt[8,:] = [2.15,3.36,0.012,0.25,0.0001,0.65]\n",
    "lesionParams_expt[9,:] = [2.09,3.33,0.041,0.26,0.0001,0.8]\n",
    "lesionParams_expt[10,:] = [2.1,3.29,0.05,0.45,0.0001,0.64]\n",
    "lesionParams_expt[11,:] = [2,3.4,0.052,0.18,0.0001,0.51]\n",
    "lesionParams_expt[12,:] = [2.63,3.17,0.84,0.67,0.0001,0.46]\n",
    "lesionParams_expt[13,:] = [2.1,3.33,0.05,0.25,0.0001,0.84]\n",
    "lesionParams_expt[14,:] = [2.07,3.33,0.055,0.16,0.0001,0.49]\n",
    "lesionParams_expt[15,:] = [2.09,3.32,0.05,0.25,0.0001,0.47]\n",
    "lesionParams_expt[16,:] = [1.77,3.64,0.041,0.25,0.0001,0.61]\n",
    "lesionParams_expt[17,:] = [2,3.33,0.031,0.19,0.0001,0.97]\n",
    "lesionParams_expt[18,:] = [2.52,3.06,0.053,0.41,0.0001,0.24]\n",
    "lesionParams_expt[19,:] = [2.1,3.31,0.031,0.26,0.0001,0.84]\n",
    "lesionParams_expt[20,:] = [2.08,3.33,0.043,0.25,0.0001,0.44]\n",
    "lesionParams_expt[21,:] = [2.14,3.32,0.052,0.39,0.0001,0.66]\n",
    "lesionParams_expt[22,:] = [2.27,3.33,0.049,0.26,0.0001,0.048]\n",
    "lesionParams_expt[23,:] = [2.08,3.33,0.049,0.25,0.0001,0.46]\n",
    "lesionParams_expt[24,:] = [2.14,3.12,0.054,0.38,0.0001,0.66]\n",
    "lesionParams_expt[25,:] = [2.82,3.3,0.05,0.46,0.0001,0.05]\n",
    "lesionParams_expt[26,:] = [2.1,3.04,0.051,0.86,0.0001,0.84]\n",
    "lesionParams_expt[27,:] = [2.06,3.29,0.048,0.18,0.0001,0.53]\n",
    "lesionParams_expt[28,:] = [1.96,3.31,0.051,0.18,0.0001,0.98]\n",
    "lesionParams_expt[29,:] = [1.83,3.51,0.058,0.2,0.0001,0.83]\n",
    "lesionParams_expt[30,:] = [2.12,2.67,0.46,0.64,0.0001,0.65]\n",
    "lesionParams_expt[31,:] = [2.08,3.33,0.051,0.25,0.0001,0.84]\n",
    "lesionParams_expt[32,:] = [2.27,3.33,0.83,0.94,0.0001,0.46]\n",
    "\n",
    "votingParams_expt = np.zeros((len(trial_tones_expt),2))\n",
    "votingParams_expt[0,:] = [2.77,0.21]\n",
    "votingParams_expt[1,:] = [2.72,0.27]\n",
    "votingParams_expt[2,:] = [2.78,0.23]\n",
    "votingParams_expt[3,:] = [2.79,0.22]\n",
    "votingParams_expt[4,:] = [2.64,0.57]\n",
    "votingParams_expt[5,:] = [2.77,0.2]\n",
    "votingParams_expt[6,:] = [2.79,0.22]\n",
    "votingParams_expt[7,:] = [2.77,0.24]\n",
    "votingParams_expt[8,:] = [2.76,0.23]\n",
    "votingParams_expt[9,:] = [2.77,0.24]\n",
    "votingParams_expt[10,:] = [2.72,0.3]\n",
    "votingParams_expt[11,:] = [2.74,0.17]\n",
    "votingParams_expt[12,:] = [2.67,0.49]\n",
    "votingParams_expt[13,:] = [2.75,0.22]\n",
    "votingParams_expt[14,:] = [2.67,0.15]\n",
    "votingParams_expt[15,:] = [2.65,0.2]\n",
    "votingParams_expt[16,:] = [2.74,0.24]\n",
    "votingParams_expt[17,:] = [2.71,0.16]\n",
    "votingParams_expt[18,:] = [2.65,0.32]\n",
    "votingParams_expt[19,:] = [2.77,0.21]\n",
    "votingParams_expt[20,:] = [2.68,0.21]\n",
    "votingParams_expt[21,:] = [2.76,0.26]\n",
    "votingParams_expt[22,:] = [2.78,0.23]\n",
    "votingParams_expt[23,:] = [2.61,0.25]\n",
    "votingParams_expt[24,:] = [2.62,0.34]\n",
    "votingParams_expt[25,:] = [2.65,0.33]\n",
    "votingParams_expt[26,:] = [3.04,0.75]\n",
    "votingParams_expt[27,:] = [2.63,0.2]\n",
    "votingParams_expt[28,:] = [2.64,0.14]\n",
    "votingParams_expt[29,:] = [2.72,0.14]\n",
    "votingParams_expt[30,:] = [2.67,0.51]\n",
    "votingParams_expt[31,:] = [2.72,0.16]\n",
    "votingParams_expt[32,:] = [2.63,0.83]\n",
    "\n",
    "allMeanTone_behvSubjArray = np.zeros((len(trial_behaviour_expt),4,len(logFreqBins)))\n",
    "GaussMeanTone_behvSubjArray = np.zeros((len(trial_behaviour_expt),4,len(logFreqBins)))\n",
    "negll_fullBayes = np.zeros((len(mleParams_expt),4))\n",
    "negll_lesionBayes = np.zeros((len(lesionParams_expt),4))\n",
    "negll_votingModel = np.zeros((len(votingParams_expt),4))\n",
    "#accuracyAcrossFreq = np.zeros((len(trial_behaviour_expt),8))\n",
    "noDistractorAccuracy = np.zeros((len(mleParams_expt),))\n",
    "oneDistractorAccuracy = np.zeros((len(mleParams_expt),))\n",
    "twoDistractorsAccuracy = np.zeros((len(mleParams_expt),))\n",
    "threeDistractorsAccuracy = np.zeros((len(mleParams_expt),))\n",
    "overallAccuracy = np.zeros((len(mleParams_expt),))\n",
    "predictionAccuracyBayes = np.zeros((len(mleParams_expt),3))\n",
    "predictionAccuracyLesion = np.zeros((len(mleParams_expt),3))\n",
    "predictionAccuracyVoting = np.zeros((len(mleParams_expt),3))\n",
    "AllTonesGaussianAccuracy = np.zeros((len(mleParams_expt),))\n",
    "OneDistGaussianAccuracy = np.zeros((len(mleParams_expt),))\n",
    "OneDistGaussianMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "OneDistGaussianSignalMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandLHAccuracy = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandLHMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandLHSignalMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandHLAccuracy = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandHLMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "OneDistOppSidebandHLSignalMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "TwoDistOppSidebandAccuracy = np.zeros((len(mleParams_expt),)) \n",
    "TwoDistOppSidebandMeanExplained = np.zeros((len(mleParams_expt),))\n",
    "BiasWithUnclearSigLowDist = np.zeros((len(mleParams_expt),2))\n",
    "BiasWithUnclearSigHighDist = np.zeros((len(mleParams_expt),2))\n",
    "allGaussLowAccuracy = np.zeros((len(mleParams_expt),))\n",
    "allGaussHighAccuracy = np.zeros((len(mleParams_expt),))\n",
    "distGaussLowAccuracy = np.zeros((len(mleParams_expt),))\n",
    "distGaussHighAccuracy = np.zeros((len(mleParams_expt),))\n",
    "accuracyVsIdealMeanBhv = np.zeros((len(mleParams_expt),))\n",
    "accuracyVsIdealSignalMeanBhv = np.zeros((len(mleParams_expt),))\n",
    "accuracyVsIdealVotingBhv = np.zeros((len(mleParams_expt),))\n",
    "\n",
    "for iSubj in range(len(trial_behaviour_expt)):\n",
    "    [idealSubjMeanBhv, \n",
    "     idealSubjSignalMeanBhv, \n",
    "     idealSubjVotingBhv] = idealSubjectBehaviour(tonesPresented=np.log10(trial_tones_expt[iSubj]))\n",
    "     \n",
    "    [accuracyVsIdealMeanBhv[iSubj],\n",
    "     accuracyVsIdealSignalMeanBhv[iSubj],\n",
    "     accuracyVsIdealVotingBhv[iSubj]] = compareBehaviourWithDistractors(tonesPresented=np.log10(trial_tones_expt[iSubj]),\n",
    "                                                                subjectBehaviour=trial_behaviour_expt[iSubj],\n",
    "                                                                idealMeanBehaviour=idealSubjMeanBhv,\n",
    "                                                                idealSignalMeanBehaviour=idealSubjSignalMeanBhv,\n",
    "                                                                idealVotingBehaviour=idealSubjVotingBhv)\n",
    "    \n",
    "    [AllGaussLow, AllGaussHigh, DistGaussLow, DistGaussHigh, \n",
    "     DistLowOtherLow, DistHighOtherHigh, DistLowOtherHigh, \n",
    "     DistHighOtherLow, DistLowOtherLowHigh, DistHighOtherLowHigh,\n",
    "     DistTwoExtrOtherGauss, DistTwoExtrSameGauss, \n",
    "     DistThreeExtr, DistThreeExtrTwoSame, \n",
    "     OneDistractor, TwoDistractors] = trialCategories(np.log10(trial_tones_expt[iSubj]), \n",
    "                                                      trial_behaviour_expt[iSubj], \n",
    "                                                      corrans_expt[iSubj])\n",
    "    \n",
    "    Accuracy, BiasWithUnclearSig = accuracies(AllGaussLow, AllGaussHigh, DistGaussLow, DistGaussHigh, \n",
    "                                               DistLowOtherLow, DistHighOtherHigh, DistLowOtherHigh, \n",
    "                                               DistHighOtherLow, DistLowOtherLowHigh, DistHighOtherLowHigh,\n",
    "                                               DistTwoExtrOtherGauss, DistTwoExtrSameGauss, \n",
    "                                               DistThreeExtrTwoSame, corrans_expt[iSubj], trial_behaviour_expt[iSubj])   \n",
    "    # Note: accuracy has been multiplied by 100 so is in percentage\n",
    "    allGaussLowAccuracy[iSubj] = np.mean(AllGaussLow[:,3]==AllGaussLow[:,4])\n",
    "    allGaussHighAccuracy[iSubj] = np.mean(AllGaussHigh[:,3]==AllGaussHigh[:,4])\n",
    "    distGaussLowAccuracy[iSubj] = np.mean(DistGaussLow[:,3]==DistGaussLow[:,4])\n",
    "    distGaussHighAccuracy[iSubj] = np.mean(DistGaussHigh[:,3]==DistGaussHigh[:,4])\n",
    "    AllTonesGaussianAccuracy[iSubj] = Accuracy[0,0]\n",
    "    OneDistGaussianAccuracy[iSubj] = Accuracy[2,0]\n",
    "    OneDistGaussianMeanExplained[iSubj] = Accuracy[2,2]\n",
    "    OneDistGaussianSignalMeanExplained[iSubj] = Accuracy[2,4]\n",
    "    OneDistOppSidebandLHAccuracy[iSubj] = Accuracy[4,0]\n",
    "    OneDistOppSidebandLHMeanExplained[iSubj] = Accuracy[4,2]\n",
    "    OneDistOppSidebandLHSignalMeanExplained[iSubj] = Accuracy[4,4]\n",
    "    OneDistOppSidebandHLAccuracy[iSubj] = Accuracy[5,0]\n",
    "    OneDistOppSidebandHLMeanExplained[iSubj] = Accuracy[5,2]\n",
    "    OneDistOppSidebandHLSignalMeanExplained[iSubj] = Accuracy[5,4]\n",
    "    TwoDistOppSidebandAccuracy[iSubj] = Accuracy[6,0]\n",
    "    TwoDistOppSidebandMeanExplained[iSubj] = Accuracy[6,2]\n",
    "    BiasWithUnclearSigLowDist[iSubj,:] = BiasWithUnclearSig[:,:2]\n",
    "    BiasWithUnclearSigHighDist[iSubj,:] = BiasWithUnclearSig[:,2:]\n",
    "    \n",
    "    DistractorGaussian = np.concatenate((DistGaussLow,DistGaussHigh),axis=0)\n",
    "    AllGaussian = np.concatenate((AllGaussLow, AllGaussHigh),axis=0)\n",
    "    DistractorOppSideband = np.concatenate((DistLowOtherHigh,DistHighOtherLow))\n",
    "    DistractorSameSideband = np.concatenate((DistLowOtherLow,DistHighOtherHigh))\n",
    "    ThreeDistractors = np.concatenate((DistThreeExtr,DistThreeExtrTwoSame),axis=0)\n",
    "    \n",
    "    noDistractorAccuracy[iSubj] = np.mean(AllGaussian[:,3]==AllGaussian[:,4])\n",
    "    oneDistractorAccuracy[iSubj] = np.mean(OneDistractor[:,3]==OneDistractor[:,4])\n",
    "    twoDistractorsAccuracy[iSubj] = np.mean(TwoDistractors[:,3]==TwoDistractors[:,4])\n",
    "    threeDistractorsAccuracy[iSubj] = np.mean(ThreeDistractors[:,3]==ThreeDistractors[:,4])\n",
    "    overallAccuracy[iSubj] = np.mean(trial_behaviour_expt[iSubj]==corrans_expt[iSubj])\n",
    "    \n",
    "    categoriesForLikelihoodPlot = [AllGaussian, DistractorGaussian, DistractorOppSideband, DistTwoExtrOtherGauss]\n",
    "    \n",
    "    #negll_full = MLE(mleParams_expt[iSubj,:], \n",
    "    #                 trial_tones=trial_tones_expt[iSubj], \n",
    "    #                 trial_behaviour=trial_behaviour_expt[iSubj])\n",
    "    #negll_lesion = MLE(lesionParams_expt[iSubj,:],\n",
    "    #                  trial_tones=trial_tones_expt[iSubj], \n",
    "    #                  trial_behaviour=trial_behaviour_expt[iSubj])\n",
    "    #negll_voting = MLE_voting(votingParams_expt[iSubj,:],\n",
    "    #                  trial_tones=trial_tones_expt[iSubj], \n",
    "    #                  trial_behaviour=trial_behaviour_expt[iSubj])\n",
    "\n",
    "    #for iTrialCategory in range(len(categoriesForLikelihoodPlot)):\n",
    "    #    negll_fullBayes[iSubj,iTrialCategory] = np.sum(negll_full[categoriesForLikelihoodPlot[iTrialCategory][:,12].astype(int)])\n",
    "    #    negll_lesionBayes[iSubj,iTrialCategory] = np.sum(negll_lesion[categoriesForLikelihoodPlot[iTrialCategory][:,12].astype(int)])\n",
    "    #    negll_votingModel[iSubj,iTrialCategory] = np.sum(negll_voting[categoriesForLikelihoodPlot[iTrialCategory][:,12].astype(int)])        \n",
    "        \n",
    "    #print('-ve logLikelihood full Bayes different categories',mleParams_expt[iSubj,:],negll_fullBayes[iSubj,:])\n",
    "    #print('-ve logLikelihood lesion model different categories',lesionParams_expt[iSubj,:],negll_lesionBayes[iSubj,:])\n",
    "    #print('-ve logLikelihood voting model different categories',votingParams_expt[iSubj,:],negll_votingModel[iSubj,:])\n",
    "    \n",
    "    #all_trial_tones, all_trial_behaviour = generate_behaviour(trial_tones_expt[iSubj], \n",
    "    #                                                          prob_back=mleParams_expt[iSubj,4], \n",
    "    #                                                          prob_low=mleParams_expt[iSubj,5], \n",
    "    #                                                          log_prior_params=mleParams_expt[iSubj,:3], \n",
    "    #                                                          sigma_sensory=mleParams_expt[iSubj,3], \n",
    "    #                                                          reps=1, n_tones=3)\n",
    "    \n",
    "    #predictionAccuracyBayes[iSubj,0] = np.mean([np.mean(trial_behaviour_expt[iSubj][allGaussLow[:,12].astype(int)]==all_trial_behaviour[allGaussLow[:,12].astype(int)]),\n",
    "    #              np.mean(trial_behaviour_expt[iSubj][allGaussHigh[:,12].astype(int)]==all_trial_behaviour[allGaussHigh[:,12].astype(int)]),\n",
    "    #              np.mean(trial_behaviour_expt[iSubj][distLowOtherLow[:,12].astype(int)]==all_trial_behaviour[distLowOtherLow[:,12].astype(int)]),\n",
    "    #              np.mean(trial_behaviour_expt[iSubj][distHighOtherHigh[:,12].astype(int)]==all_trial_behaviour[distHighOtherHigh[:,12].astype(int)]),\n",
    "    #              np.mean(trial_behaviour_expt[iSubj][distTwoExtrSameGauss[:,12].astype(int)]==all_trial_behaviour[distTwoExtrSameGauss[:,12].astype(int)])])\n",
    "    #predictionAccuracyBayes[iSubj,1] = np.mean([np.mean(trial_behaviour_expt[iSubj][distLowOtherHigh[:,12].astype(int)]==all_trial_behaviour[distLowOtherHigh[:,12].astype(int)]),\n",
    "    #                  np.mean(trial_behaviour_expt[iSubj][distHighOtherLow[:,12].astype(int)]==all_trial_behaviour[distHighOtherLow[:,12].astype(int)]),\n",
    "    #                  np.mean(trial_behaviour_expt[iSubj][distGaussHigh[:,12].astype(int)]==all_trial_behaviour[distGaussHigh[:,12].astype(int)]),\n",
    "    #                  np.mean(trial_behaviour_expt[iSubj][distGaussLow[:,12].astype(int)]==all_trial_behaviour[distGaussLow[:,12].astype(int)])])\n",
    "    #predictionAccuracyBayes[iSubj,2] = np.mean(trial_behaviour_expt[iSubj][distTwoExtrOtherGauss[:,12].astype(int)]==all_trial_behaviour[distTwoExtrOtherGauss[:,12].astype(int)])\n",
    "\n",
    "    #allMeanTone_behvSubjArray[iSubj,:,:], GaussMeanTone_behvSubjArray[iSubj,:,:] = plotting(allGaussian, \n",
    "    #                                                                                        distractorGaussian, \n",
    "    #                                                                                        distLowOtherHigh, \n",
    "    #                                                                                        distHighOtherLow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This computes the last row of the tables, where we are interested in the bias due to all the strategies\n",
    "as applicable to trials with one tone from gaussians and one tone as distractor.\n",
    "\"\"\"\n",
    "print((BiasWithUnclearSigLowDist[:,0]+BiasWithUnclearSigHighDist[:,1])/2) \n",
    "print((BiasWithUnclearSigLowDist[:,1]+BiasWithUnclearSigHighDist[:,0])/2)\n",
    "print((BiasWithUnclearSigLowDist[:,1]+BiasWithUnclearSigHighDist[:,0])/\n",
    "      (BiasWithUnclearSigLowDist[:,0]+BiasWithUnclearSigHighDist[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyzing long context data biased towards low\n",
    "\"\"\"\n",
    "\n",
    "trial_tones_exptLc = [trial_tones_d619Lc, trial_tones_c2e0Lc, trial_tones_735aLc,\n",
    "                      trial_tones_1304Lc, trial_tones_6b7fLc, trial_tones_1c3fLc,\n",
    "                      trial_tones_1604Lc, trial_tones_801dLc, trial_tones_1396Lc, \n",
    "                      trial_tones_b4c7Lc, trial_tones_a12eLc, trial_tones_c653Lc, \n",
    "                      trial_tones_4b7fLc, trial_tones_fc3aLc, trial_tones_0d04Lc,\n",
    "                      trial_tones_1570Lc, trial_tones_2099Lc, trial_tones_592dLc,\n",
    "                      trial_tones_a45dLc, trial_tones_0cf1Lc, trial_tones_e453Lc, \n",
    "                      trial_tones_214bLc,trial_tones_b7ccLc, trial_tones_6cedLc, \n",
    "                      trial_tones_188aLc, trial_tones_e12eLc]\n",
    "trial_behaviour_exptLc = [trial_behaviour_d619Lc, trial_behaviour_c2e0Lc, trial_behaviour_735aLc,\n",
    "                          trial_behaviour_1304Lc, trial_behaviour_6b7fLc, trial_behaviour_1c3fLc, \n",
    "                          trial_behaviour_1604Lc, trial_behaviour_801dLc, trial_behaviour_1396Lc, \n",
    "                          trial_behaviour_b4c7Lc, trial_behaviour_a12eLc, trial_behaviour_c653Lc, \n",
    "                          trial_behaviour_4b7fLc, trial_behaviour_fc3aLc, trial_behaviour_0d04Lc,\n",
    "                          trial_behaviour_1570Lc, trial_behaviour_2099Lc, trial_behaviour_592dLc, \n",
    "                          trial_behaviour_a45dLc,\n",
    "                          trial_behaviour_0cf1Lc, trial_behaviour_e453Lc, trial_behaviour_214bLc,\n",
    "                          trial_behaviour_b7ccLc, trial_behaviour_6cedLc, trial_behaviour_188aLc,\n",
    "                          trial_behaviour_e12eLc]\n",
    "trial_tone_cat_exptLc = [trial_tone_cat_d619Lc, trial_tone_cat_c2e0Lc, trial_tone_cat_735aLc,\n",
    "                          trial_tone_cat_1304Lc, trial_tone_cat_6b7fLc, trial_tone_cat_1c3fLc, \n",
    "                          trial_tone_cat_1604Lc, trial_tone_cat_801dLc, trial_tone_cat_1396Lc, \n",
    "                          trial_tone_cat_b4c7Lc, trial_tone_cat_a12eLc, trial_tone_cat_c653Lc, \n",
    "                          trial_tone_cat_4b7fLc, trial_tone_cat_fc3aLc, trial_tone_cat_0d04Lc,\n",
    "                          trial_tone_cat_1570Lc,\n",
    "                          trial_tone_cat_2099Lc, trial_tone_cat_592dLc, trial_tone_cat_a45dLc,\n",
    "                          trial_tone_cat_0cf1Lc, trial_tone_cat_e453Lc, trial_tone_cat_214bLc,\n",
    "                          trial_tone_cat_b7ccLc, trial_tone_cat_6cedLc, trial_tone_cat_188aLc,\n",
    "                          trial_tone_cat_e12eLc]\n",
    "corrans_exptLc = [corrans_d619Lc, corrans_c2e0Lc, corrans_735aLc, corrans_1304Lc, \n",
    "                  corrans_6b7fLc, corrans_1c3fLc, corrans_1604Lc, corrans_801dLc, \n",
    "                  corrans_1396Lc, corrans_b4c7Lc, corrans_a12eLc, corrans_c653Lc, \n",
    "                  corrans_4b7fLc, corrans_fc3aLc, corrans_0d04Lc, corrans_1570Lc, corrans_2099Lc, \n",
    "                  corrans_592dLc, corrans_a45dLc, corrans_0cf1Lc, corrans_e453Lc, \n",
    "                  corrans_214bLc, corrans_b7ccLc, corrans_6cedLc, corrans_188aLc,\n",
    "                  corrans_e12eLc]\n",
    "\n",
    "mleParams_exptLc = np.zeros((len(trial_tones_exptLc),6))\n",
    "mleParams_exptLc[0,:] = [2.53,2.87,0.05,0.19,0.5,0.68]\n",
    "mleParams_exptLc[1,:] = [2.55,2.82,0.05,0.15,0.51,0.75]\n",
    "mleParams_exptLc[2,:] = [2.54,2.84,0.06,0.15,0.69,0.61]\n",
    "mleParams_exptLc[3,:] = [2.54,3.02,0.49,0.83,0.44,0.66]\n",
    "mleParams_exptLc[4,:] = [2.35,3.12,0.05,0.14,0.26,0.55]\n",
    "mleParams_exptLc[5,:] = [2.53,2.88,0.05,0.1,0.58,0.72]\n",
    "mleParams_exptLc[6,:] = [2.56,3,0.05,0.25,0.26,0.45]\n",
    "mleParams_exptLc[7,:] = [2.64,2.96,0.04,0.37,0.49,0.54]\n",
    "mleParams_exptLc[8,:] = [2.51,2.89,0.06,0.17,0.68,0.59]\n",
    "mleParams_exptLc[9,:] = [2.24,3.19,0.06,0.2,0.37,0.61]\n",
    "mleParams_exptLc[10,:] = [2.39,3.07,0.06,0.14,0.51,0.5]\n",
    "#mleParams_exptLc[11,:] = [2.39,3.07,0.06,0.14,0.51,0.5]\n",
    "#mleParams_exptLc[12,:] = [2.39,3.07,0.06,0.14,0.51,0.5]\n",
    "#mleParams_exptLc[13,:] = [2.39,3.07,0.06,0.14,0.51,0.5]\n",
    "mleParams_exptLc[14,:] = [2.59,2.82,0.05,0.24,0.86,0.57]\n",
    "mleParams_exptLc[15,:] = [2.59,2.75,0.048,0.06,0.26,0.75]\n",
    "\n",
    "allMeanTone_behvSubjArrayLc = np.zeros((len(trial_behaviour_exptLc),4,len(logFreqBins)))\n",
    "GaussMeanTone_behvSubjArrayLc = np.zeros((len(trial_behaviour_exptLc),4,len(logFreqBins)))\n",
    "noDistractorAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "oneDistractorAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "twoDistractorsAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "threeDistractorsAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "AllTonesGaussianAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "overallAccuracyLc = np.zeros((len(mleParams_expt),))\n",
    "OneDistGaussianAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistGaussianMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistGaussianSignalMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandLHAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandLHMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandLHSignalMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandHLAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandHLMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "OneDistOppSidebandHLSignalMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "TwoDistOppSidebandAccuracyLc = np.zeros((len(trial_behaviour_exptLc),)) \n",
    "TwoDistOppSidebandMeanExplainedLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "allGaussLowAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "allGaussHighAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "distGaussLowAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "distGaussHighAccuracyLc = np.zeros((len(trial_behaviour_exptLc),))\n",
    "BiasWithUnclearSigLowDistLc = np.zeros((len(mleParams_exptLc),2))\n",
    "BiasWithUnclearSigHighDistLc = np.zeros((len(mleParams_exptLc),2))\n",
    "\n",
    "for iSubj in range(len(trial_behaviour_exptLc)):\n",
    "    [AllGaussLowLc, AllGaussHighLc, DistGaussLowLc, DistGaussHighLc, DistLowOtherLowLc, \n",
    "     DistHighOtherHighLc, DistLowOtherHighLc, DistHighOtherLowLc, DistLowOtherLowHighLc, \n",
    "     DistHighOtherLowHighLc, DistTwoExtrOtherGaussLc, \n",
    "     DistTwoExtrSameGaussLc, DistThreeExtrLc, DistThreeExtrTwoSameLc,\n",
    "     OneDistractorLc, TwoDistractorsLc] = trialCategories(np.log10(trial_tones_exptLc[iSubj]),\n",
    "                                                          trial_behaviour_exptLc[iSubj],\n",
    "                                                          corrans_exptLc[iSubj])\n",
    "    \n",
    "    AccuracyLc, BiasWithUnclearSigLc = accuracies(AllGaussLowLc, AllGaussHighLc, DistGaussLowLc, DistGaussHighLc, \n",
    "                                                DistLowOtherLowLc, DistHighOtherHighLc, DistLowOtherHighLc, \n",
    "                                                DistHighOtherLowLc, DistLowOtherLowHighLc, DistHighOtherLowHighLc,\n",
    "                                                DistTwoExtrOtherGaussLc, DistTwoExtrSameGaussLc, \n",
    "                                                DistThreeExtrTwoSameLc, corrans_exptLc[iSubj], \n",
    "                                                trial_behaviour_exptLc[iSubj]) \n",
    "    \n",
    "    # Note: accuracy has been multiplied by 100 so is in percentage\n",
    "        # Note: accuracy has been multiplied by 100 so is in percentage\n",
    "    allGaussLowAccuracyLc[iSubj] = np.mean(AllGaussLowLc[:,3]==AllGaussLowLc[:,4])\n",
    "    allGaussHighAccuracyLc[iSubj] = np.mean(AllGaussHighLc[:,3]==AllGaussHighLc[:,4])\n",
    "    distGaussLowAccuracyLc[iSubj] = np.mean(DistGaussLowLc[:,3]==DistGaussLowLc[:,4])\n",
    "    distGaussHighAccuracyLc[iSubj] = np.mean(DistGaussHighLc[:,3]==DistGaussHighLc[:,4])\n",
    "    AllTonesGaussianAccuracyLc[iSubj] = AccuracyLc[0,0]\n",
    "    OneDistGaussianAccuracyLc[iSubj] = AccuracyLc[2,0]\n",
    "    OneDistGaussianMeanExplainedLc[iSubj] = AccuracyLc[2,2]\n",
    "    OneDistGaussianSignalMeanExplainedLc[iSubj] = AccuracyLc[2,4]\n",
    "    OneDistOppSidebandLHAccuracyLc[iSubj] = AccuracyLc[4,0]\n",
    "    OneDistOppSidebandLHMeanExplainedLc[iSubj] = AccuracyLc[4,2]\n",
    "    OneDistOppSidebandLHSignalMeanExplainedLc[iSubj] = AccuracyLc[4,4]\n",
    "    OneDistOppSidebandHLAccuracyLc[iSubj] = AccuracyLc[5,0]\n",
    "    OneDistOppSidebandHLMeanExplainedLc[iSubj] = AccuracyLc[5,2]\n",
    "    OneDistOppSidebandHLSignalMeanExplainedLc[iSubj] = AccuracyLc[5,4]\n",
    "    TwoDistOppSidebandAccuracyLc[iSubj] = AccuracyLc[6,0]\n",
    "    TwoDistOppSidebandMeanExplainedLc[iSubj] = AccuracyLc[6,2]\n",
    "    BiasWithUnclearSigLowDistLc[iSubj,:] = BiasWithUnclearSigLc[:,:2]\n",
    "    BiasWithUnclearSigHighDistLc[iSubj,:] = BiasWithUnclearSigLc[:,2:]\n",
    "    \n",
    "    DistractorGaussianLc = np.concatenate((DistGaussLowLc,DistGaussHighLc),axis=0)\n",
    "    AllGaussianLc = np.concatenate((AllGaussLowLc, AllGaussHighLc),axis=0)\n",
    "    DistractorOppSidebandLc = np.concatenate((DistLowOtherHighLc,DistHighOtherLowLc))\n",
    "    DistractorSameSidebandLc = np.concatenate((DistLowOtherLowLc,DistHighOtherHighLc))\n",
    "    ThreeDistractorsLc = np.concatenate((DistThreeExtrLc,DistThreeExtrTwoSameLc),axis=0)\n",
    "    \n",
    "    noDistractorAccuracyLc[iSubj] = np.mean(AllGaussianLc[:,3]==AllGaussianLc[:,4])\n",
    "    oneDistractorAccuracyLc[iSubj] = np.mean(OneDistractorLc[:,3]==OneDistractorLc[:,4])\n",
    "    twoDistractorsAccuracyLc[iSubj] = np.mean(TwoDistractorsLc[:,3]==TwoDistractorsLc[:,4])\n",
    "    threeDistractorsAccuracyLc[iSubj] = np.mean(ThreeDistractorsLc[:,3]==ThreeDistractorsLc[:,4])\n",
    "    overallAccuracyLc[iSubj] = np.mean(trial_behaviour_exptLc[iSubj]==corrans_exptLc[iSubj])\n",
    "    \n",
    "    #allMeanTone_behvSubjArrayLc[iSubj,:,:], GaussMeanTone_behvSubjArrayLc[iSubj,:,:] = plotting(allGaussianLc, \n",
    "    #                                                                                            distractorGaussianLc, \n",
    "    #                                                                                            distLowOtherHighLc, \n",
    "    #                                                                                            distHighOtherLowLc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This computes the last row of the tables, where we are interested in the bias due to all the strategies\n",
    "as applicable to trials with one tone from gaussians and one tone as distractor.\n",
    "\"\"\"\n",
    "print((BiasWithUnclearSigLowDistLc[:,0]+BiasWithUnclearSigHighDistLc[:,1])/2) \n",
    "print((BiasWithUnclearSigLowDistLc[:,1]+BiasWithUnclearSigHighDistLc[:,0])/2)\n",
    "print((BiasWithUnclearSigLowDistLc[:,1]+BiasWithUnclearSigHighDistLc[:,0])*100/\n",
    "      (BiasWithUnclearSigLowDistLc[:,0]+BiasWithUnclearSigHighDistLc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Analyzing long context data biased towards high\n",
    "\"\"\"\n",
    "\n",
    "trial_tones_exptHc = [trial_tones_735aHc,\n",
    "                      trial_tones_1304Hc, trial_tones_6b7fHc, trial_tones_1c3fHc,\n",
    "                      trial_tones_1604Hc, trial_tones_1396Hc, \n",
    "                      trial_tones_b4c7Hc, trial_tones_a12eHc, trial_tones_c653Hc, \n",
    "                      trial_tones_4b7fHc, trial_tones_fc3aHc, trial_tones_0d04Hc,\n",
    "                      trial_tones_2099Hc, trial_tones_592dHc,\n",
    "                      trial_tones_a45dHc, trial_tones_e453Hc, \n",
    "                      trial_tones_214bHc,trial_tones_b7ccHc, trial_tones_6cedHc, \n",
    "                      trial_tones_188aHc, trial_tones_e12eHc]\n",
    "trial_behaviour_exptHc = [trial_behaviour_735aHc,\n",
    "                          trial_behaviour_1304Hc, trial_behaviour_6b7fHc, trial_behaviour_1c3fHc, \n",
    "                          trial_behaviour_1604Hc, trial_behaviour_1396Hc, \n",
    "                          trial_behaviour_b4c7Hc, trial_behaviour_a12eHc, trial_behaviour_c653Hc, \n",
    "                          trial_behaviour_4b7fHc, trial_behaviour_fc3aHc, trial_behaviour_0d04Hc,\n",
    "                          trial_behaviour_2099Hc, trial_behaviour_592dHc, \n",
    "                          trial_behaviour_a45dHc, trial_behaviour_e453Hc, trial_behaviour_214bHc,\n",
    "                          trial_behaviour_b7ccHc, trial_behaviour_6cedHc, trial_behaviour_188aHc,\n",
    "                          trial_behaviour_e12eHc]\n",
    "trial_tone_cat_exptHc = [trial_tone_cat_735aHc,\n",
    "                          trial_tone_cat_1304Hc, trial_tone_cat_6b7fHc, trial_tone_cat_1c3fHc, \n",
    "                          trial_tone_cat_1604Hc, trial_tone_cat_1396Hc, \n",
    "                          trial_tone_cat_b4c7Hc, trial_tone_cat_a12eHc, trial_tone_cat_c653Hc, \n",
    "                          trial_tone_cat_4b7fHc, trial_tone_cat_fc3aHc, trial_tone_cat_0d04Hc,\n",
    "                          trial_tone_cat_2099Hc, trial_tone_cat_592dHc, trial_tone_cat_a45dHc,\n",
    "                          trial_tone_cat_e453Hc, trial_tone_cat_214bHc,\n",
    "                          trial_tone_cat_b7ccHc, trial_tone_cat_6cedHc, trial_tone_cat_188aHc,\n",
    "                          trial_tone_cat_e12eHc]\n",
    "corrans_exptHc = [corrans_735aHc, corrans_1304Hc, \n",
    "                  corrans_6b7fHc, corrans_1c3fHc, corrans_1604Hc, \n",
    "                  corrans_1396Hc, corrans_b4c7Hc, corrans_a12eHc, corrans_c653Hc, \n",
    "                  corrans_4b7fHc, corrans_fc3aHc, corrans_0d04Hc, corrans_2099Hc, \n",
    "                  corrans_592dHc, corrans_a45dHc, corrans_e453Hc, \n",
    "                  corrans_214bHc, corrans_b7ccHc, corrans_6cedHc, corrans_188aHc,\n",
    "                  corrans_e12eHc]\n",
    "\n",
    "mleParams_exptHc = np.zeros((len(trial_tones_exptHc),6))\n",
    "\n",
    "allMeanTone_behvSubjArrayHc = np.zeros((len(trial_behaviour_exptHc),4,len(logFreqBins)))\n",
    "GaussMeanTone_behvSubjArrayHc = np.zeros((len(trial_behaviour_exptHc),4,len(logFreqBins)))\n",
    "noDistractorAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "oneDistractorAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "twoDistractorsAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "threeDistractorsAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "AllTonesGaussianAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "overallAccuracyHc = np.zeros((len(mleParams_exptHc),))\n",
    "OneDistGaussianAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistGaussianMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistGaussianSignalMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandLHAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandLHMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandLHSignalMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandHLAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandHLMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "OneDistOppSidebandHLSignalMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "TwoDistOppSidebandAccuracyHc = np.zeros((len(trial_behaviour_exptHc),)) \n",
    "TwoDistOppSidebandMeanExplainedHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "allGaussLowAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "allGaussHighAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "distGaussLowAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "distGaussHighAccuracyHc = np.zeros((len(trial_behaviour_exptHc),))\n",
    "BiasWithUnclearSigLowDistHc = np.zeros((len(mleParams_exptHc),2))\n",
    "BiasWithUnclearSigHighDistHc = np.zeros((len(mleParams_exptHc),2))\n",
    "\n",
    "for iSubj in [15,16]:#range(len(trial_behaviour_exptHc)):\n",
    "    [AllGaussLowHc, AllGaussHighHc, DistGaussLowHc, DistGaussHighHc, DistLowOtherLowHc, \n",
    "     DistHighOtherHighHc, DistLowOtherHighHc, DistHighOtherLowHc, DistLowOtherLowHighHc, \n",
    "     DistHighOtherLowHighHc, DistTwoExtrOtherGaussHc, \n",
    "     DistTwoExtrSameGaussHc, DistThreeExtrHc, DistThreeExtrTwoSameHc,\n",
    "     OneDistractorHc, TwoDistractorsHc] = trialCategories(np.log10(trial_tones_exptHc[iSubj]),\n",
    "                                                          trial_behaviour_exptHc[iSubj],\n",
    "                                                          corrans_exptHc[iSubj])\n",
    "    \n",
    "    AccuracyHc, BiasWithUnclearSigHc = accuracies(AllGaussLowHc, AllGaussHighHc, DistGaussLowHc, DistGaussHighHc, \n",
    "                                                DistLowOtherLowHc, DistHighOtherHighHc, DistLowOtherHighHc, \n",
    "                                                DistHighOtherLowHc, DistLowOtherLowHighHc, DistHighOtherLowHighHc,\n",
    "                                                DistTwoExtrOtherGaussHc, DistTwoExtrSameGaussHc, \n",
    "                                                DistThreeExtrTwoSameHc, corrans_exptHc[iSubj], \n",
    "                                                trial_behaviour_exptHc[iSubj]) \n",
    "    \n",
    "    # Note: accuracy has been multiplied by 100 so is in percentage\n",
    "        # Note: accuracy has been multiplied by 100 so is in percentage\n",
    "    allGaussLowAccuracyHc[iSubj] = np.mean(AllGaussLowHc[:,3]==AllGaussLowHc[:,4])\n",
    "    allGaussHighAccuracyHc[iSubj] = np.mean(AllGaussHighHc[:,3]==AllGaussHighHc[:,4])\n",
    "    distGaussLowAccuracyHc[iSubj] = np.mean(DistGaussLowHc[:,3]==DistGaussLowHc[:,4])\n",
    "    distGaussHighAccuracyHc[iSubj] = np.mean(DistGaussHighHc[:,3]==DistGaussHighHc[:,4])\n",
    "    AllTonesGaussianAccuracyHc[iSubj] = AccuracyHc[0,0]\n",
    "    OneDistGaussianAccuracyHc[iSubj] = AccuracyHc[2,0]\n",
    "    OneDistGaussianMeanExplainedHc[iSubj] = AccuracyHc[2,2]\n",
    "    OneDistGaussianSignalMeanExplainedHc[iSubj] = AccuracyHc[2,4]\n",
    "    OneDistOppSidebandLHAccuracyHc[iSubj] = AccuracyHc[4,0]\n",
    "    OneDistOppSidebandLHMeanExplainedHc[iSubj] = AccuracyHc[4,2]\n",
    "    OneDistOppSidebandLHSignalMeanExplainedHc[iSubj] = AccuracyHc[4,4]\n",
    "    OneDistOppSidebandHLAccuracyHc[iSubj] = AccuracyHc[5,0]\n",
    "    OneDistOppSidebandHLMeanExplainedHc[iSubj] = AccuracyHc[5,2]\n",
    "    OneDistOppSidebandHLSignalMeanExplainedHc[iSubj] = AccuracyHc[5,4]\n",
    "    TwoDistOppSidebandAccuracyHc[iSubj] = AccuracyHc[6,0]\n",
    "    TwoDistOppSidebandMeanExplainedHc[iSubj] = AccuracyHc[6,2]\n",
    "    BiasWithUnclearSigLowDistHc[iSubj,:] = BiasWithUnclearSigHc[:,:2]\n",
    "    BiasWithUnclearSigHighDistHc[iSubj,:] = BiasWithUnclearSigHc[:,2:]\n",
    "    \n",
    "    DistractorGaussianHc = np.concatenate((DistGaussLowHc,DistGaussHighHc),axis=0)\n",
    "    AllGaussianHc = np.concatenate((AllGaussLowHc, AllGaussHighHc),axis=0)\n",
    "    DistractorOppSidebandHc = np.concatenate((DistLowOtherHighHc,DistHighOtherLowHc))\n",
    "    DistractorSameSidebandHc = np.concatenate((DistLowOtherLowHc,DistHighOtherHighHc))\n",
    "    ThreeDistractorsHc = np.concatenate((DistThreeExtrHc,DistThreeExtrTwoSameHc),axis=0)\n",
    "    \n",
    "    noDistractorAccuracyHc[iSubj] = np.mean(AllGaussianHc[:,3]==AllGaussianHc[:,4])\n",
    "    oneDistractorAccuracyHc[iSubj] = np.mean(OneDistractorHc[:,3]==OneDistractorHc[:,4])\n",
    "    twoDistractorsAccuracyHc[iSubj] = np.mean(TwoDistractorsHc[:,3]==TwoDistractorsHc[:,4])\n",
    "    threeDistractorsAccuracyHc[iSubj] = np.mean(ThreeDistractorsHc[:,3]==ThreeDistractorsHc[:,4])\n",
    "    overallAccuracyHc[iSubj] = np.mean(trial_behaviour_exptHc[iSubj]==corrans_exptHc[iSubj])\n",
    "    \n",
    "    #allMeanTone_behvSubjArrayHc[iSubj,:,:], GaussMeanTone_behvSubjArrayHc[iSubj,:,:] = plotting(allGaussianHc, \n",
    "    #                                                                                            distractorGaussianHc, \n",
    "    #                                                                                            distLowOtherHighHc, \n",
    "    #                                                                                            distHighOtherLowHc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This computes the last row of the tables, where we are interested in the bias due to all the strategies\n",
    "as applicable to trials with one tone from gaussians and one tone as distractor.\n",
    "\"\"\"\n",
    "print((BiasWithUnclearSigLowDistHc[:,0]+BiasWithUnclearSigHighDistHc[:,1])/2) \n",
    "print((BiasWithUnclearSigLowDistHc[:,1]+BiasWithUnclearSigHighDistHc[:,0])/2)\n",
    "print((BiasWithUnclearSigLowDistHc[:,1]+BiasWithUnclearSigHighDistHc[:,0])*100/\n",
    "      (BiasWithUnclearSigLowDistHc[:,0]+BiasWithUnclearSigHighDistHc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = 0.1\n",
    "lowMu = 2.55\n",
    "highMu = 2.85\n",
    "    \n",
    "def BehaviourGivenOneBackground(trial_tones_expt,\n",
    "                                trial_behaviour_expt,\n",
    "                                trial_tone_cat_expt, \n",
    "                                corrans_expt):\n",
    "    unique_tones = np.unique(trial_tones_expt)\n",
    "\n",
    "    tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "    for i_tone in range(len(unique_tones)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(trial_behaviour_expt[trial_tones_expt[:,0]\\\n",
    "                                                           ==unique_tones[i_tone]])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(trial_behaviour_expt[trial_tones_expt[:,1]\\\n",
    "                                                           ==unique_tones[i_tone]])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(trial_behaviour_expt[trial_tones_expt[:,2]\\\n",
    "                                                           ==unique_tones[i_tone]])\n",
    "\n",
    "    #influence1, = plt.plot(np.log10(unique_tones), tone1_prob_behavior, label = 'Influence of Tone 1')\n",
    "    #influence2, = plt.plot(np.log10(unique_tones), tone2_prob_behavior, label = 'Influence of Tone 2')\n",
    "    #influence3, = plt.plot(np.log10(unique_tones), tone3_prob_behavior, label = 'Influence of Tone 3')\n",
    "    #influence, = plt.plot(np.log10(unique_tones), (tone1_prob_behavior+tone2_prob_behavior+tone3_prob_behavior)/3,\n",
    "                           #'k', label = 'Average Influence')\n",
    "\n",
    "    influence = (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3\n",
    "    mnTones = np.mean(trial_tones_expt,1)\n",
    "    counts, bins = np.histogram(mnTones)\n",
    "    binID = np.digitize(mnTones,bins)\n",
    "    un = np.unique(binID)\n",
    "\n",
    "    mnR = np.zeros(len(un))\n",
    "    for idx, val in enumerate(un):\n",
    "        mnR[idx] = np.mean(trial_behaviour_expt[binID == val])\n",
    "\n",
    "    tempCat = np.copy(trial_tone_cat_expt)\n",
    "\n",
    "    tempCat[tempCat == 1] = -1 \n",
    "    tempCat[tempCat == 2] = 1\n",
    "\n",
    "    sum2 = np.sum(tempCat,1)\n",
    "\n",
    "    un2 = np.unique(sum2)\n",
    "\n",
    "    mnR2 = np.zeros(len(un2))\n",
    "    for idx, val in enumerate(un2):\n",
    "        mnR2[idx] = np.mean(trial_behaviour_expt[sum2 == val])    \n",
    "        \n",
    "    #Let's look only at stimuli where 2 were from one category, and one was a background\n",
    "\n",
    "    high_1B_idx = sum2 == 2\n",
    "    low_1B_idx = sum2 == -2\n",
    "\n",
    "    # For High:\n",
    "\n",
    "    hTemp = np.copy(trial_tone_cat_expt[high_1B_idx,:])\n",
    "\n",
    "    temp2 = np.zeros(len(hTemp))\n",
    "    temp3 = np.zeros(len(hTemp))\n",
    "\n",
    "    trialTonesTemp = np.copy(trial_tones_expt[high_1B_idx,:])\n",
    "    behaviorTemp = np.copy(trial_behaviour_expt[high_1B_idx])\n",
    "\n",
    "    for idx, val in enumerate(hTemp):\n",
    "        temp = np.where(hTemp[idx,:] == 0)    \n",
    "        others = np.where(hTemp[idx,:] != 0)    \n",
    "        temp2[idx] = np.log10(trialTonesTemp[idx,temp])\n",
    "        t = np.log10(trialTonesTemp[idx,others])\n",
    "        othersMean = np.mean(t)#(t[:,0]*t[:,1])**(1/2)    \n",
    "        temp3[idx] = (temp2[idx] - othersMean)\n",
    "\n",
    "    counts, highBins = np.histogram(temp2)\n",
    "    binID = np.digitize(temp2,highBins)\n",
    "    un = np.unique(binID)\n",
    "\n",
    "    mnRespHigh = np.zeros(len(un))\n",
    "    for idx, val in enumerate(un):\n",
    "        mnRespHigh[idx] = np.mean(behaviorTemp[binID == val])\n",
    "\n",
    "\n",
    "    # For Low:\n",
    "\n",
    "    lTemp = np.copy(trial_tone_cat_expt[low_1B_idx,:])\n",
    "\n",
    "    temp2 = np.zeros(len(lTemp))\n",
    "    temp3 = np.zeros(len(lTemp))\n",
    "\n",
    "    trialTonesTemp = np.copy(trial_tones_expt[low_1B_idx,:])\n",
    "    behaviorTemp = np.copy(trial_behaviour_expt[low_1B_idx])\n",
    "\n",
    "    for idx, val in enumerate(lTemp):\n",
    "        temp = np.where(lTemp[idx,:] == 0)\n",
    "        others = np.where(lTemp[idx,:] != 0)\n",
    "        temp2[idx] = np.log10(trialTonesTemp[idx,temp])   \n",
    "        t = np.log10(trialTonesTemp[idx,others])\n",
    "        othersMean = np.mean(t)\n",
    "        temp3[idx] = (temp2[idx] - othersMean)\n",
    "    # Now, to Z-score them:\n",
    "\n",
    "    #zScoreLow = np.copy((np.log10(temp2) - lowMu) / sig)\n",
    "    zScoreLow = np.copy(temp2)\n",
    "    #zScoreLow = np.copy(temp3)\n",
    "\n",
    "    counts, lowBins = np.histogram(zScoreLow)\n",
    "    binID = np.digitize(zScoreLow,lowBins)\n",
    "    un = np.unique(binID)\n",
    "\n",
    "    mnRespLow = np.zeros(len(un))\n",
    "    for idx, val in enumerate(un):\n",
    "        mnRespLow[idx] = np.mean(behaviorTemp[binID == val])\n",
    "\n",
    "    avgRespLow = np.mean(trial_behaviour_expt[sum2 == -3])  \n",
    "    avgRespHigh = np.mean(trial_behaviour_expt[sum2 == 3])   \n",
    "\n",
    "    return lowBins, highBins, mnRespLow, mnRespHigh, avgRespLow, avgRespHigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lowBins, highBins, \n",
    " mnRespLow, mnRespHigh, \n",
    " avgRespLow, avgRespHigh] = BehaviourGivenOneBackground(trial_tones_a12e,\n",
    "                                                        trial_behaviour_a12e,\n",
    "                                                        trial_tone_cat_a12e,\n",
    "                                                        corrans_a12e)\n",
    "\n",
    "[lowBinsLc, highBinsLc, \n",
    " mnRespLowLc, mnRespHighLc, \n",
    " avgRespLowLc, avgRespHighLc] = BehaviourGivenOneBackground(trial_tones_a12eLc,\n",
    "                                                            trial_behaviour_a12eLc,\n",
    "                                                            trial_tone_cat_a12eLc,\n",
    "                                                            corrans_a12eLc)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, sharex = True, sharey = True)\n",
    "axs[0].plot(lowBins,mnRespLow,'b-')\n",
    "axs[0].plot(lowMu, avgRespLow, 'bo')  \n",
    "axs[0].plot(lowBinsLc, mnRespLowLc, 'b--')\n",
    "axs[0].axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "axs[1].plot(highBins, mnRespHigh,'r-')\n",
    "axs[1].plot(highMu, avgRespHigh, 'ro')\n",
    "axs[1].plot(highBinsLc, mnRespHighLc, 'r--')\n",
    "axs[1].axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "fig = plt.figure()  # create a figure object\n",
    "ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "ax.plot(lowBins, mnRespLow-mnRespLowLc, 'b--')\n",
    "ax.plot(highBins, mnRespHigh-mnRespHighLc, 'r--')\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel('log10(frequency)')\n",
    "plt.ylabel('p(B_H)')\n",
    "ax.legend(['Two tones are low','Two tones are high'])\n",
    "#plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/ChangeInBehaviourWithBackground_735a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def computeAggregate():\n",
    "\n",
    "    AllTonesGaussianMeanExplained = AllTonesGaussianAccuracy\n",
    "    TwoDistOppSidebandSignalMeanExplained = 100-TwoDistOppSidebandMeanExplained\n",
    "    \n",
    "    FullBayesModel = []\n",
    "    PbackLesionModel = []\n",
    "    VotingModel = []\n",
    "    \n",
    "    \"\"\"\n",
    "    Mean values of model parameters\n",
    "    \"\"\"\n",
    "    print('Complete model')\n",
    "    print('mu low', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,0]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,0])/np.sqrt(30))\n",
    "    print('mu high', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,1]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,1])/np.sqrt(30))\n",
    "    print('sigma distribution', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,2]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,2])/np.sqrt(30))\n",
    "    print('sigma sensory', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,3]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,3])/np.sqrt(30))\n",
    "    print('pback', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,4]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,4])/np.sqrt(30))\n",
    "    print('plow', np.mean(mleParams_expt[AllTonesGaussianAccuracy>=70,5]),np.std(mleParams_expt[AllTonesGaussianAccuracy>=70,5])/np.sqrt(30))\n",
    "    \n",
    "    print('Lesion model')\n",
    "    print('mu low', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,0]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,0])/np.sqrt(30))\n",
    "    print('mu high', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,1]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,1])/np.sqrt(30))\n",
    "    print('sigma distribution', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,2]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,2])/np.sqrt(30))\n",
    "    print('sigma sensory', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,3]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,3])/np.sqrt(30))\n",
    "    print('pback', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,4]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,4])/np.sqrt(30))\n",
    "    print('plow', np.mean(lesionParams_expt[AllTonesGaussianAccuracy>=70,5]),np.std(lesionParams_expt[AllTonesGaussianAccuracy>=70,5])/np.sqrt(30))\n",
    "    \n",
    "    print('Voting model')\n",
    "    print('decision boundary', np.mean(votingParams_expt[AllTonesGaussianAccuracy>=70,0]), np.std(votingParams_expt[AllTonesGaussianAccuracy>=70,0])/np.sqrt(30))\n",
    "    print('sigma distribution', np.mean(votingParams_expt[AllTonesGaussianAccuracy>=70,1]), np.std(votingParams_expt[AllTonesGaussianAccuracy>=70,1])/np.sqrt(30))\n",
    "    \n",
    "    \"\"\"\n",
    "    Accuracy vs % of data explained using strategies for all subjects in the no context case\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure(tight_layout=True)  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.hist(AllTonesGaussianAccuracy, bins=[50,60,70,80,90,100])\n",
    "    ax.set_ylim(0,30)\n",
    "    ax.set_xlim(50,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Accuracy',fontsize=15)\n",
    "    plt.ylabel('Number of participants',fontsize=15)\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/SubjectsToExclude')\n",
    "    \n",
    "    fig = plt.figure(tight_layout=True)  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot([0,1,2,3],[noDistractorAccuracy[AllTonesGaussianAccuracy>=70]*100,\n",
    "                       oneDistractorAccuracy[AllTonesGaussianAccuracy>=70]*100,\n",
    "                       twoDistractorsAccuracy[AllTonesGaussianAccuracy>=70]*100,\n",
    "                       threeDistractorsAccuracy[AllTonesGaussianAccuracy>=70]*100],'k.')\n",
    "    #ax.errorbar(x=[0,1,2,3],y=[np.median(noDistractorAccuracy[AllTonesGaussianAccuracy>=70])*100,\n",
    "    #                           np.median(oneDistractorAccuracy[AllTonesGaussianAccuracy>=70])*100,\n",
    "    #                           np.median(twoDistractorsAccuracy[AllTonesGaussianAccuracy>=70])*100,\n",
    "    #                           np.median(threeDistractorsAccuracy[AllTonesGaussianAccuracy>=70])*100],\n",
    "    #            yerr = [np.std(noDistractorAccuracy[AllTonesGaussianAccuracy>=70])/np.sqrt(30)*100,\n",
    "    #                    np.std(oneDistractorAccuracy[AllTonesGaussianAccuracy>=70])/np.sqrt(30)*100,\n",
    "    #                    np.std(twoDistractorsAccuracy[AllTonesGaussianAccuracy>=70])/np.sqrt(30)*100,\n",
    "    #                    np.std(threeDistractorsAccuracy[AllTonesGaussianAccuracy>=70])/np.sqrt(30)*100],\n",
    "    #            marker='.',color='r',linestyle='')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(-0.5,4.5)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Number of Distractors',fontsize=15)\n",
    "    plt.ylabel('Accuracy',fontsize=15)\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/AccuracyVsNoOfDistractors')    \n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10),tight_layout=True)  # create a figure object\n",
    "    ax = fig.add_subplot(3,5,1)\n",
    "    ax.plot(AllTonesGaussianAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            AllTonesGaussianMeanExplained[AllTonesGaussianAccuracy>=70], 'b.',markersize=12)\n",
    "    ax.plot(AllTonesGaussianAccuracy[[24]],AllTonesGaussianMeanExplained[[24]],'ks',markersize=12)\n",
    "    ax.set_ylim(0,100); ax.set_xlim(10,100); plt.xticks([]); plt.yticks(fontsize=20)\n",
    "    ax2 = fig.add_subplot(3,5,6)\n",
    "    ax2.plot(AllTonesGaussianAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            AllTonesGaussianMeanExplained[AllTonesGaussianAccuracy>=70], 'r.',markersize=12)\n",
    "    ax2.plot(AllTonesGaussianAccuracy[[3,7,20]],AllTonesGaussianMeanExplained[[3,7,20]],'k*',markersize=12)\n",
    "    ax2.set_ylim(0,100); ax2.set_xlim(10,100); plt.xticks([]); plt.yticks(fontsize=20)\n",
    "    ax3 = fig.add_subplot(3,5,11)\n",
    "    ax3.plot(AllTonesGaussianAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            AllTonesGaussianMeanExplained[AllTonesGaussianAccuracy>=70], 'g.',markersize=12)\n",
    "    ax3.plot(AllTonesGaussianAccuracy[[14,28,29]],AllTonesGaussianMeanExplained[[14,28,29]],'kP',markersize=14)\n",
    "    ax3.set_ylim(0,100); ax3.set_xlim(10,100); plt.xticks(fontsize=20); plt.yticks(fontsize=20)    \n",
    "    ax = fig.add_subplot(3,5,2)\n",
    "    ax.plot(OneDistGaussianAccuracy[AllTonesGaussianAccuracy>=70], \n",
    "            OneDistGaussianMeanExplained[AllTonesGaussianAccuracy>=70], 'b.',markersize=12)\n",
    "    ax.plot(OneDistGaussianAccuracy[[24]],OneDistGaussianMeanExplained[[24]],'ks',markersize=12)\n",
    "    ax.set_ylim(0,100); ax.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax2 = fig.add_subplot(3,5,7)\n",
    "    ax2.plot(OneDistGaussianAccuracy[AllTonesGaussianAccuracy>=70], \n",
    "            OneDistGaussianSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'r.',markersize=12)\n",
    "    ax2.plot(OneDistGaussianAccuracy[[3,7,20]],OneDistGaussianSignalMeanExplained[[3,7,20]],'k*',markersize=12)\n",
    "    ax2.set_ylim(0,100); ax2.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax3 = fig.add_subplot(3,5,12)\n",
    "    ax3.plot(OneDistGaussianAccuracy[AllTonesGaussianAccuracy>=70], \n",
    "            OneDistGaussianSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'g.',markersize=12)\n",
    "    ax3.plot(OneDistGaussianAccuracy[[14,28,29]],OneDistGaussianSignalMeanExplained[[14,28,29]],'kP',markersize=14)\n",
    "    ax3.set_ylim(0,100); ax3.set_xlim(10,100); plt.xticks(fontsize=20); plt.yticks([])  \n",
    "    ax = fig.add_subplot(3,5,3)\n",
    "    ax.plot(OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70], \n",
    "            OneDistOppSidebandLHMeanExplained[AllTonesGaussianAccuracy>=70], 'b.',markersize=12)\n",
    "    ax.plot(OneDistOppSidebandLHAccuracy[[24]], \n",
    "            OneDistOppSidebandLHMeanExplained[[24]], 'ks',markersize=12)\n",
    "    ax.set_ylim(0,100); ax.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax2 = fig.add_subplot(3,5,8)\n",
    "    ax2.plot(OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            OneDistOppSidebandLHSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'r.',markersize=12)\n",
    "    ax2.plot(OneDistOppSidebandLHAccuracy[[3,7,20]], \n",
    "            OneDistOppSidebandLHSignalMeanExplained[[3,7,20]], 'k*',markersize=12)\n",
    "    ax2.set_ylim(0,100); ax2.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax3 = fig.add_subplot(3,5,13)\n",
    "    ax3.plot(OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            OneDistOppSidebandLHSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'g.',markersize=12)\n",
    "    ax3.plot(OneDistOppSidebandLHAccuracy[[14,28,29]], \n",
    "            OneDistOppSidebandLHSignalMeanExplained[[14,28,29]], 'kP',markersize=14)\n",
    "    ax3.set_ylim(0,100); ax3.set_xlim(10,100); plt.xticks(fontsize=20); plt.yticks([])    \n",
    "    ax = fig.add_subplot(3,5,4)\n",
    "    ax.plot(OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            OneDistOppSidebandHLMeanExplained[AllTonesGaussianAccuracy>=70], 'b.',markersize=12)\n",
    "    ax.plot(OneDistOppSidebandHLAccuracy[[24]], \n",
    "            OneDistOppSidebandHLMeanExplained[[24]], 'ks',markersize=12)\n",
    "    ax.set_ylim(0,100); ax.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax2 = fig.add_subplot(3,5,9)\n",
    "    ax2.plot(OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            OneDistOppSidebandHLSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'r.',markersize=12)\n",
    "    ax2.plot(OneDistOppSidebandHLAccuracy[[3,7,20]], \n",
    "            OneDistOppSidebandHLSignalMeanExplained[[3,7,20]], 'k*',markersize=12)\n",
    "    ax2.set_ylim(0,100); ax2.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax3 = fig.add_subplot(3,5,14)\n",
    "    ax3.plot(OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            OneDistOppSidebandHLSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'g.',markersize=12)\n",
    "    ax3.plot(OneDistOppSidebandHLAccuracy[[14,28,29]], \n",
    "            OneDistOppSidebandHLSignalMeanExplained[[14,28,29]], 'kP',markersize=14)\n",
    "    ax3.set_ylim(0,100); ax3.set_xlim(10,100); plt.xticks(fontsize=20); plt.yticks([])    \n",
    "    ax = fig.add_subplot(3,5,5)\n",
    "    ax.plot(TwoDistOppSidebandAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            TwoDistOppSidebandMeanExplained[AllTonesGaussianAccuracy>=70], 'b.',markersize=12)\n",
    "    ax.plot(TwoDistOppSidebandAccuracy[[24]], TwoDistOppSidebandMeanExplained[[24]], 'ks',markersize=12)\n",
    "    ax.set_ylim(0,100); ax.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax2 = fig.add_subplot(3,5,10)\n",
    "    ax2.plot(TwoDistOppSidebandAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            TwoDistOppSidebandSignalMeanExplained[AllTonesGaussianAccuracy>=70], 'r.',markersize=12)\n",
    "    ax2.plot(TwoDistOppSidebandAccuracy[[3,7,20]], TwoDistOppSidebandSignalMeanExplained[[3,7,20]], 'k*',markersize=12)\n",
    "    ax2.set_ylim(0,100); ax2.set_xlim(10,100); plt.xticks([]); plt.yticks([])\n",
    "    ax3 = fig.add_subplot(3,5,15)\n",
    "    ax3.plot(TwoDistOppSidebandAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "            TwoDistOppSidebandMeanExplained[AllTonesGaussianAccuracy>=70], 'g.',markersize=12)\n",
    "    ax3.plot(TwoDistOppSidebandAccuracy[[14,28,29]], TwoDistOppSidebandMeanExplained[[14,28,29]], 'kP',markersize=14)\n",
    "    ax3.set_ylim(0,100); ax3.set_xlim(0,100); plt.xticks(fontsize=20); plt.yticks([])\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/AccuracyAllStrategies') \n",
    "    \n",
    "    fig = plt.figure(figsize=(7,5),tight_layout=True)  # create a figure object\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot((BiasWithUnclearSigLowDist[AllTonesGaussianAccuracy>=70,0]+\n",
    "             BiasWithUnclearSigHighDist[AllTonesGaussianAccuracy>=70,1])/2, \n",
    "            (BiasWithUnclearSigLowDist[AllTonesGaussianAccuracy>=70,1]+\n",
    "             BiasWithUnclearSigHighDist[AllTonesGaussianAccuracy>=70,0])/2,'b.')\n",
    "    ax.plot((BiasWithUnclearSigLowDist[[24],0]+BiasWithUnclearSigHighDist[[24],1])/2, \n",
    "            (BiasWithUnclearSigLowDist[[24],1]+BiasWithUnclearSigHighDist[[24],0])/2,'ks',markersize=12)\n",
    "    ax.plot((BiasWithUnclearSigLowDist[[3,7,20],0]+BiasWithUnclearSigHighDist[[3,7,20],1])/2, \n",
    "            (BiasWithUnclearSigLowDist[[3,7,20],1]+BiasWithUnclearSigHighDist[[3,7,20],0])/2,'k*',markersize=12)\n",
    "    ax.plot((BiasWithUnclearSigLowDist[[14,28,29],0]+BiasWithUnclearSigHighDist[[14,28,29],1])/2, \n",
    "            (BiasWithUnclearSigLowDist[[14,28,29],1]+BiasWithUnclearSigHighDist[[14,28,29],0])/2,'kP',markersize=14)\n",
    "    ax.set_xlabel('% of data when distractors lie on same side',fontsize=15)\n",
    "    ax.set_ylabel('% of data when distractors lie on opposite side',fontsize=15)\n",
    "    ax.set_ylim(0,50); ax.set_xlim(50,100); plt.xticks(fontsize=15); plt.yticks(fontsize=15)\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/BiasWithUnclearSignalData') \n",
    "    \n",
    "    \"\"\"\n",
    "    Accuracy vs % of data explained using different models in the no context case\n",
    "    \"\"\"\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(mleParams_expt[AllTonesGaussianAccuracy>=70,4],\n",
    "            (OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "             OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "             TwoDistOppSidebandAccuracy[AllTonesGaussianAccuracy>=70])/3,'k.')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,1)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('pback',fontsize=15)\n",
    "    plt.ylabel('Accuracy',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/pbackVsAccuracy')\n",
    "    \"\"\"\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(overallAccuracy[AllTonesGaussianAccuracy>=70]*100, \n",
    "            negll_fullBayes[AllTonesGaussianAccuracy>=70,0],'k.')\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Accuracy',fontsize=15)\n",
    "    plt.ylabel('-ve LogLikelihood of Full Bayesian Model',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/rawDataAnalysis/BayesianModelLikelihoodAgainstAccuracy.png')\n",
    "    \"\"\"\n",
    "        \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,0], \n",
    "            negll_lesionBayes[AllTonesGaussianAccuracy>=70,0],color='orange',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,1],\n",
    "            negll_lesionBayes[AllTonesGaussianAccuracy>=70,1],color='lightseagreen',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,2],\n",
    "            negll_lesionBayes[AllTonesGaussianAccuracy>=70,2],color='red',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,3],\n",
    "            negll_lesionBayes[AllTonesGaussianAccuracy>=70,3],color='black',marker='.',linestyle='None')\n",
    "    ax.plot(np.arange(100),np.arange(100),'k--')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.set_ylim(0,140)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('-ve LogLikelihood of Bayesian Model',fontsize=15)\n",
    "    plt.ylabel('-ve LogLikelihood of Lesion Model',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/LikelihoodFullVsLesion.png')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,0], \n",
    "            negll_votingModel[AllTonesGaussianAccuracy>=70,0],color='orange',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,1],\n",
    "            negll_votingModel[AllTonesGaussianAccuracy>=70,1],color='lightseagreen',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,2],\n",
    "            negll_votingModel[AllTonesGaussianAccuracy>=70,2],color='red',marker='.',linestyle='None')\n",
    "    ax.plot(negll_fullBayes[AllTonesGaussianAccuracy>=70,3],\n",
    "            negll_votingModel[AllTonesGaussianAccuracy>=70,3],color='black',marker='.',linestyle='None')\n",
    "    ax.plot(np.arange(100),np.arange(100),'k--')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.set_ylim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('-ve LogLikelihood of Bayesian Model',fontsize=15)\n",
    "    plt.ylabel('-ve LogLikelihood of Voting Model',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/LikelihoodFullVsVoting.png')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1) \n",
    "    mycmap=cm.Greys\n",
    "    ax.scatter((OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70]\n",
    "                +OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70])/2,\n",
    "                TwoDistOppSidebandAccuracy[AllTonesGaussianAccuracy>=70],\n",
    "                c=mycmap(mleParams_expt[AllTonesGaussianAccuracy>=70,4]),s=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Accuracy One Sideband Distractor',fontsize=15)\n",
    "    plt.ylabel('Accuracy Two Sideband Distractors',fontsize=15)\n",
    "    plt.tight_layout() \n",
    "    cbar = plt.colorbar(plt.cm.ScalarMappable(cmap=mycmap))\n",
    "    cbar.ax.tick_params(labelsize=15) \n",
    "    plt.savefig('figures/FromProlific/rawDataAnalysis/2dColormapPback.png')\n",
    "\n",
    "    \"\"\"\n",
    "    Plotting average p(B_H|mean tone) and average p(B_H|mean of signal tones)\n",
    "    \"\"\"\n",
    "    _, bin_edges = np.histogram((OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "                                 OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "                                 OneDistGaussianAccuracy[AllTonesGaussianAccuracy>=70])/3,bins=4)\n",
    "    subjectBins = np.digitize((OneDistOppSidebandHLAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "                               OneDistOppSidebandLHAccuracy[AllTonesGaussianAccuracy>=70]+\n",
    "                               OneDistGaussianAccuracy[AllTonesGaussianAccuracy>=70])/3, bin_edges)\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,0,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,1,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,2,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,3,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/AllMeanToneProbHighBadSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,0,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,1,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,2,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,3,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/AllMeanToneProbHighMediocreSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,0,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,1,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,2,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,3,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/AllMeanToneProbHighGoodSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,0,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,1,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,2,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==1,3,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/GaussMeanToneProbHighBadSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,0,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,1,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,2,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins==2,3,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/GaussMeanToneProbHighMediocreSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,0,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,1,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,2,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArray[AllTonesGaussianAccuracy>=70][subjectBins>2,3,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/GaussMeanToneProbHighGoodSubj')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Accuracy vs % of data explained using strategies for all subjects in the long context case biased towards low\n",
    "\"\"\"\n",
    "\n",
    "def computeAggregateBiasedLong():\n",
    "    AllTonesGaussianMeanExplainedLc = np.copy(AllTonesGaussianAccuracyLc)\n",
    "    TwoDistOppSidebandSignalMeanExplainedLc = 100-TwoDistOppSidebandMeanExplainedLc\n",
    "    AllTonesGaussianMeanExplainedHc = np.copy(AllTonesGaussianAccuracyHc)\n",
    "    TwoDistOppSidebandSignalMeanExplainedHc = 100-TwoDistOppSidebandMeanExplainedHc\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot([0,1,2,3],[noDistractorAccuracyLc*100,\n",
    "                       oneDistractorAccuracyLc*100,\n",
    "                       twoDistractorsAccuracyLc*100,\n",
    "                       threeDistractorsAccuracyLc*100],'k.')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(-0.5,4.5)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Number of Distractors',fontsize=15)\n",
    "    plt.ylabel('Accuracy',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(AllTonesGaussianAccuracyLc, AllTonesGaussianMeanExplainedLc, 'b.',markersize=18)\n",
    "    ax.plot(AllTonesGaussianAccuracyHc, AllTonesGaussianMeanExplainedHc, 'skyblue',\n",
    "            marker='.',markersize=18,linestyle = 'None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/AllTonesGaussAccuracyMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(AllTonesGaussianAccuracyLc, AllTonesGaussianMeanExplainedLc, 'r.',markersize=18)\n",
    "    ax.plot(AllTonesGaussianAccuracyHc, AllTonesGaussianMeanExplainedHc, 'lightsalmon',\n",
    "            marker='.',markersize=18,linestyle = 'None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/AllTonesGaussAccuracySignalMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(AllTonesGaussianAccuracyLc, AllTonesGaussianMeanExplainedLc, 'g.',markersize=18)\n",
    "    ax.plot(AllTonesGaussianAccuracyHc, AllTonesGaussianMeanExplainedHc, 'limegreen',\n",
    "            marker='.',markersize=18,linestyle = 'None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/AllTonesGaussAccuracyVotingStrategy')    \n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistGaussianAccuracyLc, OneDistGaussianMeanExplainedLc, 'b.', markersize=18,)\n",
    "    ax.plot(OneDistGaussianAccuracyHc, OneDistGaussianMeanExplainedHc, 'skyblue',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistGaussAccuracyMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistGaussianAccuracyLc, OneDistGaussianSignalMeanExplainedLc, 'r.', markersize=18)\n",
    "    ax.plot(OneDistGaussianAccuracyHc, OneDistGaussianSignalMeanExplainedHc, 'lightsalmon',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistGaussAccuracySignalMeanStrategy')  \n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistGaussianAccuracyLc, OneDistGaussianSignalMeanExplainedLc, 'g.', markersize=18)\n",
    "    ax.plot(OneDistGaussianAccuracyHc, OneDistGaussianSignalMeanExplainedHc, 'limegreen',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistGaussAccuracyVotingStrategy')       \n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyLc, OneDistOppSidebandLHMeanExplainedLc, 'b.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyHc, OneDistOppSidebandLHMeanExplainedHc, 'skyblue',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandLHAccuracyMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyLc, OneDistOppSidebandLHSignalMeanExplainedLc, 'r.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyHc, OneDistOppSidebandLHSignalMeanExplainedHc, 'lightsalmon',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandLHAccuracySignalMeanStrategy') \n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyLc, OneDistOppSidebandLHSignalMeanExplainedLc, 'g.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandLHAccuracyHc, OneDistOppSidebandLHSignalMeanExplainedHc, 'limegreen',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandLHAccuracyVotingStrategy')      \n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyLc, OneDistOppSidebandHLMeanExplainedLc, 'b.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyHc, OneDistOppSidebandHLMeanExplainedHc, 'skyblue',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandHLAccuracyMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyLc, OneDistOppSidebandHLSignalMeanExplainedLc, 'r.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyHc, OneDistOppSidebandHLSignalMeanExplainedHc, 'lightsalmon',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandHLAccuracySignalMeanStrategy')  \n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyLc, OneDistOppSidebandHLSignalMeanExplainedLc, 'g.', markersize=18)\n",
    "    ax.plot(OneDistOppSidebandHLAccuracyHc, OneDistOppSidebandHLSignalMeanExplainedHc, 'limegreen',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.yticks([])\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/OneDistOppSidebandHLAccuracyVotingStrategy')      \n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(TwoDistOppSidebandAccuracyLc, TwoDistOppSidebandMeanExplainedLc, 'b.', markersize=18)\n",
    "    ax.plot(TwoDistOppSidebandAccuracyHc, TwoDistOppSidebandMeanExplainedHc, 'skyblue', \n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/TwoDistOppSidebandAccuracyMeanStrategy')\n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(TwoDistOppSidebandAccuracyLc, TwoDistOppSidebandSignalMeanExplainedLc, 'r.', markersize=18)\n",
    "    ax.plot(TwoDistOppSidebandAccuracyHc, TwoDistOppSidebandSignalMeanExplainedHc, 'lightsalmon', \n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/TwoDistOppSidebandAccuracySignalMeanStrategy')   \n",
    "    \n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    ax.plot(TwoDistOppSidebandAccuracyLc, TwoDistOppSidebandMeanExplainedLc, 'g.', markersize=18)\n",
    "    ax.plot(TwoDistOppSidebandAccuracyHc, TwoDistOppSidebandMeanExplainedHc, 'limegreen',\n",
    "            marker='.', markersize=18, linestyle='None')\n",
    "    ax.set_ylim(0,100)\n",
    "    ax.set_xlim(0,100)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/TwoDistOppSidebandAccuracyVotingStrategy')    \n",
    "\n",
    "    \"\"\"\n",
    "    Plotting average p(B_H|mean tone) and average p(B_H|mean of signal tones)\n",
    "    \"\"\"\n",
    "    _, bin_edges = np.histogram((OneDistOppSidebandHLAccuracyLc+OneDistOppSidebandLHAccuracyLc)/2,bins=3)\n",
    "    subjectBins = np.digitize((OneDistOppSidebandHLAccuracyLc+OneDistOppSidebandLHAccuracyLc)/2, bin_edges)\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==1,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==1,0,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==1,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==1,1,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==1,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==1,2,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==1,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==1,3,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/AllMeanToneProbHighBadSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==2,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==2,0,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==2,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==2,1,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==2,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==2,2,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins==2,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins==2,3,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/AllMeanToneProbHighMediocreSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins>2,0,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins>2,0,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins>2,1,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins>2,1,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins>2,2,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins>2,2,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(allMeanTone_behvSubjArrayLc[subjectBins>2,3,:],axis=0), \n",
    "             yerr=np.std(allMeanTone_behvSubjArrayLc[subjectBins>2,3,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of all 3 tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/AllMeanToneProbHighGoodSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==1,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==1,0,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==1,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==1,1,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==1,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==1,2,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==1,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==1,3,:],axis=0)/sum(subjectBins==1))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/GaussMeanToneProbHighBadSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==2,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==2,0,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==2,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==2,1,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==2,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==2,2,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins==2,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins==2,3,:],axis=0)/sum(subjectBins==2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/GaussMeanToneProbHighMediocreSubj')\n",
    "\n",
    "    fig = plt.figure()  # create a figure object\n",
    "    ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins>2,0,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins>2,0,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins>2,1,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins>2,1,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins>2,2,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins>2,2,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.errorbar(logFreqBins, np.mean(GaussMeanTone_behvSubjArrayLc[subjectBins>2,3,:],axis=0), \n",
    "             yerr=np.std(GaussMeanTone_behvSubjArrayLc[subjectBins>2,3,:],axis=0)/sum(subjectBins>2))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Mean of tones drawn from the gaussian pdfs',fontsize=15)\n",
    "    plt.ylabel('p(B_H|mean(T))',fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/biasedLow_rawDataAnalysis/GaussMeanToneProbHighGoodSubj')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Executing both aggregate functions to generate plots\n",
    "\"\"\"\n",
    "computeAggregate()\n",
    "computeAggregateBiasedLong()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing for subject bias in the long context case\n",
    "\"\"\"\n",
    "labels = np.arange(20)\n",
    "iSubjNoandLow = np.delete(np.arange(33),[0,5,11,12,19,26,30,32])\n",
    "iSubjNoandLowandHigh = np.delete(np.arange(33),[0,1,2,5,9,11,12,19,23,26,28,30,32])\n",
    "iSubjLowandHigh = np.delete(np.arange(25),[0,1,7,18,22])\n",
    "biasNoContext = np.array([0.2,0,0.33,-0.12,0.23,-0.27,-0.1,0.43,0.33,0.21,0.16,\n",
    "                          0.52,0.2,0.09,0.21,0.27,0.31,0.33,0.095,0.02])\n",
    "biasLongContextLow = np.array([-0.3,-1.65,-0.49,-0.71,-0.24,-0.35,-0.42,0.05,-0.6,-0.81,\n",
    "                              0.026,-1.09,-0.83,-0.81,-0.63,-0.36,-0.39,-0.32,-0.54,-0.58])\n",
    "biasLongContextHigh = np.array([0.82,1,0.88,0.35,1.05,0.27,0.56,1.12,0.39,0.74,0.61,\n",
    "                                1.69,0.46,0.27,1.34,0.82,0.77,0.3,0.73,1.34])\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.4  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - width/2, OneDistOppSidebandHLAccuracy[iSubjNoandLowandHigh]/100-\n",
    "       OneDistOppSidebandLHAccuracy[iSubjNoandLowandHigh]/100, width, color='k')\n",
    "ax.bar(x + width/2, OneDistOppSidebandHLAccuracyLc[iSubjLowandHigh]/100-\n",
    "       OneDistOppSidebandLHAccuracyLc[iSubjLowandHigh]/100, width, color=[0.8,0.8,0.8])\n",
    "ax.bar(x + width/2, OneDistOppSidebandHLAccuracyHc/100-\n",
    "       OneDistOppSidebandLHAccuracyHc/100, width, color=[0.5,0.5,0.5])\n",
    "plt.xticks(fontsize=15,ticks=np.arange(0,len(labels),2),labels=np.arange(0,len(labels),2))\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Subjects',fontsize=15)\n",
    "plt.ylabel('Difference in accuracies',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/barplot=Difference in accuracies given one distractor from sidebands')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - width/2, allGaussLowAccuracy[iSubjNoandLowandHigh]-\n",
    "       allGaussHighAccuracy[iSubjNoandLowandHigh], width, color='k')\n",
    "ax.bar(x + width/2, allGaussLowAccuracyLc[iSubjLowandHigh]-\n",
    "       allGaussHighAccuracyLc[iSubjLowandHigh], width, color=[0.8,0.8,0.8])\n",
    "ax.bar(x + width/2, allGaussLowAccuracyHc-\n",
    "       allGaussHighAccuracyHc, width, color=[0.5,0.5,0.5])\n",
    "plt.xticks(fontsize=15,ticks=np.arange(0,len(labels),2),labels=np.arange(0,len(labels),2))\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Subjects',fontsize=15)\n",
    "plt.ylabel('Difference in accuracies',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/barplot=Difference in accuracies given all gauss tones')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x - width/2, distGaussHighAccuracy[iSubjNoandLowandHigh]-\n",
    "       distGaussLowAccuracy[iSubjNoandLowandHigh], width, color='k')\n",
    "ax.bar(x + width/2, distGaussHighAccuracyLc[iSubjLowandHigh]-\n",
    "       distGaussLowAccuracyLc[iSubjLowandHigh], width, color=[0.8,0.8,0.8])\n",
    "ax.bar(x + width/2, distGaussHighAccuracyHc-\n",
    "       distGaussLowAccuracyHc, width, color=[0.5,0.5,0.5])\n",
    "plt.xticks(fontsize=15,ticks=np.arange(0,len(labels),2),labels=np.arange(0,len(labels),2))\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Subjects',fontsize=15)\n",
    "plt.ylabel('Difference in accuracies',fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/barplot=Difference in accuracies given one distractor from gaussian region')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.abs(allGaussLowAccuracyLc[iSubjLowandHigh]-allGaussHighAccuracyLc[iSubjLowandHigh]-\n",
    "        (allGaussLowAccuracy[iSubjNoandLowandHigh]-allGaussHighAccuracy[iSubjNoandLowandHigh])),\n",
    "        -biasLongContextLow+biasNoContext,'k.')\n",
    "ax.plot(np.abs(allGaussLowAccuracyHc-allGaussHighAccuracyHc-\n",
    "        (allGaussLowAccuracy[iSubjNoandLowandHigh]-allGaussHighAccuracy[iSubjNoandLowandHigh])),\n",
    "        biasLongContextHigh-biasNoContext,'k.')\n",
    "plt.xlabel('Difference in accuracies across contexts',fontsize=15)\n",
    "plt.ylabel('Difference in w0',fontsize=15)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/FromProlific/biasedLongContext_rawDataAnalysis/barplot=Difference in accuracies given one distractor from gaussian region')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBiasUsingCentralTones(trial_tones, trial_behaviour, corrans):\n",
    "    twoCentralTones = np.zeros((len(trial_behaviour),12))\n",
    "    \n",
    "    n_tones = len(trial_tones[0])\n",
    "    print(high_dist[0]+1.8*high_dist[1], \n",
    "          low_dist[0]-1.8*low_dist[1], \n",
    "          log_freq_seq_mid)\n",
    "    \n",
    "    iTwoCentralTones = 0;\n",
    "    \n",
    "    for i_trial in range(len(trial_tones)):\n",
    "        if ((int(log_freq_seq_mid-0.075<trial_tones[i_trial][0]<log_freq_seq_mid+0.075) +\n",
    "             int(log_freq_seq_mid-0.075<trial_tones[i_trial][1]<log_freq_seq_mid+0.075) +\n",
    "             int(log_freq_seq_mid-0.075<trial_tones[i_trial][2]<log_freq_seq_mid+0.075))==2):\n",
    "            \n",
    "            twoCentralTones[iTwoCentralTones,:3] = trial_tones[i_trial,:]\n",
    "            twoCentralTones[iTwoCentralTones,3] = trial_behaviour[i_trial]\n",
    "            twoCentralTones[iTwoCentralTones,4] = corrans[i_trial]\n",
    "            iTwoCentralTones += 1       \n",
    "            print(trial_tones[i_trial])\n",
    "            \n",
    "    return twoCentralTones[:iTwoCentralTones,:]\n",
    "\n",
    "for iSubj in range(len(trial_behaviour_expt)):\n",
    "    twoCentralTones = checkBiasUsingCentralTones(np.log10(trial_tones_expt[iSubj]),\n",
    "                                                trial_behaviour_expt[iSubj], \n",
    "                                                corrans_expt[iSubj])\n",
    "    \n",
    "    unique_tones = np.unique(twoCentralTones[:,:3])\n",
    "\n",
    "    tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "    tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "    for i_tone in range(len(unique_tones)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(twoCentralTones[twoCentralTones[:,0]\\\n",
    "                                                           ==unique_tones[i_tone],3])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(twoCentralTones[twoCentralTones[:,1]\\\n",
    "                                                           ==unique_tones[i_tone],3])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(twoCentralTones[twoCentralTones[:,2]\\\n",
    "                                                           ==unique_tones[i_tone],3])\n",
    "    plt.plot(unique_tones,tone1_prob_behaviour)\n",
    "    plt.plot(unique_tones,tone2_prob_behaviour)\n",
    "    plt.plot(unique_tones,tone3_prob_behaviour)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for initial plots showing how accuracy varies with frequency of distractors.\n",
    "Note that the code for fn. 'accuracies' was changed to create the following plots. \n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure()  # create a figure object\n",
    "ax = fig.add_subplot(1, 1, 1)  # create an axes object in the figure\n",
    "ax.errorbar(x=[0,1,2,3],y=[np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,4]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,0]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,3]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,7]*100)],\n",
    "        yerr=[np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,4]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,0]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,3]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,7]*100)/np.sqrt(30)],\n",
    "            color='k')\n",
    "ax.errorbar(x=[0,1,2,3],y=[np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,6]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,2]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,1]*100),\n",
    "         np.mean(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,5]*100)],\n",
    "        yerr=[np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,6]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,2]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,1]*100)/np.sqrt(30),\n",
    "         np.std(accuracyAcrossFreq[(accuracyAcrossFreq[:,0]+accuracyAcrossFreq[:,1])>=1.4,5]*100)/np.sqrt(30)],\n",
    "           color='r')\n",
    "ax.set_ylim(0,100)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Frequency',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/FromProlific/rawDataAnalysis/AccuracyVariationWithFrequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expt(params, pl_orig):\n",
    "    training_trials, dist_chosen, tone_kind, low_dist, high_dist = task(freq_seq = freqSeq, \n",
    "                                                                        n_trials = 20000, n_tones = 3, \n",
    "                                                                        p_back=0.3, p_low=pl_orig,\n",
    "                                                                        lm=2.55,hm=2.85,s=0.1)\n",
    "\n",
    "    all_trial_tones, all_trial_behaviour = generate_behaviour(training_trials, reps=1, n_tones=3, \n",
    "                                                              prob_back=params[4], prob_low=params[5], \n",
    "                                                              log_prior_params=params[:3], \n",
    "                                                              sigma_sensory=params[3])\n",
    "    return all_trial_tones, all_trial_behaviour, dist_chosen\n",
    "\n",
    "def sample_exptVoting(params,pl_orig):\n",
    "    training_trials, dist_chosen, tone_kind, low_dist, high_dist = task(freq_seq = freqSeq, \n",
    "                                                                        n_trials = 20000, n_tones = 3, \n",
    "                                                                        p_back=0.3, p_low=pl_orig,\n",
    "                                                                        lm=2.55,hm=2.85,s=0.1)\n",
    "\n",
    "    all_trial_tones, all_trial_behaviour = generate_behaviourVoting(training_trials, reps=1, n_tones=3, \n",
    "                                                                    decision_boundary=params[0],\n",
    "                                                                    sigma_sensory=params[1])\n",
    "    return all_trial_tones, all_trial_behaviour, dist_chosen\n",
    "\n",
    "def posteriorAgainstPercept(expt_Params):\n",
    "    [_,_,mle_LikelihoodLatentTonegivenHigh,\n",
    "    mle_LikelihoodLatentTonegivenLow,_,mle_posterior] = posterior_array(freq_input=log_freq_perceptFinelySampled,\n",
    "                                                                        n_tones=3,\n",
    "                                                                        p_back=expt_Params[4], \n",
    "                                                                        p_low=expt_Params[5],\n",
    "                                                                        log_prior=expt_Params[:3]) \n",
    "    \n",
    "    \n",
    "    mle_LikelihoodPerceptgivenHigh = np.zeros((len(log_freq_perceptFinelySampled),\n",
    "                                               len(log_freq_perceptFinelySampled),len(log_freq_perceptFinelySampled)))\n",
    "    mle_LikelihoodPerceptgivenLow = np.zeros((len(log_freq_perceptFinelySampled),\n",
    "                                              len(log_freq_perceptFinelySampled),len(log_freq_perceptFinelySampled)))\n",
    "\n",
    "    for itrue1 in range(len(log_freq_perceptFinelySampled)):\n",
    "        for itrue2 in range(len(log_freq_perceptFinelySampled)):\n",
    "            for itrue3 in range(len(log_freq_perceptFinelySampled)):\n",
    "                mle_probPerceptgivenLatentTones = Tones3dgrid([log_freq_perceptFinelySampled[itrue1],\n",
    "                                                               log_freq_perceptFinelySampled[itrue2],\n",
    "                                                               log_freq_perceptFinelySampled[itrue3]],\n",
    "                                                               freq_seq = log_freq_perceptFinelySampled,\n",
    "                                                               sigma=expt_Params[3])\n",
    "                mle_LikelihoodPerceptgivenHigh \\\n",
    "                += mle_probPerceptgivenLatentTones * mle_LikelihoodLatentTonegivenHigh[itrue1,itrue2,itrue3]\n",
    "                mle_LikelihoodPerceptgivenLow \\\n",
    "                += mle_probPerceptgivenLatentTones * mle_LikelihoodLatentTonegivenLow[itrue1,itrue2,itrue3]\n",
    "    mle_probHighgivenPercept = mle_LikelihoodPerceptgivenHigh*(1-expt_Params[5])/\\\n",
    "    (mle_LikelihoodPerceptgivenHigh*(1-expt_Params[5]) + mle_LikelihoodPerceptgivenLow*expt_Params[5])\n",
    "    return mle_probHighgivenPercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For three participants, overall high, medium and low accuracy show the following plots of how well the Bayesian\n",
    "model, voting model and null Bayesian model captures features in data.\n",
    "\"\"\"\n",
    "\n",
    "BayesRsquared = np.zeros((len(mleParams_expt),))\n",
    "NullRsquared = np.zeros((len(lesionParams_expt),))\n",
    "VotingRsquared = np.zeros((len(votingParams_expt),))\n",
    "\n",
    "for iSubj in [3]:#range(len(mleParams_expt)):\n",
    "    \n",
    "    plt.figure()    \n",
    "    \"\"\"\n",
    "    unique_tones, exptBhv_mean, exptBhv_std = plotting_pBHgivenT(subjectBehaviour = trial_behaviour_expt[iSubj], \n",
    "                                                                 subjectTones = trial_tones_expt[iSubj], \n",
    "                                                                 contextIndex = range(len(trial_behaviour_expt[iSubj])))\n",
    "    plt.errorbar(np.log10(unique_tones), exptBhv_mean, yerr=exptBhv_std/np.sqrt(3), color='black') \n",
    "\n",
    "    mleTrialTones, mleTrialBehaviour, mleCorrNum = sample_expt(params=mleParams_expt[iSubj],\n",
    "                                                               pl_orig=0.5)\n",
    "\n",
    "    mleUnique_tones, mleBhv_mean, mleBhv_std = plotting_pBHgivenT(subjectBehaviour = mleTrialBehaviour, \n",
    "                                                               subjectTones = mleTrialTones, \n",
    "                                                               contextIndex = range(len(mleTrialBehaviour)))\n",
    "    plt.errorbar(np.log10(mleUnique_tones), mleBhv_mean, yerr=mleBhv_std/np.sqrt(3), color='red')\n",
    "    \n",
    "    mleTrialTones, mleTrialBehaviour, mleCorrNum = sample_expt(params=[2.54,2.85,0.016,0.16,0.051,0.47],\n",
    "                                                               pl_orig=0.5)\n",
    "    \n",
    "    mleUnique_tones, mleBhv_mean, mleBhv_std = plotting_pBHgivenT(subjectBehaviour = mleTrialBehaviour, \n",
    "                                                               subjectTones = mleTrialTones, \n",
    "                                                               contextIndex = range(len(mleTrialBehaviour)))\n",
    "    plt.errorbar(np.log10(mleUnique_tones), mleBhv_mean, yerr=mleBhv_std/np.sqrt(3), color='blue')\n",
    "    \"\"\"\n",
    "    #lesionTrialTones, lesionTrialBehaviour, lesionCorrNum = sample_expt(params=lesionParams_expt[iSubj],\n",
    "    #                                                                    pl_orig=0.5)\n",
    "\n",
    "    #unique_tones, lesionBhv_mean, lesionBhv_std = plotting_pBHgivenT(subjectBehaviour = lesionTrialBehaviour, \n",
    "    #                                                                 subjectTones = lesionTrialTones, \n",
    "    #                                                                 contextIndex = range(len(lesionTrialBehaviour)))\n",
    "    #plt.errorbar(np.log10(unique_tones), lesionBhv_mean, yerr=lesionBhv_std/np.sqrt(3), color='orange')\n",
    "\n",
    "    #votingTrialTones, votingTrialBehaviour, votingCorrNum = sample_exptVoting(params=votingParams_expt[iSubj],\n",
    "    #                                                                          pl_orig=0.5)\n",
    "\n",
    "    #unique_tones, votingBhv_mean, votingBhv_std = plotting_pBHgivenT(subjectBehaviour = votingTrialBehaviour, \n",
    "    #                                                                 subjectTones = votingTrialTones, \n",
    "    #                                                                 contextIndex = range(len(votingTrialBehaviour)))\n",
    "    #plt.errorbar(np.log10(unique_tones), votingBhv_mean, yerr=votingBhv_std/np.sqrt(3), color='dodgerblue') \n",
    "    \n",
    "    plt.xlabel('Tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|T)',fontsize=15)\n",
    "    plt.xticks(ticks=np.arange(1.9,3.6,0.3), labels=np.around(10**np.arange(1.9,3.6,0.3),2),fontsize=13)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/experimenter=2099_cleaner_modelfit.png')\n",
    "    \n",
    "    #BayesRsquared[iSubj] = psuedoRsquared(fitY=mleBhv_mean,\n",
    "    #                                        exptY=exptBhv_mean,\n",
    "    #                                        exptSem=exptBhv_std/np.sqrt(3))\n",
    "    #NullRsquared[iSubj] = psuedoRsquared(fitY=lesionBhv_mean,\n",
    "    #                                     exptY=exptBhv_mean,\n",
    "    #                                     exptSem=exptBhv_std/np.sqrt(3))\n",
    "    #VotingRsquared[iSubj] = psuedoRsquared(fitY=votingBhv_mean,\n",
    "    #                                     exptY=exptBhv_mean,\n",
    "    #                                     exptSem=exptBhv_std/np.sqrt(3))\n",
    "    log_freq_perceptFinelySampled=np.arange(0.6,4.7,0.02)\n",
    "    \"\"\"\n",
    "    Comparing posteriors obtained using original mle parameters, and those derived from min and max \n",
    "    (sigma_s, sigma_ss) ranges.\n",
    "    \"\"\"\n",
    "    origMLE_probHighgivenPercept = posteriorAgainstPercept([2.54,2.85,0.016,0.16,0.05,0.47])\n",
    "    \n",
    "    tone1_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "    tone2_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "    tone3_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "\n",
    "    for i_tone in range(len(log_freq_perceptFinelySampled)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(origMLE_probHighgivenPercept[i_tone,:,:])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(origMLE_probHighgivenPercept[:,i_tone,:])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(origMLE_probHighgivenPercept[:,:,i_tone])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(log_freq_perceptFinelySampled, (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3, \n",
    "             color='k')\n",
    "    plt.show()\n",
    "    \n",
    "    minMLE_probHighgivenPercept = posteriorAgainstPercept(mleParams_expt[iSubj,:])\n",
    "    \n",
    "    tone1_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "    tone2_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "    tone3_prob_behaviour = np.zeros((len(log_freq_perceptFinelySampled)))\n",
    "\n",
    "    for i_tone in range(len(log_freq_perceptFinelySampled)):\n",
    "        tone1_prob_behaviour[i_tone] = np.mean(minMLE_probHighgivenPercept[i_tone,:,:])\n",
    "        tone2_prob_behaviour[i_tone] = np.mean(minMLE_probHighgivenPercept[:,i_tone,:])\n",
    "        tone3_prob_behaviour[i_tone] = np.mean(minMLE_probHighgivenPercept[:,:,i_tone])\n",
    "        \n",
    "    plt.plot(log_freq_perceptFinelySampled, (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3, \n",
    "             'k--')\n",
    "    plt.xlabel('Tones',fontsize=15)\n",
    "    plt.ylabel('p(B_H|T)',fontsize=15)\n",
    "    plt.xticks(ticks=np.arange(0.6,4.7,1), labels=np.around(10**np.arange(0.6,4.7,1),2),fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()    \n",
    "        \n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/experimenter=e453_posteriorUsingPercepts.png')\n",
    "    \n",
    "    #lowProbDist, highProbDist = visualizeProbDistributions(sample_x=log_freq_percept,\n",
    "    #                                                       log_freq_low=mleParams_expt[iSubj,[0,2]],\n",
    "    #                                                       log_freq_high=mleParams_expt[iSubj,[1,2]])\n",
    "    #plt.figure()\n",
    "    #plt.plot(log_freq_percept,(1-mleParams_expt[iSubj,4])*mleParams_expt[iSubj,5]*lowProbDist/(1-mleParams_expt[iSubj,5]),'k')\n",
    "    #plt.plot(log_freq_percept,(1-mleParams_expt[iSubj,4])*(1-mleParams_expt[iSubj,5])*highProbDist/mleParams_expt[iSubj,5],'k')\n",
    "    #plt.plot(log_freq_percept,mleParams_expt[iSubj,4]*np.ones(len(log_freq_percept))/len(log_freq_percept),'skyblue')\n",
    "    #plt.xlabel('Tones',fontsize=15)\n",
    "    #plt.ylabel('Probability',fontsize=15)\n",
    "    #plt.xticks(ticks=np.arange(0.9,4.7,0.8), labels=np.around(10**np.arange(0.9,4.7,0.8),2),fontsize=14)\n",
    "    #plt.yticks(fontsize=15)\n",
    "    #plt.tight_layout()    \n",
    "    #plt.savefig('figures/FromProlific/rawDataAnalysis/experimenter=1c3f_probabilityDistributions.png')\n",
    "    \n",
    "#print('Difference between Bayes and Null',scipy.stats.ks_2samp(BayesRsquared, NullRsquared))\n",
    "#print('Difference between Bayes and voting',scipy.stats.ks_2samp(BayesRsquared, VotingRsquared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['735a','1304','6b7f','1c3f','1604','1396','b4c7','a12e','c653','4b7f','fc3a','0d04',\n",
    "           '2099','592d','a45d','e453','214b','b7cc','188a','e12e']\n",
    "\n",
    "for ii in range(len(trial_behaviour_exptHc)):\n",
    "    plt.figure()\n",
    "    \"\"\"\n",
    "    Comparing to low context experiment\n",
    "    \"\"\"\n",
    "    idxHigh = np.arange(len(trial_behaviour_exptLc[iSubjLowandHigh[ii]][80:]))[corrans_exptLc[iSubjLowandHigh[ii]][80:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_exptLc[iSubjLowandHigh[ii]][80:]))[corrans_exptLc[iSubjLowandHigh[ii]][80:]==0]\n",
    "    idxToKeep = np.concatenate((idxHigh, np.random.choice(idxLow,size=len(idxHigh),replace=False)))\n",
    "    print(len(idxToKeep))\n",
    "    \n",
    "    unique_tones, bhvLow_mean, bhvLow_std = plotting_pBHgivenT(subjectBehaviour = trial_behaviour_exptLc[iSubjLowandHigh[ii]][80:], \n",
    "                                                               subjectTones = trial_tones_exptLc[iSubjLowandHigh[ii]][80:], \n",
    "                                                               contextIndex = idxToKeep)\n",
    "    plt.errorbar(np.log10(unique_tones), bhvLow_mean, yerr=bhvLow_std/np.sqrt(3), color='blue') \n",
    "    \n",
    "    \"\"\"\n",
    "    Comparing to high context experiment\n",
    "    \"\"\"\n",
    "    idxHigh = np.arange(len(trial_behaviour_exptHc[ii][80:]))[corrans_exptHc[ii][80:]==1]\n",
    "    idxLow = np.arange(len(trial_behaviour_exptHc[ii][80:]))[corrans_exptHc[ii][80:]==0]\n",
    "    idxToKeep = np.concatenate((idxLow, np.random.choice(idxHigh,size=len(idxLow),replace=False)))\n",
    "    print(len(idxToKeep))\n",
    "    \n",
    "    unique_tones, bhvHigh_mean, bhvHigh_std = plotting_pBHgivenT(subjectBehaviour = trial_behaviour_exptHc[ii][80:], \n",
    "                                                                 subjectTones = trial_tones_exptHc[ii][80:], \n",
    "                                                                 contextIndex = idxToKeep)\n",
    "    plt.errorbar(np.log10(unique_tones), bhvHigh_mean, yerr=bhvHigh_std/np.sqrt(3), color='green') \n",
    "    \n",
    "    \"\"\"\n",
    "    Comparing to no context experiment\n",
    "    \"\"\"\n",
    "    unique_tones, bhvNo_mean, bhvNo_std = plotting_pBHgivenT(subjectBehaviour = trial_behaviour_expt[iSubjNoandLowandHigh[ii]], \n",
    "                                                             subjectTones = trial_tones_expt[iSubjNoandLowandHigh[ii]], \n",
    "                                                             contextIndex = range(len(trial_behaviour_expt\n",
    "                                                                                      [iSubjNoandLowandHigh[ii]])))\n",
    "    plt.errorbar(np.log10(unique_tones), bhvNo_mean, yerr=bhvNo_std/np.sqrt(3), color='red') \n",
    "    \n",
    "    \"\"\"\n",
    "    Comparing to low context simulations\n",
    "    \n",
    "    simLowTones, simLowBehaviour, simLowCorrNum = sample_expt(params=mleParams_exptLc[iSubjLowandHigh[ii]],\n",
    "                                                              pl_orig=0.7)\n",
    "\n",
    "    idxSimHigh = np.arange(len(simLowBehaviour))[simLowCorrNum==1]\n",
    "    idxSimLow = np.arange(len(simLowBehaviour))[simLowCorrNum==0]\n",
    "    idxSimToKeep = np.concatenate((idxSimHigh, np.random.choice(idxSimLow,size=len(idxSimHigh),replace=False)))\n",
    "    \n",
    "    unique_tones, simBhvLow_mean, simBhvLow_std = plotting_pBHgivenT(subjectBehaviour = simLowBehaviour, \n",
    "                                                                    subjectTones = simLowTones, \n",
    "                                                                    contextIndex = idxSimToKeep)\n",
    "    plt.errorbar(np.log10(unique_tones), simBhvLow_mean, yerr=simBhvLow_std/np.sqrt(3), color='grey')\n",
    "    \n",
    "    \n",
    "    Comparing to no context simulations\n",
    "    \n",
    "    simNoTones, simNoBehaviour, simNoCorrNum  = sample_expt(params=mleParams_expt[iSubjNoandLowandHigh[ii]],\n",
    "                                                            pl_orig=0.5)\n",
    "\n",
    "    unique_tones, simBhvNo_mean, simBhvNo_std = plotting_pBHgivenT(subjectBehaviour = simNoBehaviour, \n",
    "                                                                   subjectTones = simNoTones, \n",
    "                                                                   contextIndex = range(len(simNoBehaviour)))\n",
    "    plt.errorbar(np.log10(unique_tones), simBhvNo_mean, yerr=simBhvNo_std/np.sqrt(3), color='black')\n",
    "    \"\"\"\n",
    "    plt.xlabel('log10(Tones)',fontsize=15)\n",
    "    plt.ylabel('p(B_H|T)',fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/FromProlific/biasedLongContext_rawDataAnalysis/{subjects[ii]}_pBHgivenT_forNoandLongContext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(len(iSubjNoandLowandHigh)):\n",
    "    accuracyOverTime = np.zeros(len(trial_behaviour_expt[iSubjNoandLowandHigh[jj]]))\n",
    "    plt.figure()\n",
    "    for tr in range(20,len(trial_behaviour_expt[iSubjNoandLowandHigh[jj]])):\n",
    "        accuracyOverTime[tr] = np.sum(trial_behaviour_expt[iSubjNoandLowandHigh[jj]][tr-20:tr]\n",
    "                           ==corrans_expt[iSubjNoandLowandHigh[jj]][tr-20:tr])/20\n",
    "        #plt.plot(np.cumsum(trial_behaviour_exptLc[iSubjLowandHigh[jj]][tr-10:tr]\n",
    "        #                   ==corrans_exptLc[iSubjLowandHigh[jj]][tr-10:tr])/10,color='blue')\n",
    "        #plt.plot(np.cumsum(trial_behaviour_exptHc[jj][tr-10:tr]\n",
    "        #                   ==corrans_exptHc[jj][tr-10:tr])/10,color='green')\n",
    "    plt.plot(accuracyOverTime)    \n",
    "    plt.xlim([-10,600])\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "        #plt.savefig(f'figures/FromProlific/biasedLongContext_rawDataAnalysis/learningRate_Subj{jj}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

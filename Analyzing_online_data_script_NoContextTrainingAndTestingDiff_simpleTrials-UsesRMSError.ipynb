{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import scipy\n",
    "from scipy.optimize import minimize, fmin\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Obtaining data from a given expt\n",
    "\"\"\"\n",
    "csv_test = pd.read_csv('../auditory_categorization_noContext/important_things_not_included_in_assets/allTrials.csv')\n",
    "csv_data = pd.read_csv('auditory_categorization_prolific_online_data/human_auditory_categorization_91686_2022-01-19_20h56.51_f8da6647-2fb7-444d-9e8c-e52b3a37d4b2/6062c088821c76a49374e453_categorization_task_2021-05-17_23h24.18.467.csv');\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tones = 3\n",
    "n_trials = csv_data.shape[0]-47\n",
    "\n",
    "\"\"\"\n",
    "Get tones and values of keys pressed\n",
    "\"\"\"\n",
    "test_columns = list(csv_test.columns)\n",
    "test_tones_name = test_columns.index('Name')\n",
    "test_tones_col_idx = test_columns.index('Tones')\n",
    "test_tones_cat_col_idx = test_columns.index('Tonekind')\n",
    "\n",
    "df_names = (csv_test.iloc[:,test_tones_name]).values\n",
    "df_tones = (csv_test.iloc[:,test_tones_col_idx]).values\n",
    "df_tone_cat = (csv_test.iloc[:,test_tones_cat_col_idx]).values\n",
    "\n",
    "tones_array_orig = np.zeros((n_trials,n_tones))\n",
    "tones_array_idxs_keep = []\n",
    "\n",
    "tones_cat_array_orig = np.zeros((n_trials,n_tones))\n",
    "tones_cat_array_idxs_keep = []\n",
    "\n",
    "for i_wav in range(n_trials):\n",
    "    if isinstance(csv_data['Name'][i_wav+46],str):\n",
    "        tones_array_orig[i_wav,:] = np.array(df_tones[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                          ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "        tones_array_idxs_keep += [i_wav]\n",
    "\n",
    "        tones_cat_array_orig[i_wav,:] = np.array(df_tone_cat[np.where(csv_data['Name'][i_wav+46]\\\n",
    "                                                          ==df_names)[0]][0][1:-1].split(',')).astype(float)  \n",
    "        tones_cat_array_idxs_keep += [i_wav]\n",
    "\n",
    "\n",
    "df_tones = np.copy(tones_array_orig[tones_array_idxs_keep,:])\n",
    "df_tone_cat = np.copy(tones_cat_array_orig[tones_cat_array_idxs_keep,:])\n",
    "df_corrans = np.copy(csv_data['corrAns'][46:csv_data.shape[0]])[tones_array_idxs_keep]\n",
    "df_keys = np.copy(csv_data['test_resp.keys'][46:csv_data.shape[0]])[tones_array_idxs_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not respond to:  [  0 150]\n",
      "Got correct:  0.6883333333333334\n",
      "Got high correct:  0.7408637873754153\n",
      "Got low correct:  0.6354515050167224\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Find no response cases in the expt\n",
    "\"\"\"\n",
    "no_response = np.intersect1d(np.where(df_keys!='h')[0],np.where(df_keys!='l')[0])\n",
    "print(\"Did not respond to: \",no_response)\n",
    "\n",
    "\"\"\"\n",
    "Convert keys ['l','h'] to [0,1] and calculate accuracies\n",
    "\"\"\"\n",
    "corrans_num_orig = np.zeros_like(df_corrans)\n",
    "corrans_num_orig[df_corrans == 'h'] = 1\n",
    "\n",
    "keys_num_orig = np.zeros_like(df_keys)\n",
    "keys_num_orig[df_keys == 'h'] = 1\n",
    "\n",
    "corrans_num = corrans_num_orig[:600]\n",
    "keys_num = keys_num_orig[:600]\n",
    "tones_array = df_tones[:600]\n",
    "print(\"Got correct: \", np.sum(keys_num==corrans_num)/len(tones_array))\n",
    "print(\"Got high correct: \", np.sum((keys_num)*(corrans_num))/np.sum(corrans_num))\n",
    "print(\"Got low correct: \", np.sum((1-keys_num)*(1-corrans_num))/np.sum(1-corrans_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janaki/miniconda3/envs/glm/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/janaki/miniconda3/envs/glm/lib/python3.9/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "allTrial_tones = np.repeat(tones_array,1,axis = 0)\n",
    "allTrial_behaviour = np.reshape(keys_num,np.prod(keys_num.shape)) \n",
    "# this has been changed to check how values change with observer responses\n",
    "\n",
    "expt_tones = np.arange(90,3000,1) #array of possible true tones\n",
    "exptFreqSeqArray = np.arange(np.log10(expt_tones[0]), np.log10(expt_tones[-1]), np.log10(1003/1000)*40)\n",
    "log_freq_seq_array = np.arange(0.6,4.7,0.1)\n",
    "log_freq_percept = np.arange(0.6,4.7,0.1) # array of possible perceptual tones\n",
    "expt_freq_seq_mid = np.median(exptFreqSeqArray)\n",
    "low_dist = [expt_freq_seq_mid - 0.15,0.1]\n",
    "high_dist = [expt_freq_seq_mid + 0.15,0.1]\n",
    "\n",
    "idxs_with_response = np.delete(np.arange(len(allTrial_tones)),no_response)\n",
    "allTrial_tones_responded = allTrial_tones[idxs_with_response,:]\n",
    "allTrial_behaviour_responded = allTrial_behaviour[idxs_with_response]\n",
    "allTrial_tones_cat_responded = df_tone_cat[idxs_with_response]\n",
    "iAllGaussLow = 0\n",
    "iAllGaussHigh = 0\n",
    "allGaussLow = np.zeros((len(allTrial_behaviour_responded),5))\n",
    "allGaussHigh = np.zeros((len(allTrial_behaviour_responded),5))\n",
    "    \n",
    "for i_trial in range(len(allTrial_tones_responded)):\n",
    "    if sum(allTrial_tones_cat_responded[i_trial]==1)==3:\n",
    "        allGaussLow[iAllGaussLow,:3] = allTrial_tones_responded[i_trial,:]\n",
    "        allGaussLow[iAllGaussLow,3] = allTrial_behaviour_responded[i_trial]\n",
    "        allGaussLow[iAllGaussLow,4] = int(i_trial)\n",
    "        iAllGaussLow += 1\n",
    "    #if (sum(np.log10(allTrial_tones_responded[i_trial])<expt_freq_seq_mid)==3 \n",
    "    #    & sum(np.log10(allTrial_tones_responded[i_trial])>(low_dist[0]-2*low_dist[1]))==3):\n",
    "    #    allGaussLow[iAllGaussLow,:3] = allTrial_tones_responded[i_trial,:]\n",
    "    #    allGaussLow[iAllGaussLow,3] = allTrial_behaviour_responded[i_trial]\n",
    "    #    allGaussLow[iAllGaussLow,4] = int(i_trial)\n",
    "    #    print(i_trial, \n",
    "    #      allTrial_tones_cat_responded[i_trial], \n",
    "    #      np.log10(allGaussLow[iAllGaussLow,:3]),\n",
    "    #      allGaussLow[iAllGaussLow,3])\n",
    "    #    iAllGaussLow += 1\n",
    "    elif sum(allTrial_tones_cat_responded[i_trial]==2)==3:\n",
    "        allGaussHigh[iAllGaussHigh,:3] = allTrial_tones_responded[i_trial,:]\n",
    "        allGaussHigh[iAllGaussHigh,3] = allTrial_behaviour_responded[i_trial]    \n",
    "        allGaussHigh[iAllGaussHigh,4] = int(i_trial)\n",
    "        iAllGaussHigh += 1\n",
    "    #elif (sum(np.log10(allTrial_tones_responded[i_trial])>expt_freq_seq_mid)==3 \n",
    "    #    & sum(np.log10(allTrial_tones_responded[i_trial])<(high_dist[0]+2*high_dist[1]))==3): \n",
    "    #    allGaussHigh[iAllGaussHigh,:3] = allTrial_tones_responded[i_trial,:]\n",
    "    #    allGaussHigh[iAllGaussHigh,3] = allTrial_behaviour_responded[i_trial]    \n",
    "    #    allGaussHigh[iAllGaussHigh,4] = int(i_trial)        \n",
    "    #    iAllGaussHigh += 1\n",
    "AllGaussian = np.concatenate((allGaussLow[:iAllGaussLow], allGaussHigh[:iAllGaussHigh]),axis=0)    \n",
    "trial_tones = AllGaussian[:,:3]\n",
    "trial_behaviour = AllGaussian[:,3]\n",
    "\n",
    "unique_tones = np.unique(trial_tones)\n",
    "tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "for i_tone in range(len(unique_tones)):\n",
    "    tone1_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,0]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone2_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,1]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone3_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,2]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    \n",
    "influence = np.nanmean([tone1_prob_behaviour,tone2_prob_behaviour,\n",
    "                        tone3_prob_behaviour],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mean, sigma):\n",
    "    return np.exp(-(x-mean)**2/(2*sigma**2))\n",
    "\n",
    "def Tones1dgrid(latentTones, sigma):    \n",
    "    \n",
    "    input_array_0 = np.expand_dims(gaussian(log_freq_percept, latentTones[0], sigma), axis = 1);\n",
    "    s0 = 1/np.sum(input_array_0); \n",
    "    input_array_0 *= s0; \n",
    "    return input_array_0\n",
    "\n",
    "def Tones3dgrid(latentTones, sigma):    \n",
    "    \n",
    "    input_array_0 = np.expand_dims(gaussian(log_freq_percept, latentTones[0], sigma), axis = 1)\n",
    "    input_array_1 = np.expand_dims(gaussian(log_freq_percept, latentTones[1], sigma), axis = 1)\n",
    "    input_array_2 = np.expand_dims(gaussian(log_freq_percept, latentTones[2], sigma), axis = 1)\n",
    "    s0 = 1/np.sum(input_array_0); \n",
    "    s1 = 1/np.sum(input_array_1); \n",
    "    s2 = 1/np.sum(input_array_2);\n",
    "    input_array_0 *= s0; \n",
    "    input_array_1 *= s1; \n",
    "    input_array_2 *= s2; \n",
    "    \n",
    "    input_array_mat = np.expand_dims(input_array_0@input_array_1.T,axis=2)@(input_array_2.T) #p(T1,T2..|H)   \n",
    "                                     \n",
    "    return input_array_mat\n",
    "\n",
    "def posterior_array(freq_input, n_tones, p_low, log_prior):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    freq_input - range of all possible frequencies (percepts?)\n",
    "    p_back - prob of background\n",
    "    p_low - prob of low condition\n",
    "    log_prior - list of prior parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    log_prior_low_mean = log_prior[0]; log_prior_low_sigma = log_prior[2];\n",
    "    log_prior_high_mean = log_prior[1]; log_prior_high_sigma = log_prior[2];\n",
    "    prior_low = gaussian(x=freq_input, mean=log_prior_low_mean, sigma=log_prior_low_sigma)\n",
    "    prior_high = gaussian(x=freq_input, mean=log_prior_high_mean, sigma=log_prior_high_sigma)\n",
    "    prior_dist_mixed_high = prior_high \n",
    "    #mixture model with p(T|B) = 1/no. of possible freqs\n",
    "    prior_dist_mixed_high /= prior_dist_mixed_high.sum() #normalizing\n",
    "    prior_dist_mixed_high = np.expand_dims(prior_dist_mixed_high, axis = 1)\n",
    "    prior_dist_mixed_low = prior_low \n",
    "    #mixture model with p(T|B) = 1/no. of possible freqs\n",
    "    prior_dist_mixed_low /= prior_dist_mixed_low.sum() #normalizing\n",
    "    prior_dist_mixed_low = np.expand_dims(prior_dist_mixed_low, axis = 1)\n",
    "        \n",
    "    if n_tones == 3:\n",
    "        prior_tones_low = np.expand_dims(prior_dist_mixed_low@np.transpose\\\n",
    "                                         (prior_dist_mixed_low),axis=2)@np.transpose(prior_dist_mixed_low) \\\n",
    "        #p(T1,T2..|L) \n",
    "        \n",
    "        prior_tones_high = np.expand_dims(prior_dist_mixed_high@np.transpose\\\n",
    "                                          (prior_dist_mixed_high),axis=2)@np.transpose(prior_dist_mixed_high) \\\n",
    "        #p(T1,T2..|H) \n",
    "\n",
    "    elif n_tones == 1:\n",
    "        prior_tones_low = prior_dist_mixed_low\n",
    "        prior_tones_high = prior_dist_mixed_high\n",
    "        \n",
    "    normalizer = (1-p_low)*prior_tones_high + p_low*prior_tones_low #p(H)*p(T1,T2..|H) + p(L)*p(T1,T2..|L)\n",
    "    posterior = prior_tones_high*(1-p_low)/normalizer\n",
    "    # posterior /= np.sum(posterior)\n",
    "    \n",
    "    return prior_dist_mixed_high, prior_dist_mixed_low, prior_tones_high, prior_tones_low, normalizer, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mle function\n",
    "def errorFn(modelProbHighGivenPercept, exptInfluenceFunction):\n",
    "    return np.squeeze(modelProbHighGivenPercept)-np.squeeze(exptInfluenceFunction)\n",
    "    \n",
    "def MLE(params):\n",
    "    log_prior_low_mean, log_prior_high_mean, log_prior_sigma, sigma_sensory, prob_low = \\\n",
    "    params[0], params[1], params[2], params[3], params[4] # inputs are guesses at our parameters  \n",
    "    \n",
    "    _,_,LikelihoodLatentTonegivenHigh,LikelihoodLatentTonegivenLow,_,_ = \\\n",
    "    posterior_array(log_freq_seq_array, n_tones=1, p_low=prob_low,\\\n",
    "                    log_prior=[log_prior_low_mean,log_prior_high_mean,log_prior_sigma])\n",
    "\n",
    "    LikelihoodPerceptgivenHigh = np.zeros((len(log_freq_percept),1))\n",
    "    LikelihoodPerceptgivenLow = np.zeros((len(log_freq_percept),1))\n",
    "    \n",
    "    for itrue1 in range(len(log_freq_seq_array)):\n",
    "        probPerceptgivenLatentTones = Tones1dgrid([log_freq_seq_array[itrue1]],sigma=sigma_sensory)                                                           \n",
    "        LikelihoodPerceptgivenHigh \\\n",
    "        += probPerceptgivenLatentTones * LikelihoodLatentTonegivenHigh[itrue1]\n",
    "        LikelihoodPerceptgivenLow \\\n",
    "        += probPerceptgivenLatentTones * LikelihoodLatentTonegivenLow[itrue1]\n",
    "    probHighgivenPercept = LikelihoodPerceptgivenHigh*(1-prob_low)/\\\n",
    "    (LikelihoodPerceptgivenHigh*(1-prob_low) + LikelihoodPerceptgivenLow*(prob_low))\n",
    "    \n",
    "    probability_high = np.zeros((len(unique_tones),1))\n",
    "    for i_tone in range(len(unique_tones)):\n",
    "        input_array_mat = Tones1dgrid([np.log10(unique_tones[i_tone])],sigma=sigma_sensory)\n",
    "        probability_high[i_tone] = np.sum(np.multiply(probHighgivenPercept>0.5,input_array_mat))\n",
    "    \n",
    "    #plt.plot(probability_high)\n",
    "    #plt.plot(influence)\n",
    "    RMSerror = errorFn(probability_high, influence)\n",
    "    #plt.plot(np.abs(RMSerror))\n",
    "    #plt.show()\n",
    "    #print(sigma_sensory, np.sum(np.abs(RMSerror)))\n",
    "           \n",
    "    return(np.sum(RMSerror**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e24eaa5d234f0d87524a93def4d18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-ae22e253aa1e>:63: RuntimeWarning: invalid value encountered in true_divide\n",
      "  posterior = prior_tones_high*(1-p_low)/normalizer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-fbe86777d2fe>:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  probHighgivenPercept = LikelihoodPerceptgivenHigh*(1-prob_low)/\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 [0.18798702 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702\n",
      " 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702\n",
      " 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702\n",
      " 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702\n",
      " 0.18798702 0.18798702 0.18798702] [[2.1  2.1  2.1  2.1  2.1  2.1  2.1  2.25 2.25 2.25 2.25 2.25 2.25 2.25\n",
      "  2.25 2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.55 2.55 2.55 2.55 2.7 ]\n",
      " [3.15 3.15 3.15 3.15 3.3  3.3  3.3  2.85 3.   3.   3.   3.   3.15 3.15\n",
      "  3.15 2.7  2.7  2.85 2.85 3.   3.   3.   2.7  2.85 2.85 3.   2.85]\n",
      " [0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29\n",
      "  0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29]\n",
      " [0.45 0.5  0.55 0.6  0.4  0.45 0.5  0.6  0.45 0.5  0.55 0.6  0.4  0.45\n",
      "  0.5  0.55 0.6  0.5  0.55 0.4  0.45 0.5  0.5  0.45 0.5  0.4  0.45]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28041b2ce264a779e8992776804699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "High mean:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 [0.18798702 0.18798702 0.18798702 0.18798702 0.18798702 0.18798702\n",
      " 0.18798702 0.18798702] [[2.1  2.1  2.25 2.25 2.4  2.4  2.55 2.55]\n",
      " [3.15 3.3  3.   3.15 2.85 3.   2.7  2.85]\n",
      " [0.29 0.29 0.29 0.29 0.29 0.29 0.29 0.29]\n",
      " [0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5 ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "New optimization algorithm: uses scipy.optimize.fmin. \n",
    "Crude grid initially and then find minimum using the function.\n",
    "\"\"\"\n",
    "guess_VaryingSigma = np.asarray([0.02,1])\n",
    "nll_VaryingSigma = np.zeros(len(guess_VaryingSigma))\n",
    "thetas_VaryingSigma = np.zeros((len(guess_VaryingSigma),4))\n",
    "for s in range(len(guess_VaryingSigma)):\n",
    "    guess_low_mean = np.arange(2.1,2.71,0.15); guess_high_mean = np.arange(2.7,3.31,0.15); \n",
    "    guess_sensory_sigma = np.arange(0.05,0.4,0.01); guess_p_low = np.arange(0.4,0.61,0.05)\n",
    "\n",
    "    # Constraining guesses of means of low and high distributions based on observed behaviour in figure shown above. \n",
    "\n",
    "    neg_ll_array = np.zeros((len(guess_low_mean), len(guess_high_mean),\n",
    "                             len(guess_sensory_sigma), len(guess_p_low)))\n",
    "    for lm in tqdm(range(len(guess_low_mean))):\n",
    "        for hm in tqdm(range(len(guess_high_mean)), leave=False, desc=\"High mean\"):\n",
    "            for ss in range(len(guess_sensory_sigma)):\n",
    "                for pl in range(len(guess_p_low)):\n",
    "                    params = [guess_low_mean[lm], guess_high_mean[hm], guess_VaryingSigma[s], \\\n",
    "                              guess_sensory_sigma[ss], guess_p_low[pl]]\n",
    "                    # print(lm, hm, pb)\n",
    "                    neg_ll_array[lm,hm,ss,pl] = MLE(params) \n",
    "\n",
    "    \"\"\"\n",
    "    Means and p_back corresponding to the least negative log likelihood value\n",
    "    \"\"\"\n",
    "    idxs = np.where(neg_ll_array == np.amin(neg_ll_array)) \n",
    "    best_thetas = np.array([guess_low_mean[idxs[0]], guess_high_mean[idxs[1]], \n",
    "                            guess_sensory_sigma[idxs[2]], guess_p_low[idxs[3]]])\n",
    "    \n",
    "    print(guess_VaryingSigma[s], neg_ll_array[idxs], best_thetas)\n",
    "    #nll_VaryingSigma[s] = neg_ll_array[idxs]\n",
    "    #thetas_VaryingSigma[s,:] = best_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define mle function\n",
    "def MLE_fmin(params):\n",
    "    log_prior_low_mean, log_prior_high_mean, log_prior_sigma, sigma_sensory, prob_low = \\\n",
    "    params[0], params[1], params[2], params[3], params[4] # inputs are guesses at our parameters  \n",
    "    \n",
    "    _,_,LikelihoodLatentTonegivenHigh,LikelihoodLatentTonegivenLow,_,_ = \\\n",
    "    posterior_array(log_freq_seq_array, n_tones=len(trial_tones[0]), p_low=prob_low,\\\n",
    "                    log_prior=[log_prior_low_mean,log_prior_high_mean,log_prior_sigma])\n",
    "\n",
    "    LikelihoodPerceptgivenHigh = np.zeros((len(log_freq_percept),len(log_freq_percept),len(log_freq_percept)))\n",
    "    LikelihoodPerceptgivenLow = np.zeros((len(log_freq_percept),len(log_freq_percept),len(log_freq_percept)))\n",
    "    \n",
    "    for itrue1 in range(len(log_freq_seq_array)):\n",
    "        for itrue2 in range(len(log_freq_seq_array)):            \n",
    "            for itrue3 in range(len(log_freq_seq_array)):\n",
    "                probPerceptgivenLatentTones = Tones3dgrid([log_freq_seq_array[itrue1],\\\n",
    "                                                           log_freq_seq_array[itrue2],\\\n",
    "                                                           log_freq_seq_array[itrue3]],sigma=sigma_sensory)                                                           \n",
    "                LikelihoodPerceptgivenHigh \\\n",
    "                += probPerceptgivenLatentTones * LikelihoodLatentTonegivenHigh[itrue1,itrue2,itrue3]\n",
    "                LikelihoodPerceptgivenLow \\\n",
    "                += probPerceptgivenLatentTones * LikelihoodLatentTonegivenLow[itrue1,itrue2,itrue3]\n",
    "    probHighgivenPercept = LikelihoodPerceptgivenHigh*(1-prob_low)/\\\n",
    "    (LikelihoodPerceptgivenHigh*(1-prob_low) + LikelihoodPerceptgivenLow*(prob_low))\n",
    "        \n",
    "    neg_ll = 0; \n",
    "    probability_high = np.zeros((len(trial_tones),1))\n",
    "    for i_trial in range(len(trial_tones)):\n",
    "        input_array_mat = Tones3dgrid(np.array([np.log10(trial_tones[i_trial][0]),\\\n",
    "                                               np.log10(trial_tones[i_trial][1]),\n",
    "                                               np.log10(trial_tones[i_trial][2])]),sigma=sigma_sensory)\n",
    "        probability_high0 = np.sum(np.multiply(probHighgivenPercept>0.5,input_array_mat))\n",
    "        probability_high[i_trial] = np.sum(np.multiply(probHighgivenPercept>0.5,input_array_mat))\n",
    "            \n",
    "        if trial_behaviour[i_trial]:\n",
    "            if np.isnan(np.log(probability_high0 + 0.0000001)) \\\n",
    "            or np.isinf(np.log(probability_high0 + 0.0000001)) \\\n",
    "            or np.isnan(np.log(1-probability_high0 + 0.0000001)) \\\n",
    "            or np.isinf(np.log(1-probability_high0 + 0.0000001)):\n",
    "                pdb.set_trace()\n",
    "            neg_ll += -np.log(probability_high0 + 0.0000001) # if high dist is chosen by observer\n",
    "        else:\n",
    "            neg_ll += -np.log(1 - probability_high0 + 0.0000001) # if low dist is chosen by observer\n",
    "    print(params, neg_ll)        \n",
    "    return(neg_ll)\n",
    "\n",
    "\"\"\"\n",
    "Optimization using neadler mead method and a simplex algorithm\n",
    "\"\"\"\n",
    "minimum_nll = scipy.optimize.fmin(MLE_fmin, [2.4,3,0.02,0.25,0.5], maxiter=10000, maxfun=10000, \n",
    "                                  xtol=0.01, ftol=0.01)\n",
    "\n",
    "print(minimum_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Influence plots the way we currently understand them (11-17-2020)\n",
    "unique_tones = np.unique(trial_tones)\n",
    "tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "for i_tone in range(len(unique_tones)):\n",
    "    tone1_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,0]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone2_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,1]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone3_prob_behaviour[i_tone] = np.mean(trial_behaviour[trial_tones[:,2]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "influence1, = plt.plot(np.log10(unique_tones), tone1_prob_behaviour, label = 'Influence of Tone 1')\n",
    "influence2, = plt.plot(np.log10(unique_tones), tone2_prob_behaviour, label = 'Influence of Tone 2')\n",
    "influence3, = plt.plot(np.log10(unique_tones), tone3_prob_behaviour, label = 'Influence of Tone 3')\n",
    "influence, = plt.plot(np.log10(unique_tones), np.nanmean([tone1_prob_behaviour,tone2_prob_behaviour,\n",
    "                                                          tone3_prob_behaviour],axis=0),\n",
    "                       'k', label = 'Average Influence')\n",
    "\n",
    "_, probability_high = MLE(minimum_nll)\n",
    "\n",
    "tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "for i_tone in range(len(unique_tones)):\n",
    "    tone1_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,0]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone2_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,2]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone3_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,2]\\\n",
    "                                                      ==unique_tones[i_tone]])\n",
    "mnll_influence, = plt.plot(np.log10(unique_tones),\n",
    "                           (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3,'k.',\n",
    "                          label = 'p(B_H|T) given fmin parameters')\n",
    "\n",
    "\"\"\"\n",
    " _, probability_high = MLE(best_thetas)\n",
    "\n",
    "tone1_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone2_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "tone3_prob_behaviour = np.zeros((len(unique_tones)))\n",
    "\n",
    "for i_tone in range(len(unique_tones)):\n",
    "    tone1_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,0]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone2_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,1]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "    tone3_prob_behaviour[i_tone] = np.mean(probability_high[trial_tones[:,2]\\\n",
    "                                                       ==unique_tones[i_tone]])\n",
    "grid_influence, = plt.plot(np.log10(unique_tones),\n",
    "                           (tone1_prob_behaviour+tone2_prob_behaviour+tone3_prob_behaviour)/3,'k--',\n",
    "                          label = 'p(B_H|T) given grid parameters')\n",
    "\"\"\"\n",
    "plt.legend(handles=[influence, mnll_influence])\n",
    "\n",
    "#plt.xlim([1.9,3.6])\n",
    "plt.ylim([-0.2,1.1])\n",
    "plt.xlabel('log10(Tones)')\n",
    "plt.ylabel('p(B_H|T)')\n",
    "# plt.savefig('figures/experimenter=mark_categorization_task_2020-12-28_18h57.17.409_plow_additional_parameter.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
